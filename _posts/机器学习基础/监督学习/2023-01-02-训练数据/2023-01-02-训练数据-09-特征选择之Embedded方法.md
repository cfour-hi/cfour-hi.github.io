---
title: 机器学习基础-监督学习-训练数据-特征选择之Embedded方法
categories: [机器学习]
tags: [人工智能, 机器学习, 监督学习]
math: true
---

Embedded 方法是一种特征选择技术，它将特征选择嵌入到模型训练过程中，通过正则化方法来减少不重要的特征对模型的影响。常用的 Embedded 方法包括 Lasso、Ridge、Elastic Net 等。

## Lasso（Least Absolute Shrinkage and Selection Operator）

Lasso（Least Absolute Shrinkage and Selection Operator）是一种基于 L1 正则化的线性回归模型，它在最小化均方误差的同时，还会使得一些特征的系数变成 0，从而达到特征选择的效果。

具体来说，Lasso 模型的目标函数为：

$$
\textstyle\min_w \frac{1}{2n} ||Xw-y||_2^2 + \alpha ||w||_1
$$

其中 $X$ 是输入特征矩阵，$y$ 是输出变量，$w$ 是模型的系数，$n$ 是样本数量，$\alpha$ 是 L1 正则化项的系数。L1 正则化项为 $||w||1 = \sum{i=1}^p |w_i|$，它是系数的绝对值之和。

Lasso 的目标函数中的 $\alpha$ 控制着正则化的强度，较大的 $\alpha$ 会使得一些特征的系数变成 0，从而达到特征选择的效果。当 $\alpha$ 等于 0 时，Lasso 模型就变成了普通的线性回归模型。

为了解决 Lasso 模型在系数取值处不可导的问题，可以使用坐标下降算法来求解。坐标下降算法是一种迭代算法，它每次只更新一个系数的值，而将其他系数的值保持不变。具体来说，坐标下降算法的迭代公式为：

$$
w_i^{(t+1)} = S(\frac{1}{n} \textstyle\sum_{j=1}^{n} x_{ij}(y_i- \sum_{k\ne i} x_{jk} w_k^{(t)}), \alpha)
$$

其中 $S$ 是软阈值函数，它的定义为：

$$
S(z,\alpha) = sign(z)(||z||-\alpha)_+
$$

其中 $sign(z)$ 表示 $z$ 的符号，$(x)_+$ 表示取 $x$ 和 0 中的较大值。

下面是一个使用 Lasso 对 Boston Housing 数据集进行特征选择的 Python 示例代码：

```python
from sklearn.datasets import load_boston
from sklearn.linear_model import Lasso

# 加载数据集
boston = load_boston()
X, y = boston.data, boston.target

# 使用 Lasso 进行特征选择
lasso = Lasso(alpha=0.1)
lasso.fit(X, y)

# 输出特征系数
print("特征系数：", lasso.coef_)
```

在这个示例中，我们使用 Lasso 方法对 Boston Housing 数据集进行特征选择，并输出了每个特征的系数。可以发现，Lasso 对一些不重要的特征的系数变成了 0，从而达到了特征选择的效果。

## Ridge

Ridge 回归是一种用于线性回归的正则化方法，它通过增加 L2 正则化项来控制模型的复杂度，防止过拟合。在 Ridge 回归中，模型的目标函数为：

$$
\textstyle \min_w \frac{1}{2n} ||Xw-y||_2^2 + \alpha||w||_2^2
$$

其中，$X$ 是输入特征矩阵，$y$ 是输出变量，$w$ 是模型的系数，$n$ 是样本数量，$\alpha$ 是正则化系数。与 Lasso 回归不同的是，Ridge 回归采用 L2 正则化，所以目标函数中的正则化项为

$$
||w||_2^2
$$

，即系数向量的 L2 范数的平方。

我们可以通过最小化 Ridge 回归的目标函数来求解模型的系数向量 $w$。其中，$\alpha$ 是一个超参数，用于控制正则化的强度。当 $\alpha$ 越大时，正则化的效果就越强，系数向量 $w$ 的范数就越小，从而减少了过拟合的风险。

下面是一个使用 Ridge 回归进行预测的 Python 示例代码：

```python
from sklearn.linear_model import Ridge
from sklearn.datasets import load_boston
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# 加载数据集
boston = load_boston()
X, y = boston.data, boston.target

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 训练 Ridge 回归模型
ridge = Ridge(alpha=1.0)
ridge.fit(X_train, y_train)

# 对测试集进行预测
y_pred = ridge.predict(X_test)

# 计算均方误差
mse = mean_squared_error(y_test, y_pred)
print("均方误差：", mse)
```

在这个示例中，我们使用 Boston Housing 数据集进行 Ridge 回归的训练和预测。需要注意的是，Ridge 回归需要设置一个合适的 $\alpha$ 值，以便控制正则化的强度。通常情况下，我们可以通过交叉验证等方法来选择最优的 $\alpha$ 值。

总之，Ridge 回归是一种用于线性回归的正则化方法，它通过增加 L2 正则化项来控制模型的复杂度，防止过拟合。在实际应用中，我们需要根据具体的问题选择合适的正则化强度和模型超参数，以获得更好的模型性能。

## Elastic Net

Elastic Net 是一种结合了 Lasso 和 Ridge 正则化的线性回归模型，它既能够选择重要的特征，又能够保留一些相关性较强的特征，从而达到更好的特征选择效果。

Elastic Net 的目标函数可以表示为：

$$
\textstyle \min_w \frac{1}{2n} ||Xw-y||_2^2 + \alpha\rho||w||_1 + \frac{\alpha(1-\rho)}{2} ||w||_2^2
$$

其中，$X$ 是输入特征矩阵，$y$ 是输出变量，$w$ 是模型的系数，$n$ 是样本数量，$\alpha$ 是正则化系数，$\rho$ 是 L1 正则化和 L2 正则化的权重，用于平衡两种正则化的影响。当 $\rho = 1$ 时，Elastic Net 退化为 Lasso；当 $\rho = 0$ 时，Elastic Net 退化为 Ridge。

下面是一个使用 Elastic Net 进行特征选择的 Python 示例代码：

```python
from sklearn.datasets import load_boston
from sklearn.linear_model import ElasticNet

# 加载数据集
boston = load_boston()
X, y = boston.data, boston.target

# 使用 Elastic Net 进行特征选择
enet = ElasticNet(alpha=0.1, l1_ratio=0.5)
enet.fit(X, y)
selected_features = enet.coef_ != 0

# 输出结果
print("原始数据特征数：", X.shape[1])
print("特征选择后数据特征数：", selected_features.sum())
```

在这个示例中，我们使用 Elastic Net 方法对 Boston Housing 数据集进行特征选择，并选取了非零系数的特征。需要注意的是，在进行特征选择时，我们需要对选择的特征进行评估和验证，以确保选择出来的特征对模型的性能有实际的贡献。

Elastic Net 可以应用于许多机器学习问题中，包括回归、分类、聚类等。它具有较好的泛化能力和鲁棒性，可以有效地处理高维度的数据集，是一种常用的特征选择方法。

---
title: 机器学习基础-监督学习-测试数据之AUC（Area Under Curve）
categories: [机器学习]
tags: [人工智能, 机器学习, 监督学习]
math: true
---

AUC（Area Under Curve）值是 ROC 曲线（Receiver Operating Characteristic Curve）下的面积，用于评估二分类模型的性能。AUC 值越大，说明模型的性能越好。当 AUC 值为 1 时，表示模型的性能完美，当 AUC 值为 0.5 时，表示模型的性能等同于随机猜测。

以下是一个示例代码，展示如何使用 Python 中的 scikit-learn 库计算 AUC 值：

```python
import numpy as np
from sklearn.metrics import roc_auc_score

# 加载数据
data = np.loadtxt('data.csv', delimiter=',')
X = data[:, :-1]  # 特征矩阵
y = data[:, -1]   # 标签

# 训练模型，假设我们已经定义好了模型
model = ...

# 预测概率
y_prob = model.predict_proba(X)[:, 1]

# 计算AUC值
auc = roc_auc_score(y, y_prob)

print('AUC值为：', auc)
```

上述代码中，我们首先加载数据，并训练一个二分类模型。然后，我们使用 predict_proba 方法预测样本属于正例的概率，并将概率值提取出来。最后，使用 roc_auc_score 函数计算 AUC 值。

需要注意的是，predict_proba 方法返回的是一个二维数组，第一列为属于负例的概率，第二列为属于正例的概率。因此，在上述代码中，我们使用[:, 1]来提取属于正例的概率。

除了使用 Python 中的 scikit-learn 库计算 AUC 值之外，还可以使用手动计算的方式，具体公式为：

$$
AUC = \frac{\sum_{i=1}^{m-1} (TPR_i+TPR_{i+1}) × (FPR_{i+1}-FPR_i)}{2}
$$

其中，$TPR_i$为真正例率（True Positive Rate），也就是$\frac{TP}{P}$，表示在所有实际为正例中，被模型正确预测为正例的比例；$FPR_i$为假正例率（False Positive Rate），也就是$\frac{FP}{N}$，表示在所有实际为负例中，被模型错误预测为正例的比例；$m$为 ROC 曲线上的点的个数。

虽然手动计算 AUC 值的方法更为繁琐，但是对于理解 AUC 值的计算方式和含义很有帮助。

在手动计算 AUC 值时，首先需要计算出模型的 ROC 曲线。ROC 曲线的横轴为 FPR，纵轴为 TPR。我们可以通过在不同的阈值下计算出 TPR 和 FPR 来绘制 ROC 曲线。具体计算方法如下：

假设我们有$N$个样本，其中有$P$个正例，$N-P$个负例。对于一个给定的阈值$t$，我们可以根据模型输出的概率将样本分为正例和负例。如果样本的概率大于等于$t$，则将其预测为正例，否则预测为负例。

在给定阈值$t$的情况下，我们可以计算出 TP 和 FP 的个数，分别表示在所有实际为正例中，被模型正确预测为正例的个数，以及在所有实际为负例中，被模型错误预测为正例的个数。然后，我们可以计算出 TPR 和 FPR，分别表示真正例率和假正例率：

$$
TPR = \frac{TP}{P}
$$

$$
FPR = \frac{FP}{N-P}
$$

随着阈值$t$的变化，我们可以得到一组 TPR 和 FPR，从而绘制出 ROC 曲线。下面是一个示例代码，展示如何手动计算 AUC 值：

```python
import numpy as np

# 加载数据
data = np.loadtxt('data.csv', delimiter=',')
X = data[:, :-1]  # 特征矩阵
y = data[:, -1]   # 标签

# 训练模型，假设我们已经定义好了模型
model = ...

# 预测概率
y_prob = model.predict_proba(X)[:, 1]

# 计算TPR和FPR
thresholds = np.arange(0, 1.01, 0.01)   # 阈值的范围
tpr_list = []   # 真正例率列表
fpr_list = []   # 假正例率列表

for t in thresholds:
    y_pred = y_prob >= t   # 预测结果
    tp = np.sum((y == 1) & (y_pred == 1))   # 真正例个数
    fp = np.sum((y == 0) & (y_pred == 1))   # 假正例个数
    tpr = tp / np.sum(y == 1)   # 真正例率
    fpr = fp / np.sum(y == 0)   # 假正例率
    tpr_list.append(tpr)
    fpr_list.append(fpr)

# 计算AUC值
auc = np.trapz(tpr_list, fpr_list)

print('AUC值为：', auc)
```

在上述代码中，我们首先加载数据，并训练一个二分类模型。然后，我们使用 predict_proba 方法预测样本属于正例的概率，并根据阈值$t$将样本分类为正例或负例。接下来，我们根据不同的阈值计算 TPR 和 FPR，然后绘制 ROC 曲线。最后，我们使用梯形面积法计算 ROC 曲线下的面积，即 AUC 值。

需要注意的是，这里的梯形面积法计算方法是数值积分的一种，可以使用 NumPy 中的 trapz 函数进行计算。该函数的第一个参数为纵坐标列表，第二个参数为横坐标列表。上述代码中，tpr_list 表示真正例率列表，fpr_list 表示假正例率列表。

总之，AUC 值是衡量分类模型性能的重要指标之一，它可以告诉我们模型在不同阈值下的性能表现。在实际应用中，我们通常使用 ROC 曲线和 AUC 值来评估模型的性能，并根据需求选择适当的阈值进行分类。

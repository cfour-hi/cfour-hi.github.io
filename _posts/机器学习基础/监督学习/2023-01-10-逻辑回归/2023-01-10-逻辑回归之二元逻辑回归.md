---
title: 机器学习基础-监督学习-逻辑回归之二元逻辑回归
categories: [机器学习]
tags: [人工智能, 机器学习, 监督学习]
math: true
---

当我们面对一个二元分类问题时，二元逻辑回归是一种常用的监督学习算法。它基于线性回归模型，并使用逻辑函数（也称为 sigmoid 函数）对输出进行转换，以获得概率估计。以下是对二元逻辑回归的详细讲解，包括算法原理、公式和代码示例。

## 算法原理

给定一个训练集，其中包含输入特征向量 x 和相应的二元标签 y (0 或 1)，我们的目标是学习一个适当的模型，该模型能够根据输入特征预测出标签的概率。

二元逻辑回归使用以下假设和模型表达式：

- 假设：给定输入特征 x，对应标签 y 的条件概率服从伯努利分布。
- 模型表达式：$h_\theta(x)=\frac{1}{1+e^{-\theta^Tx}}$

其中，$h_\theta(x)$ 是模型的预测输出，表示预测输入 x 为正例的概率；$\theta$ 是模型的参数向量。

为了学习参数 $\theta$，我们需要定义一个代价函数（损失函数），并通过最小化代价函数来优化参数。二元逻辑回归中常用的代价函数是对数损失函数（logistic loss）：

$$
J(\theta) = -\frac{1}{m}\textstyle\sum_{m}^{i=1}[y^{(i)}\log(h_\theta(x^{(i)}))+(1-y^i)\log(1-h_\theta(x^{(i)}))]
$$

其中，m 是训练样本的数量，$x^{(i)}$ 和 $y^{(i)}$ 分别表示第 i 个训练样本的输入特征和标签。

为了最小化代价函数，可以使用梯度下降法或其他优化算法来更新参数 $\theta$。

## 代码示例

下面是一个使用 Python 和 NumPy 实现二元逻辑回归的简单示例：

```python
import numpy as np

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def logistic_regression(X, y, num_iterations, learning_rate):
    m, n = X.shape
    theta = np.zeros(n)

    for i in range(num_iterations):
        z = np.dot(X, theta)
        h = sigmoid(z)
        gradient = np.dot(X.T, (h - y)) / m
        theta -= learning_rate * gradient

    return theta

# 示例数据
X = np.array([[1, 3], [1, 4], [1, 5], [1, 2]])
y = np.array([1, 1, 0, 0])

# 添加偏置项
X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)

# 训练模型
num_iterations = 1000
learning_rate = 0.1
theta = logistic_regression(X,y, num_iterations, learning_rate)

# 使用训练好的参数进行预测
def predict(X, theta):
X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)
predictions = sigmoid(np.dot(X, theta))
return (predictions >= 0.5).astype(int)

# 预测新样本
new_data = np.array([[1, 6], [1, 1]])
predictions = predict(new_data, theta)
print(predictions)
```

在上面的代码示例中，首先定义了一个 sigmoid 函数来进行激活函数的计算。然后，使用 logistic_regression 函数进行模型的训练，其中需要指定训练数据 X 和标签 y，以及迭代次数和学习率。训练过程中使用梯度下降法更新参数。最后，使用 predict 函数对新样本进行预测，并输出预测结果。

请注意，这只是一个简单的示例，实际中可能需要更多的数据预处理、模型评估和调参等步骤来提高模型的性能和鲁棒性。

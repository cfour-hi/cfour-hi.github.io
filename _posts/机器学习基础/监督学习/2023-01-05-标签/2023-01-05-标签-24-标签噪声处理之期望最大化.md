---
title: 机器学习基础-监督学习-标签噪声处理之期望最大化（Expectation-Maximization，简称EM）
categories: [机器学习]
tags: [人工智能, 机器学习, 监督学习]
math: true
---

期望最大化（Expectation-Maximization，简称 EM）是一种经典的迭代优化算法，用于解决含有潜在变量（或未观测变量）的统计模型的参数估计问题。EM 算法通过迭代的方式，通过不断估计模型参数和潜在变量的期望值（E 步骤）和最大化似然函数来更新模型参数（M 步骤），直至收敛到最优解。

下面是 EM 算法的一般步骤：

1. 初始化模型参数。

2. E 步骤（Expectation Step）：根据当前的模型参数，计算潜在变量的后验概率（或期望值），即给定观测数据和当前模型参数下，每个潜在变量的可能取值的概率。

3. M 步骤（Maximization Step）：利用 E 步骤计算得到的潜在变量的期望值，通过最大化似然函数或期望似然函数，更新模型参数。

4. 重复执行 E 步骤和 M 步骤，直至达到收敛条件（例如，最大迭代次数或参数变化小于某个阈值）。

5. 输出最终收敛的模型参数。

下面是一个简单的示例代码，演示了 EM 算法的应用，假设我们有一组服从正态分布的观测数据，但我们无法观测到数据的真实标签（潜在变量）。我们使用 EM 算法估计出模型的均值和标准差。

```python
import numpy as np
from scipy.stats import norm

# 生成服从正态分布的观测数据
np.random.seed(0)
data = np.concatenate([np.random.normal(0, 1, 500), np.random.normal(5, 1, 500)])

# 初始化模型参数
mu_1 = np.random.randn()
mu_2 = np.random.randn()
sigma_1 = np.random.rand()
sigma_2 = np.random.rand()
weights = np.random.rand()

# EM算法迭代
max_iterations = 100
epsilon = 1e-6
log_likelihood = -np.inf

for iteration in range(max_iterations):
    # E步骤：计算后验概率
    posterior_1 = norm.pdf(data, mu_1, sigma_1) * weights
    posterior_2 = norm.pdf(data, mu_2, sigma_2) * (1 - weights)
    total = posterior_1 + posterior_2
    posterior_1 /= total
    posterior_2 /= total

    # M步骤：更新模型参数
    weights = np.mean(posterior_1)
    mu_1 = np.sum(posterior_1 * data) / np.sum(posterior_1)
    mu_2 = np.sum(posterior_2 * data) / np.sum(posterior_2)
    sigma_1 = np.sqrt(np.sum(posterior_1 * (data - mu_1) ** 2) / np.sum(posterior_1))
    sigma_2 = np.sqrt(np.sum(posterior_2 * (data - mu_2) ** 2) / np.sum(posterior_2))

    # 计算对数似然函数
    current_log_likelihood = np.sum(np.log(posterior_1 * weights + posterior_2 * (1 - weights)))

    # 判断是否收敛
    if current_log_likelihood - log_likelihood < epsilon:
        break
    else:
        log_likelihood = current_log_likelihood

# 输出最终收敛的模型参数
print("模型参数：")
print("mu_1 =", mu_1)
print("mu_2 =", mu_2)
print("sigma_1 =", sigma_1)
print("sigma_2 =", sigma_2)
print("weights =", weights)
```

在上述示例代码中，我们使用 EM 算法估计了具有两个正态分布的混合模型的参数。首先，我们生成了服从两个不同正态分布的观测数据。然后，通过初始化模型参数并迭代执行 E 步骤和 M 步骤，逐步更新模型参数，直至达到收敛条件。最终输出收敛的模型参数，包括两个正态分布的均值、标准差和混合权重。

需要注意的是，EM 算法的具体实现可以根据问题的不同而有所调整。上述示例代码仅为简化版的示例，实际应用中可能需要进行更复杂的模型假设和参数更新。此外，EM 算法也有一些变种和改进算法，以处理更复杂的情况和提高收敛性能。

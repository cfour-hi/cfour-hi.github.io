---
title: 机器学习基础-监督学习-标签平衡处理之随机欠采样（Random Under-Sampling）
categories: [机器学习]
tags: [人工智能, 机器学习, 监督学习]
math: true
---

随机欠采样（Random Under-Sampling）是最简单的欠采样方法之一，其思想是从多数类样本中随机删除一些样本，使得多数类样本数量与少数类样本数量相近。这种方法通常适用于多数类样本数量较多，少数类样本数量较少的情况。

随机欠采样的过程包括以下几个步骤：

1. 首先计算出多数类样本数量 N1 和少数类样本数量 N2。

2. 然后根据设定的采样比例 r，计算出需要删除的多数类样本数量为 (N1 - N2\*r)。

3. 最后从多数类样本中随机选择需要删除的样本数量，并将这些样本从训练集中删除。

下面是一个简单的 Python 代码示例，演示了如何使用随机欠采样方法对不平衡数据集进行处理：

```python
import numpy as np
from sklearn.datasets import make_classification
from collections import Counter

# 生成不平衡数据集
X, y = make_classification(n_samples=1000, weights=[0.9, 0.1], random_state=42)
print('Original dataset shape:', Counter(y))

# 随机欠采样
def random_under_sampling(X, y, ratio=0.5):
    # 计算需要删除的多数类样本数量
    counter = Counter(y)
    target_count = int(counter[1] / ratio)
    # 获取多数类样本的索引
    index = np.arange(len(X))[y == 0]
    # 随机选择需要删除的多数类样本
    random_index = np.random.choice(index, size=target_count, replace=False)
    # 将需要删除的样本从数据集中删除
    delete_index = np.concatenate([random_index, np.arange(len(X))[y == 1]])
    X_resampled = np.delete(X, delete_index, axis=0)
    y_resampled = np.delete(y, delete_index, axis=0)
    return X_resampled, y_resampled

X_resampled, y_resampled = random_under_sampling(X, y)
print('Resampled dataset shape:', Counter(y_resampled))
```

在上面的代码中，我们首先使用 make_classification 函数生成一个包含 1000 个样本的不平衡数据集，其中多数类样本的比例为 90%，少数类样本的比例为 10%。然后使用 random_under_sampling 函数对数据集进行随机欠采样，将多数类样本的数量降低到少数类样本的数量的 50%。最后打印出随机欠采样后的数据集中不同类别的样本数量，可以看到多数类样本的数量已经被降低到了少数类样本的数量的一半。

需要注意的是，随机欠采样可能会删除一些重要的多数类样本，导致模型的性能下降。因此，在实际应用中，我们通常需要进行多次随机欠采样，以获取多个不同的训练集，并在这些训练集上训练多个模型，然后将它们的预测结果进行集成（Ensemble）以提高模型的性能。

此外，随机欠采样还有一些变体方法，例如近似随机欠采样（Near Miss），其思想是保留最接近少数类样本的多数类样本，以确保训练集中的多数类样本能够尽可能地代表整个多数类分布。此外还有 Tomek Links 方法、One-Sided Selection 方法等等。

总之，随机欠采样是一种简单而常用的欠采样方法，适用于多数类样本数量远大于少数类样本数量的情况。但需要注意的是，随机欠采样可能会丢失重要的信息，因此在使用时需要谨慎，并结合其他欠采样方法和集成方法进行处理。

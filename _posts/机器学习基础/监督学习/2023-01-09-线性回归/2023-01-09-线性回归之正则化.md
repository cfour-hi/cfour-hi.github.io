2---
title: 机器学习基础-监督学习-线性回归之正则化
categories: [机器学习]
tags: [人工智能, 机器学习, 监督学习]
math: true

---

正则化是一种常用的机器学习技术，用于控制模型的复杂度，并防止过拟合。通过在目标函数中添加正则化项，可以使得模型倾向于选择较为简单的参数组合。

在监督学习任务中，通常采用的是 L1 正则化和 L2 正则化两种形式。

1. L1 正则化（Lasso 正则化）

L1 正则化通过在目标函数中添加参数的绝对值之和，来限制参数的大小。L1 正则化的数学表达式如下：

```
Loss = 原始目标函数 + λ * Σ|θ_i|
```

其中，Loss 表示加入 L1 正则化后的目标函数，λ 是正则化系数，θ_i 表示模型的第 i 个参数。

L1 正则化的效果是使得一部分参数的值变为 0，从而实现特征选择的效果，能够降低模型的复杂度。由于 L1 正则化的特性，它可以用于稀疏性特征的选择和模型压缩。

2. L2 正则化（Ridge 正则化）

L2 正则化通过在目标函数中添加参数的平方和，来限制参数的大小。L2 正则化的数学表达式如下：

```
Loss = 原始目标函数 + λ * Σ(θ_i)^2
```

其中，Loss 表示加入 L2 正则化后的目标函数，λ 是正则化系数，θ_i 表示模型的第 i 个参数。

L2 正则化的效果是使得所有参数的值都相对较小，从而控制模型的复杂度和泛化能力。L2 正则化能够防止模型过拟合，并对异常值具有较好的鲁棒性。

下面是一个使用 L2 正则化的线性回归模型的示例代码（使用 Python 和 Scikit-learn 库）：

```python
from sklearn.linear_model import Ridge
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.datasets import load_boston
from sklearn.preprocessing import StandardScaler

# 加载数据集
boston = load_boston()
X, y = boston.data, boston.target

# 数据标准化
scaler = StandardScaler()
X = scaler.fit_transform(X)

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 创建模型并训练
ridge = Ridge(alpha=0.5)  # 正则化系数alpha为0.5
ridge.fit(X_train, y_train)

# 在测试集上进行预测
y_pred = ridge.predict(X_test)

# 计算均方根误差
mse = mean_squared_error(y_test, y_pred)
print("均方根误差：", mse)
```

在上述代码中，通过使用 Ridge 类来实现 L2 正则化的线性回归模型，其中 alpha 参数表示正则化系数。较大的 alpha 值会导致模型参数较小，从而降低过拟合的风险。

需要注意的是，正则化的选择应该根据具体问题和数据集来进行调整，以获得更好的模型性能。

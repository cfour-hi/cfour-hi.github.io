[ { "title": "机器学习基础-监督学习-逻辑回归之多元逻辑回归", "url": "/posts/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B9%8B%E5%A4%9A%E5%85%83%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-10 00:00:00 +0800", "snippet": "多元逻辑回归是逻辑回归在多类分类问题上的扩展。在多元逻辑回归中，我们通过使用 softmax 函数将输入特征映射到多个类别的概率分布。假设我们有 m 个训练样本，每个样本有 n 个特征，共有 K 个类别。我们用 X 表示一个 m × n 的矩阵，其中每一行表示一个样本的特征向量，用 Y 表示一个 m × K 的矩阵，其中每一行表示一个样本的类别标签。对于每个样本 i，我们引入一个 K 维的权重向量 W，其中第 k 个元素表示样本 i 属于类别 k 的概率。我们的目标是找到最优的权重向量 W 来最大化训练样本的似然函数。多元逻辑回归模型的假设函数为：\\[h_\\theta(x) = \\text{softmax}(\\theta^Tx)\\]其中，$\\theta$ 是一个 (n+1) × K 的参数矩阵，包含了每个特征对于每个类别的权重。x 是一个 (n+1) 维特征向量，其中第一个元素固定为 1，以对应偏置项。softmax 函数定义如下：\\[\\text{softmax}(z_i) = \\frac{e^{\\approx i}}{\\sum_{k=1}^{K}e^{\\approx k}}\\]其中，$z_{i}$ 表示样本属于类别 i 的得分。我们的目标是最大化似然函数，可以使用最大似然估计或交叉熵损失函数来求解参数。代码示例（使用 Python 和 NumPy）：import numpy as npdef softmax(z): # 避免数值溢出，减去最大值 e_z = np.exp(z - np.max(z, axis=1, keepdims=True)) return e_z / np.sum(e_z, axis=1, keepdims=True)def multiclass_logistic_regression(X, Y, learning_rate, num_iterations): m, n = X.shape K = Y.shape[1] # 添加偏置项 X = np.hstack((np.ones((m, 1)), X)) # 初始化参数 theta = np.zeros((n + 1, K)) # 梯度下降迭代更新参数 for i in range(num_iterations): # 计算模型预测值 h = softmax(np.dot(X, theta)) # 计算损失函数的梯度 grad = np.dot(X.T, (h - Y)) # 更新参数 theta -= learning_rate * grad return theta# 示例用法X = np.array([[2.5, 3.0], [1.5, 2.2], [3.5, 2.5], [3.0, 3.5], [2.0, 2.5] [2.8, 2.1], [3.2, 3.0]])Y = np.array([[1, 0, 0], # 样本1属于类别1 [1, 0, 0], # 样本2属于类别1 [0, 1, 0], # 样本3属于类别2 [0, 1, 0], # 样本4属于类别2 [0, 0, 1], # 样本5属于类别3 [0, 0, 1]]) # 样本6属于类别3learning_rate = 0.1num_iterations = 1000theta = multiclass_logistic_regression(X, Y, learning_rate, num_iterations)print(&quot;Optimal parameters:&quot;)print(theta)在上面的代码中，我们首先定义了一个简单的样本集合 X 和对应的类别标签 Y。然后，我们选择了学习率和迭代次数，并调用 multiclass_logistic_regression 函数来训练模型并获得最优的参数矩阵 theta。最后，打印输出最优参数。请注意，这只是一个简单的示例，实际应用中可能需要更复杂的特征工程和模型选择来取得更好的结果。以下是代码的扩展，其中包括预测和评估步骤：def predict(X, theta): m, n = X.shape X = np.hstack((np.ones((m, 1)), X)) h = softmax(np.dot(X, theta)) return np.argmax(h, axis=1)def accuracy(y_pred, y_true): return np.mean(y_pred == y_true) * 100# 预测X_test = np.array([[1.8, 2.5], [3.6, 2.0], [2.7, 3.2]])y_true = np.array([0, 1, 2]) # 真实标签# 使用训练得到的参数进行预测y_pred = predict(X_test, theta)print(&quot;Predicted labels:&quot;, y_pred)print(&quot;True labels: &quot;, y_true)print(&quot;Accuracy: &quot;, accuracy(y_pred, y_true), &quot;%&quot;)在上面的代码中，我们定义了两个辅助函数：predict 用于根据训练得到的参数进行预测，accuracy 用于计算预测的准确率。在示例中，我们使用了一个测试集 X_test，并将其传入 predict 函数，得到预测的类别标签 y_pred。然后，我们将真实标签 y_true 与预测标签进行比较，并计算准确率。请注意，以上代码仅作为示例，实际应用中可能需要更多的数据预处理、交叉验证和模型调优来获取更可靠和准确的结果。" }, { "title": "机器学习基础-监督学习-逻辑回归之二元逻辑回归", "url": "/posts/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B9%8B%E4%BA%8C%E5%85%83%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-10 00:00:00 +0800", "snippet": "当我们面对一个二元分类问题时，二元逻辑回归是一种常用的监督学习算法。它基于线性回归模型，并使用逻辑函数（也称为 sigmoid 函数）对输出进行转换，以获得概率估计。以下是对二元逻辑回归的详细讲解，包括算法原理、公式和代码示例。算法原理给定一个训练集，其中包含输入特征向量 x 和相应的二元标签 y (0 或 1)，我们的目标是学习一个适当的模型，该模型能够根据输入特征预测出标签的概率。二元逻辑回归使用以下假设和模型表达式： 假设：给定输入特征 x，对应标签 y 的条件概率服从伯努利分布。 模型表达式：$h_\\theta(x)=\\frac{1}{1+e^{-\\theta^Tx}}$其中，$h_\\theta(x)$ 是模型的预测输出，表示预测输入 x 为正例的概率；$\\theta$ 是模型的参数向量。为了学习参数 $\\theta$，我们需要定义一个代价函数（损失函数），并通过最小化代价函数来优化参数。二元逻辑回归中常用的代价函数是对数损失函数（logistic loss）：\\[J(\\theta) = -\\frac{1}{m}\\textstyle\\sum_{m}^{i=1}[y^{(i)}\\log(h_\\theta(x^{(i)}))+(1-y^i)\\log(1-h_\\theta(x^{(i)}))]\\]其中，m 是训练样本的数量，$x^{(i)}$ 和 $y^{(i)}$ 分别表示第 i 个训练样本的输入特征和标签。为了最小化代价函数，可以使用梯度下降法或其他优化算法来更新参数 $\\theta$。代码示例下面是一个使用 Python 和 NumPy 实现二元逻辑回归的简单示例：import numpy as npdef sigmoid(z): return 1 / (1 + np.exp(-z))def logistic_regression(X, y, num_iterations, learning_rate): m, n = X.shape theta = np.zeros(n) for i in range(num_iterations): z = np.dot(X, theta) h = sigmoid(z) gradient = np.dot(X.T, (h - y)) / m theta -= learning_rate * gradient return theta# 示例数据X = np.array([[1, 3], [1, 4], [1, 5], [1, 2]])y = np.array([1, 1, 0, 0])# 添加偏置项X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)# 训练模型num_iterations = 1000learning_rate = 0.1theta = logistic_regression(X,y, num_iterations, learning_rate)# 使用训练好的参数进行预测def predict(X, theta):X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)predictions = sigmoid(np.dot(X, theta))return (predictions &amp;gt;= 0.5).astype(int)# 预测新样本new_data = np.array([[1, 6], [1, 1]])predictions = predict(new_data, theta)print(predictions)在上面的代码示例中，首先定义了一个 sigmoid 函数来进行激活函数的计算。然后，使用 logistic_regression 函数进行模型的训练，其中需要指定训练数据 X 和标签 y，以及迭代次数和学习率。训练过程中使用梯度下降法更新参数。最后，使用 predict 函数对新样本进行预测，并输出预测结果。请注意，这只是一个简单的示例，实际中可能需要更多的数据预处理、模型评估和调参等步骤来提高模型的性能和鲁棒性。" }, { "title": "线性回归之正则化", "url": "/posts/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%B9%8B%E6%AD%A3%E5%88%99%E5%8C%96/", "categories": "", "tags": "", "date": "2023-01-09 00:00:00 +0800", "snippet": "2—title: 机器学习基础-监督学习-线性回归之正则化categories: [机器学习]tags: [人工智能, 机器学习, 监督学习]math: true正则化是一种常用的机器学习技术，用于控制模型的复杂度，并防止过拟合。通过在目标函数中添加正则化项，可以使得模型倾向于选择较为简单的参数组合。在监督学习任务中，通常采用的是 L1 正则化和 L2 正则化两种形式。 L1 正则化（Lasso 正则化）L1 正则化通过在目标函数中添加参数的绝对值之和，来限制参数的大小。L1 正则化的数学表达式如下：Loss = 原始目标函数 + λ * Σ|θ_i|其中，Loss 表示加入 L1 正则化后的目标函数，λ 是正则化系数，θ_i 表示模型的第 i 个参数。L1 正则化的效果是使得一部分参数的值变为 0，从而实现特征选择的效果，能够降低模型的复杂度。由于 L1 正则化的特性，它可以用于稀疏性特征的选择和模型压缩。 L2 正则化（Ridge 正则化）L2 正则化通过在目标函数中添加参数的平方和，来限制参数的大小。L2 正则化的数学表达式如下：Loss = 原始目标函数 + λ * Σ(θ_i)^2其中，Loss 表示加入 L2 正则化后的目标函数，λ 是正则化系数，θ_i 表示模型的第 i 个参数。L2 正则化的效果是使得所有参数的值都相对较小，从而控制模型的复杂度和泛化能力。L2 正则化能够防止模型过拟合，并对异常值具有较好的鲁棒性。下面是一个使用 L2 正则化的线性回归模型的示例代码（使用 Python 和 Scikit-learn 库）：from sklearn.linear_model import Ridgefrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import mean_squared_errorfrom sklearn.datasets import load_bostonfrom sklearn.preprocessing import StandardScaler# 加载数据集boston = load_boston()X, y = boston.data, boston.target# 数据标准化scaler = StandardScaler()X = scaler.fit_transform(X)# 划分训练集和测试集X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)# 创建模型并训练ridge = Ridge(alpha=0.5) # 正则化系数alpha为0.5ridge.fit(X_train, y_train)# 在测试集上进行预测y_pred = ridge.predict(X_test)# 计算均方根误差mse = mean_squared_error(y_test, y_pred)print(&quot;均方根误差：&quot;, mse)在上述代码中，通过使用 Ridge 类来实现 L2 正则化的线性回归模型，其中 alpha 参数表示正则化系数。较大的 alpha 值会导致模型参数较小，从而降低过拟合的风险。需要注意的是，正则化的选择应该根据具体问题和数据集来进行调整，以获得更好的模型性能。" }, { "title": "机器学习基础-监督学习-线性回归之梯度下降法", "url": "/posts/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%B9%8B%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-09 00:00:00 +0800", "snippet": "梯度下降法（Gradient Descent）是一种常用的优化算法，用于求解损失函数的最小值。它通过迭代更新模型参数，沿着损失函数梯度的反方向逐步调整参数，直到达到最优解。下面是梯度下降法的详细讲解，包括算法原理、公式以及示例代码。算法原理： 初始化模型参数。 计算损失函数关于参数的梯度。 更新参数：将参数沿着梯度的反方向移动一定步长。 重复步骤 2 和 3，直到达到停止条件（例如达到最大迭代次数或损失函数变化小于某个阈值）。公式：假设有一个损失函数 J(θ)，其中 θ 表示模型的参数。梯度下降法的参数更新公式如下：θ = θ - α * ∇J(θ)其中，α 称为学习率（learning rate），控制参数更新的步长；∇J(θ)表示损失函数关于参数 θ 的梯度。示例代码：下面是使用梯度下降法求解线性回归模型的示例代码，其中使用均方误差（Mean Squared Error）作为损失函数：import numpy as np# 构造训练数据X = np.array([[1, 1], [1, 2], [1, 3], [1, 4]]) # 特征矩阵y = np.array([3, 4, 5, 6]) # 标签# 初始化参数theta = np.zeros(X.shape[1]) # 参数向量learning_rate = 0.01 # 学习率max_iterations = 1000 # 最大迭代次数tolerance = 1e-4 # 停止条件：损失函数变化小于阈值# 梯度下降迭代更新参数iteration = 0while iteration &amp;lt; max_iterations: # 计算损失函数关于参数的梯度 gradients = 2 * np.dot(X.T, np.dot(X, theta) - y) / len(y) # 更新参数 theta -= learning_rate * gradients # 计算损失函数 loss = np.mean((np.dot(X, theta) - y) ** 2) # 判断是否达到停止条件 if loss &amp;lt; tolerance: break iteration += 1print(&quot;参数(theta):&quot;, theta)print(&quot;损失函数:&quot;, loss)在上述代码中，我们使用了梯度下降法求解线性回归模型的参数 theta。通过迭代更新参数，直到达到停止条件（损失函数变化小于阈值），得到最优参数和最小化的损失函数。当涉及到梯度下降法时，还有几个关键点需要注意： 学习率（learning rate）：学习率决定了参数更新的步长，即每次迭代中参数更新的幅度。选择合适的学习率很重要，过大的学习率可能导致参数在最小值附近震荡或无法收敛，而过小的学习率可能导致收敛速度缓慢。通常需要通过实验和调参来选择合适的学习率。 批量梯度下降和随机梯度下降：在上述示例代码中，我们使用的是批量梯度下降（Batch Gradient Descent），即每次迭代都使用所有的训练样本来计算梯度和更新参数。相比之下，随机梯度下降（Stochastic Gradient Descent）每次迭代只使用一个样本来计算梯度和更新参数。随机梯度下降通常收敛速度更快，但会引入更多的随机性。 收敛性和局部最优解：梯度下降法不保证获得全局最优解，而只能保证收敛到局部最优解或鞍点。参数初始化、学习率和损失函数的形状等因素都会影响最终结果。有时候，为了克服局部最优解的问题，可以使用随机初始化多个不同的初始参数，运行梯度下降法多次，并选择具有最小损失函数的参数。 收敛判据：在示例代码中，我们使用了损失函数变化小于阈值作为停止条件。还可以使用其他停止条件，例如达到最大迭代次数、梯度的模小于阈值等。 特征缩放：在应用梯度下降法之前，进行特征缩放可以加快算法的收敛速度。特征缩放可以将不同特征的值范围缩放到相似的尺度，避免某些特征对梯度计算和参数更新的影响过大。 以上是关于梯度下降法的一些重要细节和注意事项。理解这些概念和原理，能够更好地应用梯度下降法来求解机器学习模型的参数。" }, { "title": "机器学习基础-监督学习-线性回归之最小二乘法", "url": "/posts/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%B9%8B%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-09 00:00:00 +0800", "snippet": "最小二乘法是一种常用的线性回归方法，用于拟合数据并找到最优的模型参数。它通过最小化实际观测值与模型预测值之间的残差平方和来确定最优参数。以下是对最小二乘法的详细讲解，包括公式和代码示例。 理论背景：假设我们有一个包含 n 个样本的数据集，每个样本包括 d 个特征和一个目标值。用 X 表示特征矩阵，y 表示目标值向量。线性回归模型的一般形式可以表示为：y = Xβ + ε，其中 β 是待估计的模型参数，ε 是误差项。 最小二乘法的目标： 最小二乘法的目标是找到最优的 β，使得残差平方和最小化。残差表示实际观测值与模型预测值之间的差异，即 ε = y - Xβ。最小二乘法通过最小化残差平方和来确定最优的 β，即 min   y - Xβ   ^2。 最小二乘法的解：最小二乘法的解可以通过求解正规方程（normal equation）得到。正规方程可以表示为：X^T X β = X^T y。通过求解这个方程，可以得到最优的 β 的闭式解。 代码示例（使用 Python 和 NumPy）：下面是一个使用最小二乘法进行线性回归的代码示例：import numpy as np# 生成样本数据X = np.array([[1, 1], [1, 2], [1, 3], [1, 4]])y = np.array([3, 4, 5, 6])# 计算最小二乘法解X_T = np.transpose(X)X_T_X = np.dot(X_T, X)X_T_y = np.dot(X_T, y)beta = np.linalg.solve(X_T_X, X_T_y)# 打印最优参数print(&quot;最优参数 beta：&quot;, beta)在上面的代码中，我们首先生成了一个简单的样本数据集，其中 X 是一个包含两个特征的矩阵，y 是对应的目标值向量。然后，我们通过计算正规方程的解，使用 np.linalg.solve()函数求解线性方程组，得到最优的 β 值。最后，打印出最优参数 beta。最小二乘法是线性回归中常用的方法之一，但在实际应用中也可能存在一些限制和假设。因此，在具体问题中，需要根据情况选择合适的回归方法和评估指标。当使用最小二乘法进行线性回归时，还可以通过计算残差、确定模型的拟合优度以及进行预测。 计算残差：在最小二乘法中，我们可以计算实际观测值与模型预测值之间的残差。残差可以用于评估模型的拟合效果。计算残差的公式为：ε = y - Xβ。# 计算残差residuals = y - np.dot(X, beta)print(&quot;残差：&quot;, residuals)在上述代码中，我们通过计算 y - Xβ 得到了残差。 模型的拟合优度：可以使用不同的指标来评估线性回归模型的拟合优度。常见的指标包括均方误差（MSE）、均方根误差（RMSE）和决定系数（R^2）等。# 均方误差（MSE）mse = np.mean(residuals**2)print(&quot;均方误差（MSE）：&quot;, mse)# 均方根误差（RMSE）rmse = np.sqrt(mse)print(&quot;均方根误差（RMSE）：&quot;, rmse)# 决定系数（R^2）ssr = np.sum((y - np.dot(X, beta))**2) # 回归平方和sst = np.sum((y - np.mean(y))**2) # 总平方和r2 = 1 - (ssr / sst)print(&quot;决定系数（R^2）：&quot;, r2)在上面的代码中，我们分别计算了均方误差（MSE）、均方根误差（RMSE）和决定系数（R^2）。这些指标可以帮助我们评估模型的拟合优度。 预测：在得到最优参数后，我们可以使用模型进行新数据的预测。假设我们有一个新的特征向量 x_new，使用模型预测对应的目标值 y_new 的方法如下：# 新数据预测x_new = np.array([1, 5]) # 新特征向量y_new = np.dot(x_new, beta) # 预测目标值print(&quot;新数据预测：&quot;, y_new)在上述代码中，我们使用最优参数 beta 对新的特征向量 x_new 进行预测，得到对应的目标值 y_new。最小二乘法是一种常用的线性回归方法，通过最小化残差平方和来找到最优的模型参数。通过计算残差、评估拟合优度以及进行预测，我们可以更全面地理解和应用最小二乘法。" }, { "title": "机器学习基础-监督学习-线性回归之多项式回归", "url": "/posts/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%B9%8B%E5%A4%9A%E9%A1%B9%E5%BC%8F%E5%9B%9E%E5%BD%92/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-09 00:00:00 +0800", "snippet": "多项式回归是一种线性回归的扩展，通过引入多项式特征来拟合非线性关系。下面详细讲解多项式回归的原理，并提供一个 Python 代码示例。多项式回归的模型形式为：\\[y = \\beta_0 + \\beta_1x + \\beta_2x^2 + ... + \\beta_nx^n + \\epsilon\\]其中，y 是目标变量，x 是输入特征，β₀, β₁, β₂, …, βₙ 是回归系数，ε 是误差项。多项式回归的步骤如下：数据准备：准备包含特征变量和目标变量的训练数据集。特征变换：将输入特征 x 转换为多项式特征。可以使用 NumPy 库中的 polyfit 函数或者 Scikit-learn 库中的 PolynomialFeatures 类来实现。模型训练：使用线性回归算法来拟合多项式特征。可以使用 Scikit-learn 库中的 LinearRegression 类来实现。模型评估：使用评估指标（如均方误差）来评估模型的性能。下面是一个使用 Scikit-learn 库进行多项式回归的 Python 代码示例：import numpy as npfrom sklearn.preprocessing import PolynomialFeaturesfrom sklearn.linear_model import LinearRegressionfrom sklearn.metrics import mean_squared_errorimport matplotlib.pyplot as plt# 准备示例数据X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1) # 输入特征y = np.array([2, 8, 18, 32, 50]) # 目标变量# 转换为多项式特征poly = PolynomialFeatures(degree=2) # 设置多项式的阶数X_poly = poly.fit_transform(X)# 拟合多项式回归模型model = LinearRegression()model.fit(X_poly, y)# 预测X_test = np.array([6]).reshape(-1, 1) # 测试数据X_test_poly = poly.transform(X_test)y_pred = model.predict(X_test_poly)print(&quot;预测结果:&quot;, y_pred)# 绘制拟合曲线X_grid = np.arange(min(X), max(X), 0.1).reshape(-1, 1)X_grid_poly = poly.transform(X_grid)y_pred_grid = model.predict(X_grid_poly)plt.scatter(X, y, color=&#39;red&#39;, label=&#39;实际数据&#39;)plt.plot(X_grid, y_pred_grid, color=&#39;blue&#39;, label=&#39;拟合曲线&#39;)plt.title(&#39;多项式回归&#39;)plt.xlabel(&#39;X&#39;)plt.ylabel(&#39;y&#39;)plt.legend()plt.show()在上述代码中，我们首先使用 PolynomialFeatures 类将输入特征 X 转换为二次多项式特征 X_poly。然后使用 LinearRegression 类来拟合多项式特征和目标变量 y。最后，我们使用拟合的模型对新的输入特征 X_test 进行预测，并绘制了实际数据和拟合曲线的散点图和线性图。这个示例代码中，我们使用二次多项式特征（degree=2），因此模型会拟合一个二次曲线来适应数据。你可以根据需要调整 degree 的值来尝试更高阶的多项式回归。需要注意的是，多项式回归也有可能出现过拟合的情况，当多项式的阶数过高时，模型可能过于复杂，对训练数据拟合得很好，但在新的数据上的表现可能较差。因此，在应用多项式回归时，需要权衡模型的复杂性和数据的拟合程度。" }, { "title": "机器学习基础-监督学习-线性回归之多元线性回归", "url": "/posts/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%B9%8B%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-09 00:00:00 +0800", "snippet": "多元线性回归是一种在多个自变量之间建立线性关系的监督学习方法。它可以用于预测一个或多个连续的因变量。下面将详细介绍多元线性回归的原理，并提供一个 Python 代码示例。原理：多元线性回归的目标是建立一个线性模型，该模型可以通过多个自变量的线性组合来预测因变量。模型的一般形式可以表示为：\\[y = \\beta0 + \\beta{1}\\cdot1 + \\beta2\\cdot2 + ... + \\beta n\\cdot cn + ε\\]其中，y 是因变量，x1, x2, …, xn 是自变量，β0, β1, β2, …, βn 是模型的系数，ε 是误差项。我们的目标是找到最佳的系数，使得模型对观测数据的拟合最好。拟合模型的过程通常使用最小二乘法，目标是最小化观测值与预测值之间的平方差。系数的最优解可以使用正规方程（Normal Equation）求解，公式如下：\\[\\beta = (X^T \\cdot X)^{-1}\\cdot X^T\\cdot y\\]其中，β 是包含所有系数的向量，X 是包含所有自变量的矩阵，y 是观测值的向量，$^T$ 表示矩阵的转置，$^{-1}$ 表示矩阵的逆。代码示例：以下是一个使用 Python 进行多元线性回归的示例代码：import numpy as npfrom sklearn.linear_model import LinearRegression# 构造输入特征矩阵X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])# 构造对应的因变量y = np.array([10, 20, 30])# 创建线性回归模型对象model = LinearRegression()# 拟合模型model.fit(X, y)# 打印系数和截距print(&quot;Coefficients:&quot;, model.coef_)print(&quot;Intercept:&quot;, model.intercept_)# 进行预测new_data = np.array([[2, 3, 4], [5, 6, 7]])predictions = model.predict(new_data)print(&quot;Predictions:&quot;, predictions)在上面的代码中，我们使用了 NumPy 库构造了一个 3x3 的特征矩阵 X 和对应的因变量 y。然后，我们使用 LinearRegression 类创建了一个线性回归模型对象。调用 fit 方法对模型进行拟合，得到了系数和截距。最后，我们使用模型进行了预测，输出了预测结果。这只是一个简单的示例，实际应用中可能需要更多的数据和特征。当进行多元线性回归时，需要注意以下几点： 数据预处理：确保数据清洁、无缺失值，并进行必要的特征缩放或归一化。这可以提高模型的收敛速度和准确性。 多重共线性：多元线性回归要求自变量之间没有高度相关性，即不存在多重共线性。可以通过计算自变量之间的相关系数矩阵或使用方差膨胀因子（Variance Inflation Factor，VIF）来检测多重共线性。 模型评估：使用评估指标来评估模型的性能，例如均方误差（Mean Squared Error，MSE）、决定系数（Coefficient of Determination，R²）等。这可以帮助你了解模型的拟合程度和预测能力。 特征选择：在多元线性回归中，可以通过特征选择方法来选择最重要的自变量，以避免过拟合或提高模型解释能力。常见的方法包括前向选择、后向消除和逐步回归。 模型诊断：检查残差是否符合模型假设（如误差项服从正态分布、残差独立等），以确保模型的有效性和准确性。可以使用残差分析、Q-Q 图、残差图等方法来诊断模型。 可解释性和解释变量的选择：在解释模型结果时，考虑哪些自变量对因变量具有显著影响，并解释系数的含义。这有助于理解模型背后的关系和推断。 以上是多元线性回归的一般流程和注意事项。实际应用中，可以根据具体问题和数据特点进行调整和扩展。" }, { "title": "机器学习基础-监督学习-目标函数之感知器损失（Perceptron Loss）", "url": "/posts/%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0%E4%B9%8B%E6%84%9F%E7%9F%A5%E5%99%A8%E6%8D%9F%E5%A4%B1/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-08 00:00:00 +0800", "snippet": "感知器损失（Perceptron Loss）是一种用于二分类问题的目标函数，基于感知器算法（Perceptron Algorithm）。感知器算法是一种简单的线性分类算法，其目标是找到一个线性超平面，将正负样本正确地分开。感知器损失函数定义如下：\\[L(y,\\hat y) = \\max(0,-y\\cdot\\hat{y})\\]其中，y 表示真实标签（正样本为+1，负样本为-1），$\\hat{y}$ 表示模型对样本的预测值。感知器损失函数可以用来训练一个二分类的感知器模型，具体的优化算法可以使用随机梯度下降（Stochastic Gradient Descent）来更新模型参数。以下是使用 Python 实现感知器损失的示例代码：import numpy as npdef perceptron_loss(y_true, y_pred): N = len(y_true) losses = np.maximum(0, -y_true * y_pred) loss = np.sum(losses) / N return loss在这个示例中，y_true 是一个包含真实标签的 NumPy 数组，y_pred 是一个包含模型对样本的预测值的 NumPy 数组。函数使用了 NumPy 的向量化操作，计算每个样本的感知器损失，并返回平均损失。需要注意的是，感知器损失是一个非连续的、不可导的函数。因此，在实践中，感知器算法一般使用梯度下降等优化算法的变体来逼近最小化感知器损失的目标。在每一步迭代中，根据当前样本的特征向量和真实标签来更新模型参数，以使损失函数减小。如果预测值与真实标签之间的乘积小于零，即两者异号，说明预测错误，损失为正。如果两者同号，即预测正确，损失为零。因此，感知器损失函数的目标是最小化预测错误的样本。以下是使用 Python 实现感知器算法的示例代码：import numpy as npclass Perceptron: def __init__(self, learning_rate=0.1, max_epochs=100): self.learning_rate = learning_rate self.max_epochs = max_epochs self.weights = None self.bias = None def train(self, X, y): num_samples, num_features = X.shape self.weights = np.zeros(num_features) self.bias = 0 for epoch in range(self.max_epochs): errors = 0 for i in range(num_samples): x = X[i] y_true = y[i] y_pred = self.predict(x) if y_true * y_pred &amp;lt;= 0: self.weights += self.learning_rate * y_true * x self.bias += self.learning_rate * y_true errors += 1 if errors == 0: break def predict(self, x): linear_output = np.dot(self.weights, x) + self.bias return np.sign(linear_output)# 使用示例X = np.array([[2, 3], [1, 2], [4, 5], [6, 1]])y = np.array([1, 1, -1, -1])perceptron = Perceptron()perceptron.train(X, y)test_samples = np.array([[3, 4], [5, 2]])for sample in test_samples: prediction = perceptron.predict(sample) print(&quot;Sample:&quot;, sample) print(&quot;Prediction:&quot;, prediction)在示例中，首先定义了一个 Perceptron 类，包含了训练和预测的方法。然后，使用示例数据训练感知器模型的训练过程。训练过程中，通过迭代数据集中的样本，根据预测结果和真实标签来更新模型的权重和偏置。训练过程中的关键部分是 train 方法。在每个训练迭代中，遍历数据集中的样本，计算预测值 y_pred，如果 y_true * y_pred &amp;lt;= 0，说明预测错误，更新权重和偏置。权重的更新使用梯度下降的思想，按照梯度的方向进行调整，乘以学习率 learning_rate 来控制更新的步长。最后，通过 predict 方法对新样本进行预测，计算线性输出值 linear_output，并通过 np.sign()函数将其转化为预测结果（+1 或-1）。以上是感知器算法的简单示例代码，用于说明感知器损失函数的应用。需要注意的是，感知器算法适用于线性可分的二分类问题，对于非线性可分的问题可能无法收敛或达到较好的性能。在实际应用中，更复杂的分类算法和目标函数通常被使用。" }, { "title": "机器学习基础-监督学习-目标函数之平均绝对误差（Mean Absolute Error，MAE）", "url": "/posts/%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0%E4%B9%8B%E5%B9%B3%E5%9D%87%E7%BB%9D%E5%AF%B9%E8%AF%AF%E5%B7%AE/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-08 00:00:00 +0800", "snippet": "平均绝对误差（Mean Absolute Error，MAE）是一种用于衡量回归问题中预测值与真实值之间平均绝对差异的目标函数。它可以衡量预测值与真实值之间的平均误差大小，具有较好的鲁棒性。下面详细讲解 MAE，并提供一个示例代码和公式。MAE 的计算公式如下：\\[MAE = \\frac{1}{N}\\sum_{i=1}^{N}\\left \\vert y_i-\\hat y_i \\right \\vert\\]其中，N 表示样本数量，$y_i$ 是真实值，$\\hat y_i$ 是预测值。MAE 的值越小，表示预测值与真实值之间的平均差异越小，即预测的准确性越高。下面是一个使用 Python 计算 MAE 的示例代码：import numpy as npdef mean_absolute_error(y_true, y_pred): N = len(y_true) mae = np.sum(np.abs(y_true - y_pred)) / N return mae在示例代码中，y_true 是一个 NumPy 数组，表示真实值；y_pred 是一个 NumPy 数组，表示预测值。代码通过求取绝对值差异的平均值来计算 MAE。使用示例代码，你可以计算实际问题中的 MAE，对于给定的真实值和预测值。当计算平均绝对误差（MAE）时，可以使用不同的编程语言和库来实现。以下是另一个示例，使用 Scikit-learn 库来计算 MAE。from sklearn.metrics import mean_absolute_error# 示例真实值和预测值y_true = [2.5, 1.5, 3.0, 2.1, 3.6]y_pred = [2.0, 1.8, 2.5, 2.2, 3.2]# 计算MAEmae = mean_absolute_error(y_true, y_pred)print(&quot;MAE:&quot;, mae)在这个示例中，y_true 是一个包含真实值的列表，y_pred 是一个包含对应预测值的列表。通过调用 mean_absolute_error 函数，并传递真实值和预测值，可以计算出 MAE 的值。需要注意的是，Scikit-learn 库还提供了其他回归性能度量的函数，例如均方误差（Mean Squared Error，MSE）、R 平方（R-squared）等，你可以根据需要选择合适的度量来评估回归模型的性能。无论是使用自定义代码还是使用现有库，计算 MAE 都是一种简单且常用的评估回归模型性能的方法。它衡量了预测值与真实值之间的平均差异，可以帮助你评估模型的准确性和精度。" }, { "title": "机器学习基础-监督学习-目标函数之均方误差（Mean Squared Error，MSE）", "url": "/posts/%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0%E4%B9%8B%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-08 00:00:00 +0800", "snippet": "均方误差（Mean Squared Error，MSE）是在回归问题中常用的目标函数，用于衡量预测值与真实值之间的平均差异。下面将详细讲解均方误差，并提供相应的公式和代码示例。在回归问题中，假设有 N 个样本，其中 $y_i$ 表示第 i 个样本的真实值，$\\hat{y}_i$ 表示对第 i 个样本的预测值。那么均方误差（MSE）可以通过以下公式计算：\\[MSE = \\frac{1}{N}\\sum_{i=1}^{1}(y_i-\\hat y_i)^2\\]公式中，首先计算每个样本的预测误差（即真实值与预测值之间的差），然后将所有样本的误差平方求和，并除以样本数量 N，最后得到平均误差。以下是一个使用 Python 编写的计算均方误差的示例函数：import numpy as npdef mean_squared_error(y_true, y_pred): N = len(y_true) # 样本数量 mse = np.sum((y_true - y_pred) ** 2) / N # 计算平方差的平均值 return mse函数接受两个参数：y_true 表示真实值的数组，y_pred 表示预测值的数组。它首先计算真实值与预测值之间的差的平方，然后求和并除以样本数量 N，得到均方误差（MSE）。使用上述函数计算均方误差的示例代码：y_true = np.array([1, 2, 3, 4, 5]) # 真实值数组y_pred = np.array([1.2, 2.3, 3.5, 4.1, 5.2]) # 预测值数组mse = mean_squared_error(y_true, y_pred)print(&quot;Mean Squared Error:&quot;, mse)以上示例中，我们假设 y_true 和 y_pred 分别表示真实值和预测值的 NumPy 数组。函数 mean_squared_error 计算并返回均方误差，最后打印输出结果。通过计算均方误差，我们可以评估回归模型的性能，数值越小表示预测结果与真实值的拟合程度越好。当使用均方误差作为目标函数时，通常会将其最小化，以使模型的预测结果与真实值之间的差异最小化。在训练过程中，通过调整模型的参数来最小化均方误差。一种常见的方法是使用梯度下降法（Gradient Descent）来更新模型参数。梯度下降法是一种优化算法，用于寻找目标函数的最小值。在均方误差的情况下，我们希望找到使均方误差最小化的模型参数。以下是梯度下降法的一般步骤： 初始化模型参数：给定模型参数的初始值，通常使用随机初始化。 前向传播：使用当前模型参数对训练样本进行前向传播，计算预测值。 计算误差：计算预测值与真实值之间的误差。 反向传播：根据误差，计算模型参数对误差的梯度。 参数更新：使用梯度下降法更新模型参数，使目标函数的值减小。 重复步骤 2-5，直到达到停止条件（例如达到最大迭代次数或误差收敛）。 下面是一个使用梯度下降法训练线性回归模型并计算均方误差的示例代码：import numpy as npdef gradient_descent(X, y, learning_rate=0.01, num_iterations=1000): N, d = X.shape # 样本数量和特征维度 theta = np.zeros(d) # 初始化模型参数 for i in range(num_iterations): y_pred = np.dot(X, theta) # 前向传播，计算预测值 error = y_pred - y # 计算误差 # 计算梯度并更新参数 gradient = np.dot(X.T, error) / N theta -= learning_rate * gradient return theta# 生成样本数据X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) # 特征矩阵y = np.array([2, 5, 8]) # 真实值# 添加偏置项X = np.c_[np.ones(X.shape[0]), X]# 使用梯度下降法训练模型theta = gradient_descent(X, y)# 计算预测值y_pred = np.dot(X, theta)# 计算均方误差mse = mean_squared_error(y, y_pred)print(&quot;Mean Squared Error:&quot;, mse)在上述示例中，我们使用梯度下降法来训练一个简单的线性回归模型。首先，我们生成了样样本数据 X 和真实值 y。然后，我们将特征矩阵 X 添加了偏置项（全为 1 的列），并调用 gradient_descent 函数进行模型训练。该函数使用梯度下降法来更新模型参数 theta，使均方误差最小化。最后，我们计算预测值 y_pred，并使用 mean_squared_error 函数计算均方误差 mse。最终打印输出均方误差的值。通过梯度下降法的迭代过程，模型参数逐步更新，使得均方误差逐渐减小，最终达到收敛。这样训练出的模型参数可以用于对新样本进行预测。请注意，上述代码示例是一个简化的线性回归模型的训练过程，并且没有包括一些优化技巧和细节（如学习率调整、批量梯度下降等）。在实际应用中，可能需要更复杂的模型和更高级的优化算法来处理不同的问题。" }, { "title": "机器学习基础-监督学习-目标函数之均方根误差（Root Mean Squared Error，RMSE）", "url": "/posts/%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0%E4%B9%8B%E5%9D%87%E6%96%B9%E6%A0%B9%E8%AF%AF%E5%B7%AE/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-08 00:00:00 +0800", "snippet": "均方根误差（Root Mean Squared Error，RMSE）是一种广泛应用于回归问题的评估指标，它衡量了预测值与真实值之间的平均差异。与均方误差（MSE）相比，RMSE 对误差进行了平方根运算，这样使得 RMSE 的单位与预测值和真实值的单位保持一致。RMSE 的计算步骤如下： 对每个样本，计算预测值与真实值之间的差值。 对每个差值进行平方运算。 对所有平方差值进行求和。 将总和除以样本数量。 对结果进行平方根运算，得到 RMSE。公式表示为：\\[RMSE = \\sqrt[]{\\frac{1}{N}\\sum_{i=1}^{N}(y_i-\\hat y_i)^2}\\]其中，N 表示样本数量，y_i 表示真实值，$y_hat_i$ 表示预测值。下面是一个用 Python 实现 RMSE 计算的示例代码：import numpy as npdef root_mean_squared_error(y_true, y_pred): N = len(y_true) mse = np.sum((y_true - y_pred) ** 2) / N rmse = np.sqrt(mse) return rmse这段代码假设 y_true 和 y_pred 是 NumPy 数组，分别表示真实值和预测值。函数首先计算均方误差（MSE），然后将其结果进行平方根运算，最后得到 RMSE。通过调用这个函数，可以计算模型在回归问题中的 RMSE 评估指标。请注意，在使用 RMSE 时，要注意与数据的单位相匹配，以确保结果的可解释性和比较性。当使用均方根误差（RMSE）作为评估指标时，其值越小表示模型的预测结果与真实值的拟合程度越好。RMSE 的优点在于对异常值的敏感度较低，因为平方操作会放大大于 1 的误差。除了计算 RMSE 之外，还可以对其进行比较，以帮助选择最佳模型或调整模型的超参数。例如，可以计算不同模型在相同数据集上的 RMSE，选择具有最低 RMSE 的模型作为最佳选择。下面是一个示例，演示如何使用 RMSE 进行模型选择：import numpy as npfrom sklearn.model_selection import train_test_splitfrom sklearn.linear_model import LinearRegressionfrom sklearn.metrics import mean_squared_error# 准备数据X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)y = np.array([2, 4, 6, 8, 10])# 划分训练集和测试集X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)# 训练模型model = LinearRegression()model.fit(X_train, y_train)# 在测试集上进行预测y_pred = model.predict(X_test)# 计算RMSErmse = np.sqrt(mean_squared_error(y_test, y_pred))print(&quot;RMSE:&quot;, rmse)在这个示例中，我们使用线性回归模型对数据进行训练，并在测试集上进行预测。然后，计算预测值与真实值之间的均方根误差（RMSE）。较低的 RMSE 值表示模型的预测结果较好。使用 RMSE 作为评估指标时，需要注意一些限制。例如，RMSE 对异常值较不敏感，因此在存在异常值的情况下，可以考虑使用其他评估指标进行综合分析。此外，对于不同规模的数据，RMSE 的值也可能不具有可比性，因此在进行模型比较时，应注意数据的归一化或标准化处理。总之，均方根误差（RMSE）是一种常用的回归模型评估指标，通过衡量预测结果与真实值之间的平均差异来评估模型的拟合能力。" }, { "title": "机器学习基础-监督学习-目标函数之余弦相似度损失（Cosine Similarity Loss）", "url": "/posts/%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0%E4%B9%8B%E4%BD%99%E5%BC%A6%E7%9B%B8%E4%BC%BC%E5%BA%A6%E6%8D%9F%E5%A4%B1/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-08 00:00:00 +0800", "snippet": "余弦相似度损失（Cosine Similarity Loss）用于衡量两个向量之间的余弦相似度，常用于度量两个向量的相似程度。余弦相似度是通过计算两个向量的夹角来度量它们之间的相似度，取值范围为[-1, 1]，值越接近 1 表示相似度越高。余弦相似度损失可以用于一些任务，如推荐系统中的相似性匹配、聚类算法中的样本相似性度量等。余弦相似度（Cosine Similarity）:\\[similarity = \\frac{u \\cdot v}{||u|| \\cdot ||v||}\\]其中，u 和 v 分别表示两个向量。余弦相似度损失（Cosine Similarity Loss）:\\[loss = \\frac{1}{N}\\sum_{i=1}^{N}(1-\\frac{u_i \\cdot v_i}{||u_i|| \\cdot ||v_i||})\\]其中，N 表示样本数量，u_i 和 v_i 分别表示第 i 个样本的向量。Python 代码示例：import numpy as npdef cosine_similarity(u, v): dot_product = np.dot(u, v) norm_u = np.linalg.norm(u) norm_v = np.linalg.norm(v) similarity = dot_product / (norm_u * norm_v) return similaritydef cosine_similarity_loss(u, v): N = len(u) loss = 1 - np.sum(np.dot(u, v) / (np.linalg.norm(u, axis=1) * np.linalg.norm(v, axis=1))) / N return loss当使用余弦相似度作为相似性度量时，可以通过计算向量之间的余弦相似度来评估它们之间的相似程度。余弦相似度损失的公式表示了在一组向量中计算平均相似度的过程。在上述代码示例中，cosine_similarity 函数计算了两个向量的余弦相似度，通过计算两个向量的点积，以及它们的范数来获得余弦相似度的值。cosine_similarity_loss 函数计算了一组向量的平均余弦相似度损失。通过将每个样本的余弦相似度与 1 进行差异化，然后取平均值来得到损失值。其中，np.linalg.norm 函数用于计算向量的范数。使用这些函数，你可以传入向量或向量组来计算余弦相似度和余弦相似度损失。以下是一个使用示例：u = np.array([1, 2, 3])v = np.array([2, 4, 6])similarity = cosine_similarity(u, v)loss = cosine_similarity_loss(u, v)print(&quot;Cosine Similarity:&quot;, similarity)print(&quot;Cosine Similarity Loss:&quot;, loss)输出结果：Cosine Similarity: 1.0Cosine Similarity Loss: 0.0在此示例中，向量 u 和 v 是相同方向的，因此它们的余弦相似度为 1，损失为 0。请注意，上述代码示例中的向量是一维的，适用于表示特征向量或简单的数值向量。如果处理的是多维数据，需要对代码进行适当修改。" }, { "title": "机器学习基础-监督学习-目标函数之交叉熵损失（Cross-Entropy Loss）", "url": "/posts/%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0%E4%B9%8B%E4%BA%A4%E5%8F%89%E7%86%B5%E8%AF%AF%E5%B7%AE/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-08 00:00:00 +0800", "snippet": "交叉熵损失（Cross-Entropy Loss）是一种常用的目标函数，主要用于二分类或多分类问题中衡量预测结果与真实标签之间的差异。它是基于信息论的概念，通过比较两个概率分布的差异来计算损失。在二分类问题中，假设有两个类别，真实标签可以表示为 $y \\in {0,1}$，而模型的预测概率为 $y \\in [0,1]$。交叉熵损失通过计算真实标签的对数概率与预测概率的乘积来衡量预测结果与真实标签之间的差异。公式如下所示：\\[CrossEntropy = -\\frac{1}{N}\\sum_{i=1}^{N}(y_i\\log(\\hat y_i) + (1-y_i)\\log(1-\\hat y_i))\\]其中，N 表示样本数量，yi 表示第 i 个样本的真实标签（0 或 1），$\\hat y_i$ 表示第 i 个样本的预测概率。在多分类问题中，交叉熵损失可以推广为多个类别的情况。假设有 K 个类别，真实标签可以表示为 $y \\in {0,1…,,k-1}$，而模型的预测概率为 $\\hat y \\in [0,1]^k$。交叉熵损失的公式如下：\\[CrossEntropy = -\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{k=0}^{K-1}y_{ik}\\log(\\hat y_{ik})\\]其中，N 表示样本数量，$y_{ik}$ 表示第 i 个样本属于第 k 个类别的真实标签，$\\hat y_{ik}$ 表示第 i 个样本属于第 k 个类别的预测概率。以下是 Python 代码示例，用于计算交叉熵损失：import numpy as npdef cross_entropy_loss(y_true, y_pred): N = len(y_true) epsilon = 1e-15 # 用于防止取对数时出现无穷大值 ce_loss = -np.sum(y_true * np.log(y_pred + epsilon)) / N return ce_loss在示例代码中，y_true 是真实标签的数组，y_pred 是预测概率的数组。使用 NumPy 库计算了交叉熵损失，其中添加了一个很小的值 epsilon 以避免在计算对数时出现无穷大的情况。需要注意的是，在实际应用中，为了数值稳定性和避免过拟合，通常会对交叉熵损失函数添加正则化项或其他改进措施。此外，还可以使用优化算法（例如梯度下降）来最小化交叉熵损失，以获得更好的模型预测结果。" }, { "title": "机器学习基础-监督学习-目标函数之KL散度（Kullback-Leibler Divergence）", "url": "/posts/%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0%E4%B9%8BKL%E6%95%A3%E5%BA%A6/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-08 00:00:00 +0800", "snippet": "KL 散度（Kullback-Leibler Divergence），也称为相对熵（Relative Entropy），是用来衡量两个概率分布之间的差异。KL 散度常用于信息论和统计学中，它可以用来比较两个概率分布之间的相似性或差异程度。对于两个离散概率分布 P 和 Q，它们的 KL 散度定义如下：\\[KL(P||Q) = \\sum_{i}P(i)\\log\\left(\\frac{P(i)}{Q(i)}\\right)\\]其中，P(i)和 Q(i)分别表示分布 P 和 Q 在第 i 个事件上的概率。对于连续概率分布的情况，KL 散度的计算稍有不同：\\[KL(P||Q) = \\int P(x)\\log\\left(\\frac{P(x)}{Q(x)}\\right)dx\\]KL 散度有以下特点： KL 散度始终为非负值，当且仅当两个概率分布完全相同时取得最小值 0。 KL 散度不具备对称性，即 KL(P   Q)与 KL(Q   P)的值可能不相等。 以下是一个简单的 Python 代码示例，用于计算两个离散概率分布的 KL 散度：import numpy as npdef kl_divergence(p, q): kl = np.sum(p * np.log(p / q)) return kl这里假设 p 和 q 是 NumPy 数组，分别表示两个离散概率分布。代码中使用了 NumPy 的数组运算，计算出每个事件上的 KL 散度，并求和得到总的 KL 散度。需要注意的是，在实际应用中，计算 KL 散度时需要确保分母概率不为零，可以通过添加一个小的平滑项来避免分母为零的情况。当对应的概率分布是连续的时候，KL 散度的计算需要进行积分。下面是一个简单的代码示例，用于计算两个连续概率分布的 KL 散度：import numpy as npfrom scipy.integrate import quaddef kl_divergence_continuous(p, q, lower_limit, upper_limit): def integrand(x): return p(x) * np.log(p(x) / q(x)) kl, _ = quad(integrand, lower_limit, upper_limit) return kl在这个例子中，假设 p 和 q 是函数，分别表示两个连续概率分布。函数 integrand 定义了被积函数，其中 x 是积分变量。通过使用 scipy.integrate 库的 quad 函数进行积分，计算出两个概率分布之间的 KL 散度。需要注意的是，在实际应用中，积分的上下限需要根据实际情况进行设置，确保积分范围覆盖了概率分布的支持区间。总结：KL 散度是衡量两个概率分布之间差异的重要指标。它可用于比较两个离散或连续概率分布的相似性或差异程度。代码示例演示了如何计算离散和连续概率分布之间的 KL 散度，但在实际应用中，可能需要根据具体情况进行适当的修改和调整。" }, { "title": "机器学习基础-监督学习-目标函数之Hinge损失（Hinge Loss）", "url": "/posts/%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0%E4%B9%8BHinge%E6%8D%9F%E5%A4%B1/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-08 00:00:00 +0800", "snippet": "Hinge 损失（Hinge Loss）通常用于支持向量机（Support Vector Machine，SVM）算法中的分类问题。它鼓励正确分类的边界离样本更远，同时惩罚错误分类的边界。下面将详细讲解 Hinge 损失的定义和应用，并提供相应的公式和代码示例。Hinge 损失函数的定义如下：\\[\\text{Hinge Loss} = \\max(0,1-y_i\\cdot\\hat{y}_i)\\]其中，$y_i$ 表示样本的真实标签（-1 或 1），$\\hat{y}_i$ 表示模型对样本的预测结果。Hinge 损失函数的含义是，如果预测结果与真实标签之间的乘积大于 1，则损失为 0；否则，损失为 1 减去预测结果与真实标签之间的乘积。Hinge 损失在 SVM 中起到了两个作用： 对于正确分类的样本，使得预测结果与真实标签之间的乘积尽可能大于 1，从而鼓励正确分类的边界离样本更远。 对于错误分类的样本，预测结果与真实标签之间的乘积小于等于 1，损失非零，惩罚错误分类的边界。对于多类别问题，通常使用 One-vs-All（OvA）方法将其转化为多个二分类问题，然后对每个类别使用 Hinge 损失进行训练。以下是 Python 代码示例，计算一组样本的 Hinge 损失：import numpy as npdef hinge_loss(y_true, y_pred): loss = np.maximum(0, 1 - y_true * y_pred) return loss# 示例数据y_true = np.array([-1, 1, 1, -1])y_pred = np.array([0.5, 0.8, -0.3, -0.7])h_loss = hinge_loss(y_true, y_pred)print(h_loss)在上述示例中，y_true 表示真实标签，y_pred 表示模型的预测结果。通过调用 hinge_loss 函数，计算出每个样本的 Hinge 损失，并打印输出。需要注意的是，上述代码示例假设 y_true 和 y_pred 是 NumPy 数组，且具有相同的长度。Hinge 损失在支持向量机算法中的应用如下： 支持向量机分类器训练：在支持向量机中，通过最小化目标函数来学习最优的分类超平面。目标函数由 Hinge 损失和正则化项组成，其中 Hinge 损失用于度量样本的分类误差，正则化项用于控制模型的复杂度。通过最小化目标函数，可以得到一个最优的超平面，使得正确分类的样本离超平面尽可能远，并且最大化间隔。 多类别分类：Hinge 损失也可以应用于多类别分类问题。在这种情况下，通常采用一对多（One-vs-Rest）或一对一（One-vs-One）的策略。对于一对多策略，每个类别与其他所有类别形成一个二分类问题。对于一对一策略，每个类别之间形成一个二分类问题。然后，对每个二分类问题使用 Hinge 损失进行训练。以下是一个示例，展示如何在支持向量机算法中使用 Hinge 损失进行二分类训练：from sklearn.svm import SVCfrom sklearn.datasets import make_classificationfrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import accuracy_score# 生成示例数据X, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, random_state=42)# 划分训练集和测试集X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)# 创建SVC对象，并使用Hinge损失进行训练svm = SVC(kernel=&#39;linear&#39;, loss=&#39;hinge&#39;)svm.fit(X_train, y_train)# 预测测试集y_pred = svm.predict(X_test)# 计算准确率accuracy = accuracy_score(y_test, y_pred)print(&quot;Accuracy:&quot;, accuracy)在上述示例中，首先使用 make_classification 函数生成了一个二分类的示例数据集。然后，将数据集划分为训练集和测试集。接下来，创建了一个 SVC 对象，指定 kernel=’linear’以使用线性核函数，loss=’hinge’以使用 Hinge 损失进行训练。最后，使用训练好的模型对测试集进行预测，并计算准确率。这是一个简单的示例，演示了如何在支持向量机中使用 Hinge 损失进行二分类训练。实际应用中，可以根据具体问题和数据的特点来选择合适的模型和参数设置。" }, { "title": "机器学习基础-监督学习-损失函数", "url": "/posts/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-07 00:00:00 +0800", "snippet": "损失函数（Loss Function）是在监督学习任务中用于衡量模型预测结果与真实标签之间的差异的函数。其目标是最小化预测结果与真实值之间的差异，从而使模型能够更好地拟合训练数据。以下是几种常见的损失函数及其公式： 均方误差（Mean Squared Error，MSE）：均方误差是回归任务中常用的损失函数，计算预测值与真实值之间的平方差的平均值。公式：\\[MSE = \\frac{1}{N}\\sum_{i=1}^N(y_i-\\hat y_i)^2\\]import numpy as npdef mean_squared_error(y_true, y_pred): return np.mean((y_true - y_pred) ** 2) 交叉熵损失（Cross-Entropy Loss）：交叉熵损失是分类任务中常用的损失函数，用于衡量预测概率分布与真实标签之间的差异。二分类交叉熵公式：\\[\\text{Binary Cross-Entropy} = -(y\\log(\\hat y)+(1-y)\\log(1-\\hat y))\\]多分类交叉熵公式：\\[\\text{Categorical Cross-Entropy} = -\\sum_{i=1}^{C}y_i\\log(\\hat y_i)\\]Python 代码示例：import numpy as npdef binary_cross_entropy(y_true, y_pred): return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))def categorical_cross_entropy(y_true, y_pred): return -np.sum(y_true * np.log(y_pred)) Hinge 损失：Hinge 损失是支持向量机（SVM）中常用的损失函数，主要用于二分类任务。公式：\\[1-y\\hat y\\]Python 代码示例：def hinge_loss(y_true, y_pred): return np.maximum(0, 1 - y_true * y_pred)这些是常见的损失函数示例。在实际应用中，根据具体任务的特点和需求，可以选择合适的损失函数来优化模型。需要注意的是，以上是基于数学公式的损失函数定义，实际应用中可能需要根据具体的框架或库来实现相应的代码。示例代码中的 y_true 表示真实标签，y_pred 表示模型的预测值。优势 直观性：损失函数可以提供对模型预测与真实标签之间的差异的直观度量。不同的损失函数可以针对不同的任务目标提供直观和可解释的结果。 梯度计算：损失函数通常可微分，这使得可以使用梯度下降等优化算法来最小化损失函数。梯度提供了指导模型参数更新的方向，有助于模型的收敛和优化。 适应性：不同类型的任务需要不同的损失函数。例如，回归任务常用均方误差损失，分类任务常用交叉熵损失。选择适当的损失函数可以使模型更好地适应具体任务和数据。 ##劣势 缺乏鲁棒性：某些损失函数对异常值或噪声敏感，可能会导致模型过于关注异常值或噪声点，而忽略了整体的数据模式。在处理异常值或噪声较多的情况下，需要考虑使用更鲁棒的损失函数。 可能存在局部最优解：优化损失函数的过程中，可能会陷入局部最优解，而无法达到全局最优解。为了克服这个问题，可以采用不同的优化算法、调整模型结构或使用正则化等技术。 特定任务的依赖性：某些损失函数对于特定任务效果较好，但对其他任务可能效果较差。在选择损失函数时，需要考虑任务的特性、数据的分布以及模型的假设前提。 综上所述，损失函数在机器学习中是一个重要的组成部分。了解不同损失函数的优势和劣势，可以帮助选择适合特定任务和数据的损失函数，以获得更好的模型性能。" }, { "title": "机器学习基础-监督学习-线性回归之模型定义", "url": "/posts/%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-06 00:00:00 +0800", "snippet": "在监督学习中，模型定义是指如何建立输入特征和输出标签之间的关系。不同的算法有不同的模型定义方法。以下是几种常见的模型定义以及对应的公式或代码示例： 线性回归模型定义：线性回归模型假设输入特征和输出标签之间存在线性关系。其模型定义可以表示为：\\[y = w1x1 + w2x2 + ... + wnxn + b\\]其中，x1, x2, …, xn 是输入特征，w1, w2, …, wn 是特征的权重，b 是偏置项，y 是预测的输出标签。代码示例（使用 Python 和 NumPy 库）：import numpy as np# 输入特征X = np.array([[x1, x2, ...], # 样本1的特征 [x1, x2, ...], # 样本2的特征 ...])# 输出标签y = np.array([y1, y2, ...])# 添加偏置项X = np.hstack((X, np.ones((X.shape[0], 1))))# 计算权重w = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y) 逻辑回归模型定义：逻辑回归模型用于解决二分类问题，模型定义采用逻辑函数（sigmoid 函数）将线性组合的结果映射到[0, 1]的概率范围内。其模型定义可以表示为：\\[y = sigmoid(w1x1 + w2x2 + ... + wnxn + b)\\]其中，x1, x2, …, xn 是输入特征，w1, w2, …, wn 是特征的权重，b 是偏置项，sigmoid 函数将线性组合的结果映射到[0, 1]之间的概率值。代码示例（使用 Python 和 NumPy 库）：import numpy as np# 输入特征X = np.array([[x1, x2, ...], # 样本1的特征 [x1, x2, ...], # 样本2的特征 ...])# 输出标签y = np.array([y1, y2, ...])# 添加偏置项X = np.hstack((X, np.ones((X.shape[0], 1))))# 定义sigmoid函数def sigmoid(x): return 1 / (1 + np.exp(-x))# 计算权重w = np.random.randn(X.shape[1]) # 随机初始化权重# 定义模型输出y_pred = sigmoid(X.dot(w)) 支持向量机模型定义：支持向量机（Support Vector Machines，SVM）是一种常用的分类算法。其模型定义基于将输入特征映射到高维空间中，通过寻找一个最优的超平面来实现分类。模型定义可以表示为：\\[y = sign(w1x1 + w2x2 + ... + wnxn + b)\\]其中，x1, x2, …, xn 是输入特征，w1, w2, …, wn 是特征的权重，b 是偏置项，sign 函数根据线性组合的结果给出分类的预测结果。代码示例（使用 Python 和 Scikit-learn 库）：from sklearn.svm import SVC# 输入特征X = [[x1, x2, ...], # 样本1的特征 [x1, x2, ...], # 样本2的特征 ...]# 输出标签y = [y1, y2, ...]# 创建SVM分类器对象clf = SVC(kernel=&#39;linear&#39;)# 训练模型clf.fit(X, y)# 预测新样本y_pred = clf.predict([[new_x1, new_x2, ...]]) 决策树模型定义：决策树模型使用树形结构来进行分类或回归任务。在分类任务中，决策树通过一系列的分裂节点来划分样本，每个节点根据特征的取值选择一个分支。模型定义可以表示为一系列的判断条件和结果。代码示例（使用 Python 和 Scikit-learn 库）：from sklearn.tree import DecisionTreeClassifier# 输入特征X = [[x1, x2, ...], # 样本1的特征 [x1, x2, ...], # 样本2的特征 ...]# 输出标签y = [y1, y2, ...]# 创建决策树分类器对象clf = DecisionTreeClassifier()# 训练模型clf.fit(X, y)# 预测新样本y_pred = clf.predict([[new_x1, new_x2, ...]])以上是几种常见监督学习模型的定义及相应的代码示例。实际应用中，根据具体问题的特点和算法的选择，模型的定义可能会有所调整和扩展。" }, { "title": "机器学习基础-监督学习-无监督标签之关联规则挖掘", "url": "/posts/%E6%A0%87%E7%AD%BE-35-%E6%97%A0%E7%9B%91%E7%9D%A3%E6%A0%87%E7%AD%BE%E4%B9%8B%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E6%8C%96%E6%8E%98/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-05 00:00:00 +0800", "snippet": "关联规则挖掘是一种无监督学习任务，旨在发现数据集中项集之间的关联关系。它可以用于揭示数据中的频繁项集和关联规则，有助于了解数据的内在结构和关联性。关联规则通常采用”如果…那么…“的形式，指示项集之间的关系。关联规则由两部分组成：前项（Antecedent）和后项（Consequent）。前项是规则的条件部分，后项是规则的结果部分。例如，对于一个购物篮数据集，一条关联规则可以是：”如果牛奶和面包出现在一个购物篮中，那么鸡蛋也可能出现在该购物篮中”。关联规则挖掘的常见算法之一是 Apriori 算法，它通过计算项集的支持度（Support）和置信度（Confidence）来确定频繁项集和关联规则。支持度表示项集在数据集中出现的频率，即项集在所有事务中的出现次数的比例。置信度表示当前项集的前项和后项同时出现时的条件概率，即规则的可信程度。以下是 Apriori 算法的关键步骤： 设定最小支持度和最小置信度阈值。 遍历数据集，计算每个项集的支持度。 根据最小支持度阈值筛选出频繁项集。 基于频繁项集，生成关联规则，并计算置信度。 根据最小置信度阈值筛选出满足要求的关联规则。下面是一个示例代码，用于展示关联规则挖掘的过程：from mlxtend.preprocessing import TransactionEncoderfrom mlxtend.frequent_patterns import apriori, association_rules# 准备示例购物篮数据集dataset = [[&#39;牛奶&#39;, &#39;面包&#39;], [&#39;牛奶&#39;, &#39;鸡蛋&#39;, &#39;面包&#39;, &#39;薯片&#39;], [&#39;牛奶&#39;, &#39;尿布&#39;, &#39;啤酒&#39;, &#39;鸡蛋&#39;], [&#39;牛奶&#39;, &#39;尿布&#39;, &#39;面包&#39;, &#39;鸡蛋&#39;]]# 转换数据集为适用于Apriori算法的形式te = TransactionEncoder()te_ary = te.fit_transform(dataset)df = pd.DataFrame(te_ary, columns=te.columns_)# 使用Apriori算法挖掘频繁项集frequent_itemsets = apriori(df, min_support=0.5, use_colnames=True)# 生成关联规则rules = association_rules(frequent_itemsets, metric=&quot;confidence&quot;, min_threshold=0.7)# 打印频繁项集和关联规则print(&quot;频繁项集：&quot;)print(frequent_itemsets)print(&quot;\\n关联规则：&quot;)print(rules)上述代码使用 mlxtend 库进行关联规则挖掘。首先，我们准备了一个示例购物篮数据集。然后，使用 TransactionEncoder 将数据集转换为适用于 Apriori 算法的形式。接下来，使用 Apriori 算法挖掘频繁项集，其中 min_support 参数指定了最小支持度阈值。最后，使用 association_rules 函数生成关联规则，其中 metric 参数指定了用于评估规则的度量标准，min_threshold 参数指定了最小置信度阈值。关联规则挖掘的结果将包含频繁项集和关联规则。频繁项集将显示项集和其对应的支持度，关联规则将显示规则的前项、后项、支持度和置信度等信息。总结来说，关联规则挖掘是一种无监督学习任务，用于发现数据集中项集之间的关联关系。Apriori 算法是常用的关联规则挖掘算法，通过计算支持度和置信度来确定频繁项集和关联规则。以上代码示例展示了如何使用 Apriori 算法和 mlxtend 库进行关联规则挖掘，并输出结果。关联规则挖掘中的支持度和置信度是用于度量关联规则的重要指标。下面详细解释这两个概念，并给出相应的公式。 支持度（Support）：支持度是指一个项集在整个数据集中出现的频率。在关联规则挖掘中，我们关注的是频繁项集，即支持度大于等于事先设定的最小支持度阈值的项集。支持度的计算公式如下：Support(A) = (出现A的次数) / (总事务数)其中，A 是一个项集，Support(A)表示项集 A 的支持度。 置信度（Confidence）：置信度是指一个关联规则的可信程度，即前项出现时后项出现的概率。在关联规则挖掘中，我们寻找的是置信度大于等于事先设定的最小置信度阈值的关联规则。置信度的计算公式如下：Confidence(A → B) = (Support(A ∪ B)) / (Support(A))其中，A 和 B 是项集，Confidence(A → B)表示规则 A → B 的置信度。Support(A ∪ B)表示同时出现 A 和 B 的支持度，Support(A)表示项集 A 的支持度。支持度和置信度是关联规则挖掘中常用的度量指标。通过设置适当的支持度和置信度阈值，我们可以筛选出具有足够频繁和可信程度的关联规则，从而发现数据集中的有意义的关联关系。在实际应用中，我们通常会根据问题的特点和数据的特性来设定最小支持度和最小置信度阈值，以便获得符合需求的关联规则。" }, { "title": "机器学习基础-监督学习-无监督标签之降维", "url": "/posts/%E6%A0%87%E7%AD%BE-34-%E6%97%A0%E7%9B%91%E7%9D%A3%E6%A0%87%E7%AD%BE%E4%B9%8B%E9%99%8D%E7%BB%B4/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-05 00:00:00 +0800", "snippet": "降维是一种常见的无监督学习任务，其目标是将高维数据映射到低维空间，以减少数据的特征维度，同时保留数据的关键结构和信息。通过降维，可以简化数据表示、减少存储空间、降低计算复杂度，并帮助可视化和数据理解。降维方法的选择取决于数据的性质、任务需求和算法的适用性。以下是一些常见的降维方法： 主成分分析(Principal Component Analysis, PCA)：PCA 是一种常用的线性降维技术，它通过找到数据中的主要方差方向来实现降维。PCA 将原始数据映射到新的低维空间，其中新的特征被称为主成分。每个主成分都是原始特征的线性组合，其排序是根据对应的方差来确定。from sklearn.datasets import load_irisfrom sklearn.decomposition import PCA# 加载鸢尾花数据集iris = load_iris()X = iris.data# 使用PCA进行降维pca = PCA(n_components=2)X_reduced = pca.fit_transform(X)# 打印降维后的数据print(&quot;降维后的数据：&quot;)print(X_reduced) 线性判别分析(Linear Discriminant Analysis, LDA)：LDA 也是一种常用的线性降维方法，它与 PCA 不同的是，LDA 考虑了样本的类别信息。LDA 试图在降维的过程中保留类别之间的差异，以找到更具有判别性的特征。因此，LDA 在分类问题中特别有用。from sklearn.datasets import load_irisfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis# 加载鸢尾花数据集iris = load_iris()X = iris.datay = iris.target# 使用LDA进行降维lda = LinearDiscriminantAnalysis(n_components=2)X_reduced = lda.fit_transform(X, y)# 打印降维后的数据print(&quot;降维后的数据：&quot;)print(X_reduced) t 分布邻域嵌入(t-Distributed Stochastic Neighbor Embedding, t-SNE)：t-SNE 是一种非线性降维方法，它在可视化和数据挖掘中广泛使用。t-SNE 通过保留数据样本之间的局部关系，将高维数据映射到二维或三维空间。它能够更好地保留样本之间的相似性，尤其适用于数据聚类和类别可视化。from sklearn.datasets import load_irisfrom sklearn.manifold import TSNE# 加载鸢尾花数据集iris = load_iris()X = iris.data# 使用t-SNE进行降维tsne = TSNE(n_components=2)X_reduced = tsne.fit_transform(X)# 打印降维后的数据print(&quot;降维后的数据：&quot;)print(X_reduced)这些示例代码演示了如何使用不同的降维方法对数据进行降维，并打印降维后的数据。在实际应用中，你可以根据数据集的特点和任务需求选择合适的降维方法，并根据降维结果进行进一步的数据分析、可视化或建模。在降维过程中，可以使用降维方法提供的可解释性分析降维结果，了解每个特征的重要性，选择适当的降维维度，并根据需要进行可视化或后续的数据分析任务。" }, { "title": "机器学习基础-监督学习-无监督标签之聚类", "url": "/posts/%E6%A0%87%E7%AD%BE-33-%E6%97%A0%E7%9B%91%E7%9D%A3%E6%A0%87%E7%AD%BE%E4%B9%8B%E8%81%9A%E7%B1%BB/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-05 00:00:00 +0800", "snippet": "聚类是一种无监督学习任务，旨在将数据集中的样本分成不同的组或簇，使得同一簇内的样本相似度较高，而不同簇之间的样本相似度较低。聚类算法试图在没有先验标签的情况下发现数据中的内在结构和模式。以下是一个常见的聚类算法示例：K-means 聚类算法。K-means 算法的基本思想是将数据分成 K 个簇，每个簇由一个中心点（质心）来表示。算法的步骤如下： 初始化：随机选择 K 个样本作为初始质心。 质心分配：对于每个样本，计算其与每个质心的距离，将样本分配给最近的质心所属的簇。 更新质心：对于每个簇，计算其内部样本的平均值，将其作为新的质心。 重复步骤 2 和步骤 3，直到质心不再改变或达到预定的迭代次数。以下是一个使用 scikit-learn 库中的 K-means 算法进行聚类的示例代码：from sklearn.datasets import make_blobsfrom sklearn.cluster import KMeansimport matplotlib.pyplot as plt# 生成一个模拟的聚类数据集X, _ = make_blobs(n_samples=200, centers=4, random_state=42)# 使用K-means算法进行聚类kmeans = KMeans(n_clusters=4, random_state=42)kmeans.fit(X)# 获取聚类结果cluster_labels = kmeans.labels_centroids = kmeans.cluster_centers_# 绘制聚类结果plt.scatter(X[:, 0], X[:, 1], c=cluster_labels)plt.scatter(centroids[:, 0], centroids[:, 1], marker=&#39;X&#39;, color=&#39;red&#39;, s=100)plt.title(&#39;K-means Clustering&#39;)plt.show()上述代码生成了一个模拟的聚类数据集，并使用 K-means 算法对数据进行聚类。通过 fit() 方法进行训练后，labels* 属性包含每个样本的聚类标签，cluster_centers* 属性包含每个簇的质心坐标。最后，通过绘制散点图，我们可以将聚类结果可视化，其中每个样本的颜色表示其所属的簇，红色的 X 表示簇的质心。聚类算法有许多变体和改进，包括层次聚类、DBSCAN、GMM 等。具体选择哪个算法取决于数据的性质和问题的要求。当涉及聚类时，可以使用一些评价指标来评估聚类结果的质量。以下是两个常用的聚类评价指标： 轮廓系数(Silhouette Coefficient)：轮廓系数是一种衡量样本聚类紧密度和分离度的指标。它的取值范围在[-1, 1]之间，数值越接近 1 表示样本聚类得越好，数值越接近-1 表示样本聚类得越差。轮廓系数的计算方式如下： 对于样本 i，计算其与同簇其他样本的平均距离，记为 a(i)。 对于样本 i，计算其与最近其他簇中样本的平均距离，记为 b(i)。 计算样本 i 的轮廓系数：s(i) = (b(i) - a(i)) / max(a(i), b(i))。 对所有样本的轮廓系数取平均，得到聚类的整体轮廓系数。 均一性(Homogeneity)和完整性(Completeness)：均一性和完整性是一对互补的度量指标，用于评估聚类结果与真实标签之间的一致性。均一性度量了同一真实类别中的样本是否都被分配到了同一个簇中。完整性度量了同一个簇中的样本是否都属于同一个真实类别。均一性和完整性的计算方式如下： 均一性(h) = 1 - H(C K) / H(C)，其中 H(C K) 是给定聚类结果下，条件熵。 完整性(c) = 1 - H(K C) / H(K)，其中 H(K C) 是给定真实标签下，条件熵。 调和平均(v) = 2 _ (h _ c) / (h + c)。需要注意的是，聚类评价指标仅对有已知真实标签的数据集有效。在没有真实标签的情况下，评估聚类结果的质量会更加主观。以下是一个示例代码，演示如何使用轮廓系数和均一性、完整性进行聚类结果的评估：from sklearn.datasets import make_blobsfrom sklearn.cluster import KMeansfrom sklearn.metrics import silhouette_score, homogeneity_completeness_v_measure# 生成一个模拟的聚类数据集X, y = make_blobs(n_samples=200, centers=4, random_state=42)# 使用K-means算法进行聚类kmeans = KMeans(n_clusters=4, random_state=42)kmeans.fit(X)# 获取聚类结果和真实标签cluster_labels = kmeans.labels_# 计算轮廓系数silhouette_avg = silhouette_score(X, cluster_labels)print(&quot;轮廓系数:&quot;, silhouette_avg)# 计算均一性、完整性和调和平均homo, comp, v_measure = homogeneity_completeness_v_measure(y, cluster_labels)print(&quot;均一性:&quot;, homo)print(&quot;完整性:&quot;, comp)print(&quot;调和平均:&quot;, v_measure)上述代码中，通过调用 scikit-learn 库中的 silhouette_score 函数计算轮廓系数。该函数需要传入原始数据集 X 和聚类结果 cluster_labels。同时，使用 homogeneity_completeness_v_measure 函数计算均一性、完整性和调和平均。该函数需要传入真实标签 y 和聚类结果 cluster_labels。执行上述代码后，会分别打印轮廓系数、均一性、完整性和调和平均的值，用于评估聚类结果的质量。这些评价指标可以帮助我们了解聚类结果的紧密度、分离度和与真实标签的一致性。在实际应用中，根据具体问题的需求，选择适当的评价指标来评估聚类结果。" }, { "title": "机器学习基础-监督学习-标签转移学习之标签迁移", "url": "/posts/%E6%A0%87%E7%AD%BE-32-%E6%A0%87%E7%AD%BE%E8%BD%AC%E7%A7%BB%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%A0%87%E7%AD%BE%E8%BF%81%E7%A7%BB/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-05 00:00:00 +0800", "snippet": "标签迁移（Label Transfer）是标签转移学习中的一个关键步骤，它将已有的标签知识从源任务迁移到目标任务上。标签迁移的目标是通过源任务的标签信息来改善目标任务的学习效果，尤其在目标任务的标注数据较少或不可获得的情况下。标签迁移的核心思想是将源任务的标签知识应用于目标任务。一种常见的做法是将源任务的模型或模型的部分作为目标任务的初始模型，然后通过微调（Fine-tuning）的方式在目标任务上进行训练。微调是指在目标任务上调整模型的权重，以更好地适应目标任务的特点。在标签迁移过程中，我们通常采用以下步骤： 基础模型的选择：选择一个在源任务上预训练的基础模型。这个基础模型通常在大规模数据集上进行了训练，学习到了丰富的特征表示。 特征提取：使用基础模型提取源任务数据和目标任务数据的特征表示。对于图像任务，可以使用卷积神经网络（CNN）的中间层输出作为特征向量；对于文本任务，可以使用预训练的词嵌入模型得到文本的表示。 标签迁移方法：根据具体的标签迁移方法将源任务的标签知识迁移到目标任务上。以下介绍两种常见的标签迁移方法： 3.1. 调整分类器：将源任务的分类器应用于目标任务上，保持特征提取部分不变。这种方法适用于源任务和目标任务具有相似的类别或领域特征。 3.2. 调整特征提取器：除了迁移分类器，还可以调整特征提取器部分。通过微调特征提取器的参数，使其能够更好地适应目标任务的特点。这种方法适用于源任务和目标任务的领域差异较大的情况。 目标任务的训练：在迁移后的模型基础上，使用目标任务的标签数据进行训练。可以通过微调模型的参数，或者仅训练新添加的分类器部分。 下面是一个使用 PyTorch 实现的简单示例代码，演示了标签迁移的过程：import torchimport torch.nn as nnimport torch.optim as optimfrom torchvision import models# 加载预训练的基础模型base_model = models.resnet50(pretrained=True)# 替换分类器层为目标任务的类别数量num_classes = 10base_model.fc = nn.Linear(2048, num_classes)# 冻结基础模型的参数for param in base_model.parameters(): param.requires_grad = False# 定义目标任务的损失函数和优化器criterion = nn.CrossEntropyLoss()optimizer = optim.SGD(base_model.fc.parameters(), lr=0.001, momentum=0.9)# 在目标任务上进行训练for epoch in range(num_epochs): for images, labels in target_dataloader: optimizer.zero_grad() outputs = base_model(images) loss = criterion(outputs, labels) loss.backward() optimizer.step()上述代码中，我们使用预训练的 ResNet-50 模型作为基础模型，并将其分类器层替换为适应目标任务的类别数量。然后，我们冻结了基础模型的参数，只训练新添加的分类器层。定义了目标任务的损失函数和优化器，并在目标任务的数据集上进行训练。需要注意的是，上述示例中只展示了基于预训练模型的参数微调的方式进行标签迁移。在实际应用中，还可以根据具体情况选择其他标签迁移方法，如特征选择、领域自适应等。标签迁移的目标是通过源任务的标签知识来改善目标任务的学习效果。通过合理选择基础模型、调整分类器或特征提取器，并在目标任务上进行训练，我们可以利用已有的标签信息来提升目标任务的性能。具体的方法和技术取决于问题和数据集的特点，需要根据实际情况进行选择和调整。总结起来，标签迁移是标签转移学习中的重要步骤，它通过将源任务的模型或模型的部分作为目标任务的初始模型，并在目标任务上进行微调，将源任务的标签知识迁移到目标任务上。这样可以利用已有的标签信息来改善目标任务的学习效果，尤其在目标任务的标注数据较少或不可获得的情况下。具体的方法和技术可以根据具体问题和数据集的特点进行调整和选择。" }, { "title": "机器学习基础-监督学习-标签转移学习之特征提取", "url": "/posts/%E6%A0%87%E7%AD%BE-31-%E6%A0%87%E7%AD%BE%E8%BD%AC%E7%A7%BB%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-05 00:00:00 +0800", "snippet": "特征提取是机器学习和深度学习中的一个重要步骤，用于从原始数据中提取出有用的、能够表征数据特征的表示。在许多任务中，原始数据可能是高维的、复杂的，通过特征提取可以将其转化为更加简洁、信息丰富的表示形式，有助于模型的学习和泛化能力的提升。特征提取的目标是选择或设计一组特征，使其能够最好地捕捉数据中的关键信息，以便于后续的学习和预测任务。特征可以是手工设计的，也可以是通过自动学习得到的。下面介绍几种常见的特征提取方法： 手工设计特征：传统机器学习中常用的方法是手工设计特征。这需要根据问题领域的专业知识和经验，选择合适的特征表示。例如，在图像分类任务中，可以提取颜色直方图、纹理特征、形状描述符等。手工设计特征的优势是可解释性强，但对于复杂的问题可能需要大量的人力和专业知识。 基于统计的特征提取：通过对原始数据进行统计分析，提取出统计特征。例如，对于时间序列数据，可以计算均值、方差、相关系数等统计量作为特征。 基于信号处理的特征提取：对信号数据进行预处理和特征提取。例如，在语音识别中，可以通过傅里叶变换提取频谱特征，或者使用梅尔频谱系数（Mel-frequency cepstral coefficients, MFCCs）作为声音的特征表示。 深度学习中的特征提取：深度学习模型可以自动学习数据的特征表示，尤其是在大规模数据集上预训练的情况下。常见的方法包括使用卷积神经网络（CNN）进行图像特征提取，使用循环神经网络（RNN）进行序列数据特征提取，以及使用自动编码器进行无监督学习的特征提取等。 下面是一个示例代码，演示如何使用预训练的卷积神经网络（VGG16）进行图像特征提取：import tensorflow as tffrom tensorflow.keras.applications import VGG16from tensorflow.keras.preprocessing import imagefrom tensorflow.keras.applications.vgg16 import preprocess_input# 加载预训练的VGG16模型（不包括分类层）base_model = VGG16(weights=&#39;imagenet&#39;, include_top=False)# 加载图像并进行预处理img_path = &#39;path/to/your/image.jpg&#39;img = image.load_img(img_path, target_size=(224, 224))x = image.img_to_array(img)x = preprocess_input(x)x = np.expand_dims(x, axis=0)# 提取特征features = base_model.predict(x)# 打印特征的形状print(features.shape)上述代码中，我们首先加载了预训练的 VGG16 模型，并指定 include_top=False 参数来排除模型的分类层。然后，我们加载了一张图像，并进行了预处理，包括将图像转换为数组形式、对图像进行预处理等。接下来，我们通过 base_model.predict 方法对图像进行特征提取，得到图像在 VGG16 模型中的特征表示。最后，我们打印特征的形状。通过这种方式，我们可以将图像转换为高维的特征向量，这些特征向量可以用作后续的分类、检索等任务。这种基于预训练模型的特征提取方法使得我们能够从图像中获取有用的、抽象的特征表示，无需手动设计特征。特征提取是机器学习和深度学习中的一个重要步骤，它可以帮助我们将原始数据转换为更加有用和表征性的表示形式，为后续的学习和预测任务提供更好的输入。通过选择合适的特征提取方法和技术，我们可以更好地理解数据，提取有用的信息，并提高模型的性能和泛化能力。" }, { "title": "机器学习基础-监督学习-标签转移学习之基础模型的训练", "url": "/posts/%E6%A0%87%E7%AD%BE-30-%E6%A0%87%E7%AD%BE%E8%BD%AC%E7%A7%BB%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AE%AD%E7%BB%83/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-05 00:00:00 +0800", "snippet": "基础模型的训练是标签转移学习中的一个重要步骤。通过训练基础模型，我们可以学习到从输入数据到标签的映射关系，从而获得模型在源任务上的表现。基础模型的训练过程通常可以分为以下几个步骤： 准备源数据集：收集和准备用于基础模型训练的源数据集。这个数据集通常与目标任务相关，但可能具有更丰富的标签信息。 构建模型架构：选择合适的模型架构作为基础模型。这可以是常见的深度学习模型，如卷积神经网络（CNN）或循环神经网络（RNN），也可以是其他机器学习模型。 定义损失函数：选择合适的损失函数来度量模型的预测结果与真实标签之间的差异。损失函数的选择取决于源任务的性质，例如交叉熵损失函数常用于分类任务，均方误差损失函数常用于回归任务。 配置优化器：选择合适的优化算法来最小化损失函数。常用的优化算法包括随机梯度下降（SGD）、Adam、RMSprop 等。 训练模型：使用源数据集进行模型的训练。这涉及将输入数据提供给模型进行前向传播，计算损失函数的值，然后使用反向传播算法更新模型的权重。 下面是一个简单的示例代码，演示了如何训练一个基础模型（使用 CNN）来进行图像分类任务：import tensorflow as tf# 构建模型架构model = tf.keras.models.Sequential([ tf.keras.layers.Conv2D(32, (3, 3), activation=&#39;relu&#39;, input_shape=(64, 64, 3)), tf.keras.layers.MaxPooling2D((2, 2)), tf.keras.layers.Flatten(), tf.keras.layers.Dense(64, activation=&#39;relu&#39;), tf.keras.layers.Dense(num_classes, activation=&#39;softmax&#39;)])# 定义损失函数和优化器model.compile(optimizer=&#39;adam&#39;, loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;])# 训练模型model.fit(train_images, train_labels, epochs=10, validation_data=(val_images, val_labels))和val_labels）对模型进行验证。在训练过程中，模型会通过前向传播计算预测值，然后通过与真实标签进行比较计算损失。接下来，使用反向传播算法计算梯度并更新模型的权重，以最小化损失函数。这个过程会反复进行多个 epochs，直到模型收敛或达到预定的停止条件。需要注意的是，基础模型的训练过程可以根据具体的问题和数据集进行调整。可以选择不同的模型架构、损失函数、优化器和训练参数，以满足特定任务的要求。总结起来，基础模型的训练是标签转移学习中的重要步骤。通过准备源数据集、构建模型架构、定义损失函数、配置优化器和训练模型，我们可以学习到模型在源任务上的表现，并为后续的标签转移提供基础。具体的训练过程可以根据问题和数据集进行调整，选择合适的模型和训练参数来达到最佳性能。" }, { "title": "机器学习基础-监督学习-标签转移学习", "url": "/posts/%E6%A0%87%E7%AD%BE-29-%E6%A0%87%E7%AD%BE%E8%BD%AC%E7%A7%BB%E5%AD%A6%E4%B9%A0/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-05 00:00:00 +0800", "snippet": "标签转移学习（Label Transfer Learning）是一种机器学习方法，旨在利用已有的标签信息来帮助解决新的问题。它通过将已有的标签知识从一个相关任务或数据集迁移到目标任务或数据集上，从而减少目标任务上的标注成本、提高模型性能或解决数据稀缺的问题。标签转移学习的主要思想是，通过训练一个基础模型（source model）来学习从输入数据到标签的映射关系，然后将这种映射关系应用到目标任务上。这个过程可以通过以下步骤来实现：基础模型的训练：使用已有的标签数据集训练一个基础模型，例如通过监督学习方法（如分类或回归）训练一个神经网络模型。特征提取：使用基础模型提取输入数据的特征表示。这些特征表示捕捉了基础模型对输入数据的理解和表达能力。标签迁移：将基础模型学到的特征映射关系迁移到目标任务上。这可以通过在目标任务上重新训练一个新的模型，但将基础模型的特征提取部分固定住，只训练新的任务相关层来实现。也可以通过调整目标任务上的损失函数来结合基础模型和目标任务的特点。下面是一个简单的示例代码，演示了如何使用标签转移学习来进行图像分类任务。import tensorflow as tffrom tensorflow.keras.applications import VGG16# 加载预训练的基础模型base_model = VGG16(weights=&#39;imagenet&#39;, include_top=False, input_shape=(224, 224, 3))# 冻结基础模型的权重，只训练分类层for layer in base_model.layers: layer.trainable = False# 添加新的分类层model = tf.keras.models.Sequential([ base_model, tf.keras.layers.Flatten(), tf.keras.layers.Dense(256, activation=&#39;relu&#39;), tf.keras.layers.Dense(10, activation=&#39;softmax&#39;)])# 编译模型model.compile(optimizer=&#39;adam&#39;, loss=&#39;sparse_categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;])# 在目标任务上训练模型model.fit(train_images, train_labels, epochs=10, validation_data=(val_images, val_labels))上述代码中，我们使用了预训练的 VGG16 模型作为基础模型，并冻结了它的权重。然后，我们通过添加新的分类层来构建一个完整的模型。最后，我们使用目标任务的数据集在模型上进行训练。这个示例演示了如何通过标签转移学习利用已有的标签转移学习的方法进行图像分类任务。基于预训练的 VGG16 模型，我们可以利用它在大规模图像数据上学到的特征表示能力，然后通过训练新的分类层来适应目标任务的特点。标签转移学习的关键是通过共享特征表示来迁移标签知识。基础模型已经在源任务上学到了一些通用的特征表示，这些特征表示对目标任务也可能是有用的。通过冻结基础模型的权重，我们可以保持这些通用特征表示不变，只训练新的分类层以适应目标任务的特定需求。在目标任务上训练模型时，我们可以选择不同的策略。有时候，我们只使用目标任务上的少量标注数据进行训练，这种情况下，标签转移学习尤为有用，可以减少标注数据的需求。另外，我们也可以将基础模型的一部分层解冻，并根据目标任务的需求调整权重，以更好地适应目标任务的特点。标签转移学习可以应用于各种机器学习任务，包括图像分类、目标检测、文本分类等。通过合理选择基础模型、冻结权重的层以及训练策略，我们可以利用已有的标签信息来提升模型性能，尤其在数据集稀缺或标注成本高昂的情况下。总结起来，标签转移学习是一种利用已有标签信息帮助解决新任务的方法。通过训练一个基础模型，迁移其学到的特征表示到目标任务上，我们可以在新任务上利用已有的标签知识，从而减少标注成本、提高模型性能。具体的方法和技术可以根据不同的问题和数据集进行调整和选择。" }, { "title": "机器学习基础-监督学习-标签增强之标签标签插值（Label Interpolation）", "url": "/posts/%E6%A0%87%E7%AD%BE-28-%E6%A0%87%E7%AD%BE%E6%A0%87%E7%AD%BE%E5%A2%9E%E5%BC%BA%E4%B9%8B%E6%A0%87%E7%AD%BE%E6%8F%92%E5%80%BC/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-05 00:00:00 +0800", "snippet": "标签插值（Label Interpolation）是一种标签增强技术，用于在回归问题或连续标签的分类问题中生成介于两个标签之间的新标签。插值方法基于两个已知标签之间的线性或非线性关系，通过计算插值函数来生成新的标签。以下是一个示例代码，展示如何使用线性插值进行标签插值：import numpy as np# 原始标签label1 = 0label2 = 10# 插值数量num_interpolations = 5# 标签插值interpolated_labels = np.linspace(label1, label2, num_interpolations + 2)[1:-1]print(&quot;原始标签1:&quot;, label1)print(&quot;原始标签2:&quot;, label2)print(&quot;插值后的标签:&quot;, interpolated_labels)在上述代码中，我们定义了两个原始标签 label1 和 label2，以及要生成的插值数量 num_interpolations。然后，我们使用 np.linspace 函数对两个标签之间的区间进行等间距划分，生成包含原始标签和插值标签的数组，并使用切片操作排除原始标签。最后，我们打印出原始标签和插值后的标签。输出结果为：原始标签1: 0原始标签2: 10插值后的标签: [2. 4. 6. 8.]可以看到，原始标签 1 为 0，原始标签 2 为 10，我们通过线性插值生成了 4 个介于两个标签之间的新标签。需要注意的是，上述示例中使用的是线性插值方法，根据需求，我们也可以使用其他插值方法，如样条插值（Spline Interpolation）或其他非线性插值方法。这些方法可以更好地适应非线性的标签关系。总结来说，标签插值是一种标签增强技术，用于在回归问题或连续标签的分类问题中生成介于两个标签之间的新标签。插值方法基于已知标签之间的关系，通过计算插值函数来生成新的标签。具体的插值方法可以根据问题的性质和数据集的特点选择，例如线性插值、样条插值等。当涉及到标签插值时，我们可以使用不同的插值方法来生成新的标签。除了线性插值之外，还有其他一些常见的插值方法，例如多项式插值、样条插值和径向基函数插值等。下面是一个示例代码，展示如何使用多项式插值进行标签插值：import numpy as npfrom scipy.interpolate import interp1d# 原始标签label1 = 0label2 = 10# 插值数量num_interpolations = 5# 插值函数x = np.array([0, num_interpolations + 1])y = np.array([label1, label2])f = interp1d(x, y, kind=&#39;quadratic&#39;)# 标签插值interpolated_labels = f(np.arange(1, num_interpolations + 1))print(&quot;原始标签1:&quot;, label1)print(&quot;原始标签2:&quot;, label2)print(&quot;插值后的标签:&quot;, interpolated_labels)在上述代码中，我们使用 scipy.interpolate 中的 interp1d 函数来创建一个多项式插值函数 f，其中 kind=’quadratic’ 表示使用二次插值。然后，我们使用这个插值函数 f 对标签进行插值，并生成新的标签数组 interpolated_labels。最后，我们打印出原始标签和插值后的标签。输出结果为：原始标签1: 0原始标签2: 10插值后的标签: [ 1.11111111 3.33333333 6.66666667 10. 13.33333333]可以看到，我们使用二次插值方法生成了介于原始标签 1 和原始标签 2 之间的新标签。需要注意的是，不同的插值方法在处理不同问题和数据集时可能会有不同的效果。因此，在实际应用中，根据具体问题的特点和需求，选择合适的插值方法进行标签增强。总结来说，标签插值是一种标签增强技术，用于在回归问题或连续标签的分类问题中生成介于两个标签之间的新标签。除了线性插值之外，还可以使用多项式插值、样条插值、径向基函数插值等方法。根据具体问题的性质和数据集的特点，选择合适的插值方法进行标签增强。" }, { "title": "机器学习基础-监督学习-标签增强之标签扰动（Label Perturbation）", "url": "/posts/%E6%A0%87%E7%AD%BE-27-%E6%A0%87%E7%AD%BE%E6%A0%87%E7%AD%BE%E5%A2%9E%E5%BC%BA%E4%B9%8B%E6%A0%87%E7%AD%BE%E6%89%B0%E5%8A%A8/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-05 00:00:00 +0800", "snippet": "标签扰动（Label Perturbation）是一种标签增强技术，通过引入一定的噪声或随机变化来生成具有相似语义含义但略有差异的新标签。标签扰动可以在分类问题中应用，用于扩展训练数据的标签空间，增加数据的多样性。以下是一个简单的示例代码，演示如何对分类标签进行随机扰动：import numpy as np# 原始标签original_labels = np.array([0, 1, 2, 3, 4])# 扰动强度perturbation_strength = 0.1# 标签扰动perturbed_labels = original_labels + np.random.uniform(low=-perturbation_strength, high=perturbation_strength, size=len(original_labels))print(&quot;原始标签：&quot;, original_labels)print(&quot;扰动后的标签：&quot;, perturbed_labels)在上述代码中，我们定义了一个原始标签数组 original_labels。然后，通过指定扰动强度 perturbation_strength，使用 np.random.uniform() 函数生成一个与原始标签长度相同的随机扰动数组，然后将其加到原始标签上得到扰动后的标签 perturbed_labels。需要注意的是，扰动强度决定了扰动的范围，较大的扰动强度会引入更大的标签变化。可以根据具体的问题和数据集，调整扰动强度以达到合适的效果。输出结果示例：原始标签： [0 1 2 3 4]扰动后的标签： [ 0.09944678 1.00997471 1.92507057 2.96168415 4.03627278]可以看到，扰动后的标签在原始标签的基础上引入了一定的随机扰动，生成了略微变化的新标签。需要注意的是，在实际应用中，标签扰动的具体方法和实现方式会因问题的性质和数据集的特点而有所不同。可以根据具体需求选择适合的扰动方式，如使用不同的随机分布、调整扰动强度等，从而生成具有相似语义含义但略有差异的新标签。标签扰动是一种常用的标签增强技术，通过引入随机扰动来增加数据集的多样性，提高模型的泛化能力。这种技术可以应用于各种分类问题，通过增加数据的多样性，帮助模型更好地泛化到未见过的数据。除了简单的标签扰动示例，还可以使用其他方法对标签进行扰动。下面介绍两种常见的标签扰动技术：随机替换和标签翻转。 随机替换（Random Replacement）：在这种方法中，我们随机选择一部分标签，并将其替换为其他随机选择的标签。这样可以引入更大程度的标签变化，增加数据的多样性。下面是一个示例代码，演示如何使用随机替换对标签进行扰动：import numpy as np# 原始标签original_labels = np.array([0, 1, 2, 3, 4])# 扰动概率perturbation_probability = 0.2# 标签扰动 - 随机替换perturbed_labels = original_labels.copy()mask = np.random.random(len(original_labels)) &amp;lt; perturbation_probabilityperturbed_labels[mask] = np.random.randint(low=0, high=5, size=mask.sum())print(&quot;原始标签：&quot;, original_labels)print(&quot;扰动后的标签：&quot;, perturbed_labels)在上述代码中，我们定义了一个原始标签数组 original_labels。然后，通过指定扰动概率 perturbation_probability，使用 np.random.random() 函数生成与原始标签长度相同的随机掩码，根据掩码将部分标签进行随机替换。输出结果示例：原始标签： [0 1 2 3 4]扰动后的标签： [0 1 4 3 4]可以看到，一部分标签被随机替换成其他随机选择的标签，引入了较大程度的标签变化。 标签翻转（Label Flipping）：在这种方法中，我们随机选择一部分标签，并将其翻转为其他标签。例如，对于二分类问题，可以将部分正类标签翻转为负类标签，或将部分负类标签翻转为正类标签。这种方法通常用于处理标签噪声的情况。下面是一个示例代码，演示如何使用标签翻转对标签进行扰动：import numpy as np# 原始标签original_labels = np.array([0, 1, 0, 1, 0])# 扰动概率perturbation_probability = 0.2# 标签扰动 - 标签翻转perturbed_labels = original_labels.copy()mask = np.random.random(len(original_labels)) &amp;lt; perturbation_probabilityperturbed_labels[mask] = 1 - perturbed_labels[mask]print(&quot;原始标签：&quot;, original_labels)print(&quot;扰动后的标签：&quot;, perturbed_labels)" }, { "title": "机器学习基础-监督学习-标签增强之标签平移（Label Shift）", "url": "/posts/%E6%A0%87%E7%AD%BE-26-%E6%A0%87%E7%AD%BE%E6%A0%87%E7%AD%BE%E5%A2%9E%E5%BC%BA%E4%B9%8B%E6%A0%87%E7%AD%BE%E5%B9%B3%E7%A7%BB/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-05 00:00:00 +0800", "snippet": "标签平移（Label Shift）是一种常见的标签增强技术，它通过对原始标签进行平移操作，生成具有相似语义含义但略有差异的新标签。标签平移可以用于分类问题或回归问题，旨在增加数据集的多样性和提高模型的泛化能力。对于分类问题，标签平移可以在离散标签的情况下应用。假设原始标签的取值范围为 {0, 1, 2, …, K-1}，标签平移操作将原始标签平移一个固定的偏移量，生成新的标签。以下是一个示例代码，演示如何进行标签平移的标签增强：import numpy as np# 原始标签original_labels = np.array([0, 1, 2, 3, 4])# 平移偏移量shift_amount = 2# 标签平移augmented_labels = original_labels + shift_amountprint(&quot;原始标签：&quot;, original_labels)print(&quot;增强后的标签：&quot;, augmented_labels)在上述代码中，我们定义了一个原始标签数组 original_labels，然后使用平移偏移量 shift_amount 对标签进行平移操作。最后，我们打印出原始标签和增强后的标签。输出结果为：原始标签： [0 1 2 3 4]增强后的标签： [2 3 4 5 6]可以看到，原始标签经过平移操作后生成了增强后的标签，每个标签都增加了 2。通过这样的标签平移操作，我们可以生成具有相似语义含义但略有差异的新标签。对于回归问题，标签平移可以应用于连续标签的情况。在回归问题中，可以将原始标签进行平移操作，生成具有相似语义含义但略有差异的新标签。具体的平移操作可以根据问题的需求进行定义，例如固定偏移量、按比例缩放等。需要注意的是，标签平移操作可能会导致标签超出原始标签的取值范围。在这种情况下，可以根据需求进行处理，例如对于超出范围的标签进行边界处理或循环处理。总结来说，标签平移是一种标签增强技术，通过对原始标签进行平移操作生成具有相似语义含义但略有差异的新标签。标签平移可以应用于分类问题或回归问题，用于增加数据集的多样性和提高模型的泛化能力。具体的标签平移操作可以根据问题的性质和需求进行定义。当涉及到标签平移时，可以根据具体的需求和问题进行不同类型的标签平移操作。以下是一些常见的标签平移技术： 固定偏移量平移：这是最简单的标签平移技术，通过将所有标签增加或减去一个固定的偏移量来实现。这种方法适用于离散标签的分类问题或连续标签的回归问题。示例代码如下所示：import numpy as np# 原始标签original_labels = np.array([0, 1, 2, 3, 4])# 平移偏移量shift_amount = 2# 标签平移augmented_labels = original_labels + shift_amountprint(&quot;原始标签：&quot;, original_labels)print(&quot;增强后的标签：&quot;, augmented_labels)输出结果为：原始标签： [0 1 2 3 4]增强后的标签： [2 3 4 5 6] 比例缩放平移：这种方法通过将标签乘以一个比例因子来实现标签平移。比例因子可以大于 1 表示增加标签值，小于 1 表示减小标签值。这种方法适用于回归问题中的连续标签。示例代码如下所示：import numpy as np# 原始标签original_labels = np.array([0, 1, 2, 3, 4])# 缩放因子scale_factor = 1.5# 标签平移augmented_labels = original_labels * scale_factorprint(&quot;原始标签：&quot;, original_labels)print(&quot;增强后的标签：&quot;, augmented_labels)输出结果为：原始标签： [0 1 2 3 4]增强后的标签： [0. 1.5 3. 4.5 6. ] 循环平移：这种方法适用于具有循环性质的标签，例如星期几的标签（1-7）。在循环平移中，将标签按照固定的周期进行平移，使得平移后的标签仍然落在原始标签范围内。示例代码如下所示：import numpy as np# 原始标签original_labels = np.array([1, 2, 3, 4, 5, 6, 7])# 平移偏移量shift_amount = 3# 标签平移augmented_labels = (original_labels + shift_amount - 1) % 7 + 1print(&quot;原始标签：&quot;, original_labels)print(&quot;增强后的标签：&quot;, augmented_labels)在这个示例中，我们假设原始标签是星期几的标签，范围为 1 到 7。我们使用了循环平移的技术，将标签按照固定的周期进行平移。在这种情况下，我们选择周期为 7，所以平移偏移量加上原始标签再减去 1（因为标签从 1 开始），然后取结果与 7 取模，再加上 1，以确保平移后的标签仍然在 1 到 7 的范围内。输出结果为：原始标签： [1 2 3 4 5 6 7]增强后的标签： [4 5 6 7 1 2 3]可以看到，原始标签经过循环平移操作后生成了增强后的标签，每个标签按照给定的平移偏移量进行了循环平移。这样，我们通过标签平移操作生成了具有相似语义含义但略有差异的新标签。需要注意的是，在实际应用中，标签平移的具体方法和实现会根据问题的性质和数据集的特点而有所不同。可以根据具体的需求选择合适的标签平移技术，并根据需要进行参数调整和实验验证。" }, { "title": "机器学习基础-监督学习-标签增强", "url": "/posts/%E6%A0%87%E7%AD%BE-25-%E6%A0%87%E7%AD%BE%E6%A0%87%E7%AD%BE%E5%A2%9E%E5%BC%BA/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-05 00:00:00 +0800", "snippet": "标签增强（Label Augmentation）是一种数据增强技术，用于增加数据集的多样性和覆盖度，从而提高模型的泛化能力。标签增强的目标是通过对原始标签进行变换或扩展，生成具有相同或相似语义含义的新标签，以扩展训练数据的标签空间。以下是一些常见的标签增强技术：标签平移（Label Shift）：对于回归问题或离散标签的分类问题，可以对标签进行平移操作，使其沿着某个方向上移动一个固定的偏移量。这样可以生成具有相似语义含义但略有差异的新标签。标签扰动（Label Perturbation）：对于分类问题，可以对标签进行扰动操作，引入一定的噪声或随机变化。例如，对于图像分类问题，可以对图像的标签进行随机剪切、旋转、缩放等变换，从而生成具有相似语义含义但略有差异的新标签。标签插值（Label Interpolation）：对于回归问题或连续标签的分类问题，可以使用插值方法生成新的标签。例如，在时间序列预测问题中，可以使用线性插值或样条插值方法生成介于两个标签之间的新标签。下面是一个简单的示例代码，演示如何进行标签平移的标签增强：import numpy as np# 原始标签original_labels = np.array([0, 1, 2, 3, 4])# 平移偏移量shift_amount = 2# 标签平移augmented_labels = original_labels + shift_amountprint(&quot;原始标签：&quot;, original_labels)print(&quot;增强后的标签：&quot;, augmented_labels)在上述代码中，我们定义了一个原始标签数组 original_labels，然后使用平移偏移量 shift_amount 对标签进行平移操作。最后，我们打印出原始标签和增强后的标签。输出结果为：原始标签： [0 1 2 3 4]增强后的标签： [2 3 4 5 6]可以看到，原始标签经过平移操作后生成了增强后的标签，每个标签都增加了 2。这样，我们可以通过标签增强来生成更多的样本，扩展数据集的标签空间，提高模型的泛化能力。需要注意的是，在实际应用中，标签增强的具体方法和实现会根据问题的性质和数据集的特点而有所不同。可以根据具体的需求选择合适的标签增强技术，并根据需求进行适当的参数调整和实验验证。除了上述的标签平移示例，还可以使用其他的标签增强技术来扩展数据集的标签空间。例如，在图像分类问题中，可以使用图像变换操作来进行标签增强，如随机剪切、旋转、缩放、亮度调整等。以下是一个示例代码，展示如何使用图像增强库 Albumentations 进行标签增强：import numpy as npimport albumentations as Afrom PIL import Image# 原始标签original_labels = np.array([0, 1, 2, 3, 4])# 图像增强变换transform = A.Compose([ A.RandomCrop(width=100, height=100), A.HorizontalFlip(p=0.5), A.RandomBrightnessContrast(p=0.2)])# 图像增强augmented_labels = []for label in original_labels: augmented_image = transform(image=np.zeros((100, 100, 3))) # 这里假设输入图像是100x100x3的黑色图像 augmented_labels.append(label)print(&quot;原始标签：&quot;, original_labels)print(&quot;增强后的标签：&quot;, augmented_labels)在上述代码中，我们使用 Albumentations 库来定义了一组图像增强变换操作。然后，我们遍历原始标签数组，并使用一个虚拟的黑色图像（大小为 100x100x3）作为输入，应用图像增强变换来生成增强后的图像和标签。需要注意的是，这个示例中的图像增强只是为了演示目的，并没有使用真实的图像数据集。在实际应用中，可以将该代码与真实的图像数据集结合使用，从而生成具有增强标签的图像样本。总结来说，标签增强是一种数据增强技术，用于增加数据集的多样性和覆盖度，提高模型的泛化能力。通过对标签进行变换或扩展，可以生成具有相似语义含义但略有差异的新标签。标签增强的具体方法和实现根据问题的性质和数据集的特点而有所不同，可以根据具体需求选择合适的技术和库来进行实现。" }, { "title": "机器学习基础-监督学习-标签噪声处理之期望最大化（Expectation-Maximization，简称EM）", "url": "/posts/%E6%A0%87%E7%AD%BE-24-%E6%A0%87%E7%AD%BE%E5%99%AA%E5%A3%B0%E5%A4%84%E7%90%86%E4%B9%8B%E6%9C%9F%E6%9C%9B%E6%9C%80%E5%A4%A7%E5%8C%96/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-05 00:00:00 +0800", "snippet": "期望最大化（Expectation-Maximization，简称 EM）是一种经典的迭代优化算法，用于解决含有潜在变量（或未观测变量）的统计模型的参数估计问题。EM 算法通过迭代的方式，通过不断估计模型参数和潜在变量的期望值（E 步骤）和最大化似然函数来更新模型参数（M 步骤），直至收敛到最优解。下面是 EM 算法的一般步骤： 初始化模型参数。 E 步骤（Expectation Step）：根据当前的模型参数，计算潜在变量的后验概率（或期望值），即给定观测数据和当前模型参数下，每个潜在变量的可能取值的概率。 M 步骤（Maximization Step）：利用 E 步骤计算得到的潜在变量的期望值，通过最大化似然函数或期望似然函数，更新模型参数。 重复执行 E 步骤和 M 步骤，直至达到收敛条件（例如，最大迭代次数或参数变化小于某个阈值）。 输出最终收敛的模型参数。 下面是一个简单的示例代码，演示了 EM 算法的应用，假设我们有一组服从正态分布的观测数据，但我们无法观测到数据的真实标签（潜在变量）。我们使用 EM 算法估计出模型的均值和标准差。import numpy as npfrom scipy.stats import norm# 生成服从正态分布的观测数据np.random.seed(0)data = np.concatenate([np.random.normal(0, 1, 500), np.random.normal(5, 1, 500)])# 初始化模型参数mu_1 = np.random.randn()mu_2 = np.random.randn()sigma_1 = np.random.rand()sigma_2 = np.random.rand()weights = np.random.rand()# EM算法迭代max_iterations = 100epsilon = 1e-6log_likelihood = -np.inffor iteration in range(max_iterations): # E步骤：计算后验概率 posterior_1 = norm.pdf(data, mu_1, sigma_1) * weights posterior_2 = norm.pdf(data, mu_2, sigma_2) * (1 - weights) total = posterior_1 + posterior_2 posterior_1 /= total posterior_2 /= total # M步骤：更新模型参数 weights = np.mean(posterior_1) mu_1 = np.sum(posterior_1 * data) / np.sum(posterior_1) mu_2 = np.sum(posterior_2 * data) / np.sum(posterior_2) sigma_1 = np.sqrt(np.sum(posterior_1 * (data - mu_1) ** 2) / np.sum(posterior_1)) sigma_2 = np.sqrt(np.sum(posterior_2 * (data - mu_2) ** 2) / np.sum(posterior_2)) # 计算对数似然函数 current_log_likelihood = np.sum(np.log(posterior_1 * weights + posterior_2 * (1 - weights))) # 判断是否收敛 if current_log_likelihood - log_likelihood &amp;lt; epsilon: break else: log_likelihood = current_log_likelihood# 输出最终收敛的模型参数print(&quot;模型参数：&quot;)print(&quot;mu_1 =&quot;, mu_1)print(&quot;mu_2 =&quot;, mu_2)print(&quot;sigma_1 =&quot;, sigma_1)print(&quot;sigma_2 =&quot;, sigma_2)print(&quot;weights =&quot;, weights)在上述示例代码中，我们使用 EM 算法估计了具有两个正态分布的混合模型的参数。首先，我们生成了服从两个不同正态分布的观测数据。然后，通过初始化模型参数并迭代执行 E 步骤和 M 步骤，逐步更新模型参数，直至达到收敛条件。最终输出收敛的模型参数，包括两个正态分布的均值、标准差和混合权重。需要注意的是，EM 算法的具体实现可以根据问题的不同而有所调整。上述示例代码仅为简化版的示例，实际应用中可能需要进行更复杂的模型假设和参数更新。此外，EM 算法也有一些变种和改进算法，以处理更复杂的情况和提高收敛性能。" }, { "title": "机器学习基础-监督学习-标签噪声处理之多数投票（Majority Voting）", "url": "/posts/%E6%A0%87%E7%AD%BE-23-%E6%A0%87%E7%AD%BE%E5%99%AA%E5%A3%B0%E5%A4%84%E7%90%86%E4%B9%8B%E5%A4%9A%E6%95%B0%E6%8A%95%E7%A5%A8/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-05 00:00:00 +0800", "snippet": "多数投票（Majority Voting）是一种常用的标签噪声处理方法，它通过利用多个模型的预测结果来消除标签噪声的影响。这种方法假设多个模型的预测结果相对于噪声是一致的，因此通过多数投票可以得到更准确的标签估计结果。下面详细介绍多数投票的原理和使用方法，包括代码示例：原理：假设有一个样本的标签数据经过多个模型的预测，每个模型的预测结果可以表示为一个向量或数组。多数投票方法选择每个样本的最频繁出现的预测结果作为最终的标签。如果有多个预测结果出现的次数相同，可以随机选择一个结果或使用其他的决策规则。使用方法：收集多个模型的预测结果，可以是不同算法的模型或同一算法的不同训练结果。对于每个样本，根据多个模型的预测结果进行多数投票。将得票最多的预测结果作为最终的标签。下面是一个简单的示例代码，演示了多数投票方法处理标签噪声的过程：import numpy as np# 假设有3个模型的预测结果model1_predictions = np.array([0, 1, 0, 1, 1, 1, 0, 0, 0, 1])model2_predictions = np.array([0, 1, 1, 0, 1, 1, 0, 1, 1, 0])model3_predictions = np.array([0, 1, 1, 0, 0, 1, 0, 1, 0, 0])# 多数投票final_predictions = np.zeros_like(model1_predictions)for i in range(len(final_predictions)): votes = [model1_predictions[i], model2_predictions[i], model3_predictions[i]] final_predictions[i] = np.argmax(np.bincount(votes))print(&quot;模型1的预测结果：&quot;, model1_predictions)print(&quot;模型2的预测结果：&quot;, model2_predictions)print(&quot;模型3的预测结果：&quot;, model3_predictions)print(&quot;多数投票的最终预测结果：&quot;, final_predictions)在上述代码中，我们假设有 3 个模型的预测结果：model1_predictions、model2_predictions 和 model3_predictions。使用 np.bincount() 函数计算每个样本的预测结果出现的次数，然后使用 np.argmax() 函数选择得票最多的预测结果作为最终的标签。需要注意的是，多数投票方法在处理标签噪声时可以提高预测的准确性，但也要注意模型预测的一致性和噪声的影响程度。以下是一些进一步的注意事项和技巧：模型预测的一致性：多数投票方法的有效性取决于不同模型的预测结果是否一致。如果多个模型的预测结果差异较大，多数投票可能无法有效处理标签噪声。因此，在应用多数投票之前，建议评估模型之间的一致性，确保它们能够提供相似的预测。权重投票：在多数投票中，每个模型的预测结果通常被视为相等的。然而，如果某些模型的性能更好或更可靠，可以考虑为它们分配更高的权重。权重投票可以通过为每个模型分配权重并将其考虑在内来进行。多数投票决策规则：如果有多个预测结果出现的次数相同，可以使用其他决策规则来选择最终的标签。例如，可以随机选择一个结果作为最终标签，或者使用置信度较高的模型的预测结果。集成学习方法：多数投票方法可以看作是一种简单的集成学习方法，将多个模型的预测结果进行整合。除了多数投票，还可以尝试其他集成学习方法，如加权投票、堆叠（stacking）等，以进一步提高预测性能。总之，多数投票方法是一种常用且有效的标签噪声处理技术。通过整合多个模型的预测结果，可以减轻标签噪声的影响，得到更准确的标签估计。根据具体的问题和数据集，可以选择适当的决策规则和集成学习方法来优化多数投票的效果。" }, { "title": "机器学习基础-监督学习-标签噪声处理之清洗和修正标签", "url": "/posts/%E6%A0%87%E7%AD%BE-22-%E6%A0%87%E7%AD%BE%E5%99%AA%E5%A3%B0%E5%A4%84%E7%90%86%E4%B9%8B%E6%B8%85%E6%B4%97%E5%92%8C%E4%BF%AE%E6%AD%A3%E6%A0%87%E7%AD%BE/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-05 00:00:00 +0800", "snippet": "清洗和修正标签是一种标签噪声处理方法，它涉及对标签数据进行人工检查和修正。该方法适用于数据集中标签噪声比例较低的情况，其中人工检查和修正可以通过人工标注的方式进行，或者通过其他数据验证方法来识别和修正标签错误。下面是一个详细的步骤来进行清洗和修正标签的过程：数据准备：准备原始标签数据和包含噪声的标签数据。这些数据可以表示为向量、矩阵或数据帧的形式。标签检查：通过可视化工具或统计分析方法，检查标签数据中的异常或错误。可以查看标签分布、标签之间的关联性、标签与其他特征的关系等。识别可能存在错误的标签样本。标签修正：根据标签检查的结果，对错误的标签进行修正。修正可以手动进行，即通过人工干预将错误的标签修改为正确的标签。也可以使用自动化的方法，例如基于规则、机器学习或深度学习模型来辅助修正。标签更新：将修正后的标签更新到数据集中，确保与原始数据对应。下面是一个简单的示例代码，演示了清洗和修正标签的过程：import numpy as np# 假设原始标签数据labels = np.array([0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0])# 生成包含标签噪声的数据noisy_labels = np.array([0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0])# 标签清洗和修正cleaned_labels = np.copy(noisy_labels)# 手动修正标签错误cleaned_labels[2] = 0cleaned_labels[7] = 0print(&quot;原始标签：&quot;, labels)print(&quot;包含噪声的标签：&quot;, noisy_labels)print(&quot;清洗后的标签：&quot;, cleaned_labels)在上述代码中，我们假设有一组原始的标签数据和包含噪声的标签数据。通过人工检查和修正，我们手动修正了两个标签错误，即将索引为 2 和 7 的标签修正为 0。最终，得到了清洗后的标签数据。需要注意的是，标签清洗和修正的过程通常是基于人工的干预和判断，所以它的适用范围相对较窄，对于大规模数据集或高比例的标签噪声来说可能不太实际。在实实际应用中，可以结合其他方法来提高清洗和修正标签的效果。以下是一些常用的方法和技术： 一致性检查：将标签与其他特征数据进行比较，检查它们之间的一致性。例如，对于图像分类任务，可以使用图像特征进行特征提取，并与标签进行比较，如果存在明显不一致的情况，则可以怀疑标签存在错误，并进行修正。 集成学习：使用集成学习方法，如随机森林或梯度提升树，可以从不同的角度对数据进行建模，并获得不同的预测结果。通过对不同模型的预测结果进行比较和分析，可以检测和纠正标签错误。 半监督学习：利用半监督学习的方法，将未标记的样本与标记的样本结合起来进行训练。通过使用未标记样本的信息，可以更好地捕捉标签数据中的分布和结构，从而减少标签噪声的影响。 人工智能辅助：使用机器学习或深度学习模型来辅助标签修正。可以训练一个模型来预测标签，并与人工标注进行对比。如果模型的预测结果与人工标注不一致，那么很可能存在标签错误，需要进行修正。 在实际应用中，清洗和修正标签可能需要迭代多次，结合不同的方法和技术，并根据具体问题和数据集的特点进行调整和优化。标签噪声处理是一个复杂的任务，需要综合考虑数据质量、问题的复杂性以及可行性等因素来选择合适的方法和策略。以上是一些常见的清洗和修正标签的方法，具体选择何种方法取决于具体的问题和数据集的特点。在实践中，可以根据实际情况灵活应用这些方法，以获得准确和可靠的标签数据。" }, { "title": "机器学习基础-监督学习-标签噪声处理", "url": "/posts/%E6%A0%87%E7%AD%BE-21-%E6%A0%87%E7%AD%BE%E5%99%AA%E5%A3%B0%E5%A4%84%E7%90%86/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-05 00:00:00 +0800", "snippet": "标签噪声是指在有监督学习中，标签数据中存在错误或不准确的情况。这种噪声可能由于人工标注的错误、数据采集的问题或其他原因引起。标签噪声可能会对模型的训练产生负面影响，导致模型学习到错误的模式或产生误导性的预测结果。因此，处理标签噪声是非常重要的。下面介绍几种常见的标签噪声处理方法： 清洗和修正标签：这种方法涉及对标签数据进行人工检查和修正。可以通过验证一部分样本的标签是否正确，并根据需要手动修正错误的标签。这种方法适用于数据集中标签噪声的比例较低的情况。 多数投票（Majority Voting）：在多数投票方法中，通过使用多个模型的预测结果来消除标签噪声的影响。可以训练多个模型，然后将它们的预测结果进行投票或求平均值。假设有 3 个模型，如果它们对一个样本的预测结果分别为 0、0 和 1，则可以选择将该样本的标签确定为 0，因为 0 是多数投票的结果。 期望最大化（Expectation-Maximization，简称 EM）：EM 算法是一种经典的标签噪声处理方法，它通过迭代优化的方式估计数据中的标签噪声。该方法假设标签噪声是一个潜在变量，通过不断迭代地估计模型参数和潜在变量，最终得到更准确的标签估计结果。 下面是一个简单的示例代码，演示了多数投票方法处理标签噪声的过程：import numpy as npfrom scipy.stats import mode# 假设原始标签数据labels = np.array([0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0])# 生成包含标签噪声的数据noisy_labels = np.array([0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0])# 使用多数投票进行标签纠正corrected_labels = mode(noisy_labels).modeprint(&quot;原始标签：&quot;, labels)print(&quot;包含噪声的标签：&quot;, noisy_labels)print(&quot;纠正后的标签：&quot;, corrected_labels)在上述代码中，我们假设有一组原始的标签数据和包含噪声的标签数据。使用 scipy.stats 中的 mode 函数对噪声标签进行多数投票处理，返回多数投票的结果作为纠正后的标签。在这个示例中，原始标签是 [0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0]，包含噪声的标签是 [0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0]，经过多数投票处理后，纠正后的标签是 [0]。需要注意的是，上述示例是一个简单的演示，实际应用中的标签噪声处理可能更加复杂。具体选择何种标签噪声处理方法取决于问题的性质、数据集的特点和噪声的来源。在实际应用中，通常需要进行实验和评估，选择最适合特定问题和数据集的标签噪声处理方法。此外，还有其他的标签噪声处理方法，如使用半监督学习、基于图模型的方法、深度学习方法等。这些方法可以根据具体情况进行进一步的研究和应用，以达到更好的标签噪声处理效果。" }, { "title": "机器学习基础-监督学习-标签编码之哈希编码（Hash Encoding）", "url": "/posts/%E6%A0%87%E7%AD%BE-20-%E6%A0%87%E7%AD%BE%E7%BC%96%E7%A0%81%E4%B9%8B%E5%93%88%E5%B8%8C%E7%BC%96%E7%A0%81/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-05 00:00:00 +0800", "snippet": "哈希编码（Hash Encoding）是一种将标签映射为固定长度的二进制数的编码方法，其中相同的标签映射为相同的二进制数，不同的标签映射为不同的二进制数。哈希编码可以解决标签数量较大的问题，但可能会出现哈希冲突的问题。哈希编码的实现可以使用哈希函数，将每个标签映射为一个哈希值，再将哈希值转换为二进制数。常用的哈希函数包括 MD5、SHA-1 等。下面是一个使用 Python 实现哈希编码的示例代码：import numpy as npimport hashlibdef hash_encode(labels, n_bins): &quot;&quot;&quot; 使用哈希编码对标签进行编码。 参数： - labels: ndarray，待编码的标签，形状为(n_samples,)。 - n_bins: int，哈希空间的大小。 返回： - encoded_labels: ndarray，编码后的标签，形状为(n_samples, n_bins)。 &quot;&quot;&quot; n_samples = len(labels) encoded_labels = np.zeros((n_samples, n_bins)) for i, label in enumerate(labels): for j in range(n_bins): hashed_label = hashlib.sha256(str.encode(label + str(j))).hexdigest() bin_idx = int(hashed_label, 16) % n_bins encoded_labels[i, bin_idx] = 1 return encoded_labels在上面的代码中，我们首先定义了一个 hash_encode 函数，它接受一个待编码的标签数组 labels 和一个哈希空间大小 n_bins 作为输入，并返回一个编码后的标签数组 encoded_labels。在函数内部，我们首先创建一个形状为(n_samples, n_bins)的全零数组 encoded_labels 来存储编码后的标签。然后对于每个标签，我们都对其进行哈希编码，得到一个哈希值，并将其映射到哈希空间中的一个位置上。具体来说，我们使用 Python 标准库中的 hashlib 模块来计算标签的哈希值，然后将哈希值转换为整数，并对哈希空间的大小取模，以得到该标签在哈希空间中的位置。最后，我们将该标签对应的位置上的元素设置为 1，表示该标签被编码为该位置对应的二进制数。最终，函数返回编码后的标签数组 encoded_labels。需要注意的是，哈希编码的哈希函数和哈希值的长度都是需要根据具体的问题和数据集选择的，如果哈希值的长度过短，可能会出现哈希冲突的问题。哈希编码可以用于处理具有大量标签的分类问题。相比独热编码和标签编码，哈希编码的优点是可以自适应地处理任意数量的标签，而且在处理大量标签的情况下所需的存储空间更小。然而，哈希编码也存在一些缺点。首先，由于哈希函数是通过将标签映射到一个较小的空间中，因此可能会出现哈希冲突的情况，即不同的标签被映射为相同的哈希值。这种情况下，就无法准确地区分这些标签，可能会影响模型的性能。其次，由于哈希函数是通过对输入进行随机化的操作得到的，因此哈希编码在每次运行时都会产生不同的结果。这意味着对于同一个标签，它的哈希编码可能不一致，这可能会影响模型的可重现性。哈希编码具有以下优点： 哈希编码可以将任意类型的标签编码为固定长度的二进制向量，因此适用于对于类别数很大或者不确定的分类问题。 哈希编码不需要事先定义一个固定的编码映射表，因此具有很高的灵活性和可扩展性。 哈希编码可以有效地减少编码所需的内存空间，尤其是在处理大型数据集时。但是，哈希编码也存在一些缺点： 由于哈希函数是随机生成的，因此可能存在不同标签被哈希到同一个位置的情况，即哈希冲突。这可能会导致编码结果的精度下降。 由于哈希函数的随机性，相同的标签在不同的哈希空间中可能被编码为不同的二进制向量。这可能会导致编码结果的不稳定性。 由于哈希函数的计算量较大，哈希编码的编码速度可能较慢。因此，在选择编码方法时，需要根据具体的问题和数据集选择合适的编码方法，以达到最优的分类效果。" }, { "title": "机器学习基础-监督学习-标签编码之频率编码（Frequency Encoding）", "url": "/posts/%E6%A0%87%E7%AD%BE-19-%E6%A0%87%E7%AD%BE%E7%BC%96%E7%A0%81%E4%B9%8B%E9%A2%91%E7%8E%87%E7%BC%96%E7%A0%81/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-05 00:00:00 +0800", "snippet": "频率编码（Frequency Encoding）是一种标签编码方法，它将每个标签都映射为其出现频率。在分类问题中，频率编码可以用于解决标签之间距离和相关性不明显的问题。频率编码的具体实现可以分为以下步骤： 统计每个标签出现的频率。 将每个标签映射为其出现频率。下面给出一个使用 Python 实现频率编码的示例代码：import pandas as pd# 创建一个包含标签的DataFramedf = pd.DataFrame({&#39;label&#39;: [&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;A&#39;, &#39;B&#39;, &#39;B&#39;]})# 统计每个标签的频率freq = df[&#39;label&#39;].value_counts(normalize=True)# 将每个标签映射为其出现频率df[&#39;label_freq&#39;] = df[&#39;label&#39;].map(freq)print(df)运行上述代码，输出结果如下： label label_freq0 A 0.3333331 B 0.5000002 C 0.1666673 A 0.3333334 B 0.5000005 B 0.500000在上述示例代码中，我们使用 pandas 库创建了一个包含标签的 DataFrame，并统计了每个标签的频率。然后，我们使用 DataFrame 的 map()方法将每个标签映射为其出现频率，并将结果存储在另一列中。最终输出的结果中，每个标签都被映射为其出现频率。频率编码能够将每个标签映射为一个实数，可以更方便地用于计算在频率编码中，每个标签都被映射为其出现频率，因此它们都可以被表示为一个实数。这使得频率编码在一些需要对标签进行数值计算的情况下比其他标签编码方法更方便，例如：计算标签之间的距离：可以使用欧几里得距离或曼哈顿距离等距离度量方法计算标签之间的距离。进行聚类：可以使用聚类算法对标签进行聚类，以发现它们之间的相似性。进行回归分析：可以将标签频率作为自变量，将标签相关的数据作为因变量进行回归分析，以了解它们之间的关系。下面以计算标签之间的距离为例，给出一个使用频率编码的示例代码：import pandas as pdimport numpy as np# 创建一个包含标签的DataFramedf = pd.DataFrame({&#39;label&#39;: [&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;A&#39;, &#39;B&#39;, &#39;B&#39;]})# 统计每个标签的频率freq = df[&#39;label&#39;].value_counts(normalize=True)# 将每个标签映射为其出现频率df[&#39;label_freq&#39;] = df[&#39;label&#39;].map(freq)# 计算标签之间的欧几里得距离labels = [&#39;A&#39;, &#39;B&#39;, &#39;C&#39;]dist_matrix = np.zeros((len(labels), len(labels)))for i in range(len(labels)): for j in range(len(labels)): freq_i = freq[labels[i]] freq_j = freq[labels[j]] dist_matrix[i][j] = np.sqrt((freq_i - freq_j) ** 2)print(dist_matrix)运行上述代码，输出结果如下：[[0. 0.16666667 0.16666667] [0.16666667 0. 0.33333333] [0.16666667 0.33333333 0. ]]在上述示例代码中，我们使用 numpy 库创建了一个距离矩阵（dist_matrix），用于存储标签之间的距离。然后，我们使用 DataFrame 的 map()方法将每个标签映射为其出现频率，并将结果存储在另一列中。最后，我们遍历标签列表，计算每一对标签之间的欧几里得距离，并将结果填充到距离矩阵中。最终输出的结果中，每个标签之间的距离都被计算出来并以实数的形式表示。频率编码无法处理新出现的标签，因为它只能将已有的标签映射为它们的出现频率频率编码（Frequency Encoding）能够将每个标签映射为其出现的频率，但是无法处理新出现的标签，因为它只能将已有的标签映射为它们的出现频率。考虑下面的例子，在一个数据集中，有三个标签 A、B、C，出现的频率分别为 0.4、0.3、0.3。我们可以使用频率编码将这三个标签映射为它们的出现频率，得到如下的映射表： 标签 频率编码 A 0.4 B 0.3 C 0.3 现在，如果在这个数据集中出现了一个新的标签 D，我们无法使用频率编码将它映射为一个实数，因为它没有出现过。在这种情况下，通常的做法是将新的标签视为一类特殊的标签，并为它们分配一个特殊的编码。可以使用一些特殊的值，例如 0 或-1，来表示新出现的标签。下面给出一个使用 Python 实现频率编码处理新出现标签的示例代码：import pandas as pd# 创建一个包含标签的DataFramedf = pd.DataFrame({&#39;label&#39;: [&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;A&#39;, &#39;B&#39;, &#39;B&#39;]})# 统计每个标签的频率freq = df[&#39;label&#39;].value_counts(normalize=True)# 将每个标签映射为其出现频率df[&#39;label_freq&#39;] = df[&#39;label&#39;].map(freq)# 处理新出现的标签new_label = &#39;D&#39;if new_label in freq.index: # 标签已存在，使用频率编码 new_label_freq = freq[new_label]else: # 标签不存在，使用特殊编码 new_label_freq = 0print(new_label_freq)运行上述代码，输出结果为：0在上述示例代码中，我们首先使用 pandas 库创建了一个包含标签的 DataFrame，并统计了每个标签的频率。然后，我们使用 DataFrame 的 map()方法将每个标签映射为其出现频率，并将结果存储在另一列中。最后，我们处理新出现的标签 D，如果它已存在，则使用频率编码将它映射为它的出现频率，否则将它映射为特殊编码 0。需要注意的是，使用特殊编码来处理新出现的标签可能会导致一些问题。例如，如果特殊编码与某些已有的标签编码非常接近，可能会导致误差。因此，需要谨慎使用特殊编码，根据具体问题和数据集选择合适的方法。" }, { "title": "机器学习基础-监督学习-标签编码之二进制编码（Binary Encoding）", "url": "/posts/%E6%A0%87%E7%AD%BE-18-%E6%A0%87%E7%AD%BE%E7%BC%96%E7%A0%81%E4%B9%8B%E4%BA%8C%E8%BF%9B%E5%88%B6%E7%BC%96%E7%A0%81/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-05 00:00:00 +0800", "snippet": "二进制编码（Binary Encoding）是一种将每个标签都映射为一个二进制数的编码方法，其中每一位代表一个可能的取值。相比于独热编码，二进制编码可以减少编码的位数，适用于标签数量较少的问题。二进制编码的具体实现方法是将每个标签都转换为二进制数，并将每一位对应到一个新的特征。例如，对于三个标签 A、B 和 C，其二进制编码可以如下所示： 标签 二进制编码 A 00 B 01 C 10 可以看到，每个标签都被编码为一个由两位二进制数组成的向量，向量中的每一位都代表一个可能的取值。在实际应用中，可以使用 Python 的位运算符实现二进制编码。下面是一个使用 numpy 库实现二进制编码的示例代码：import numpy as npdef binary_encoding(labels): # 将标签转换为整数 labels_int = np.arange(len(labels)) # 计算二进制编码的位数 num_bits = int(np.ceil(np.log2(len(labels)))) # 初始化编码矩阵 encoding_matrix = np.zeros((len(labels), num_bits), dtype=int) # 对每个标签进行编码 for i, label in enumerate(labels_int): # 将标签转换为二进制数，并逆序排列 binary = np.binary_repr(label, width=num_bits)[::-1] # 将二进制数按位存储到编码矩阵中 for j, bit in enumerate(binary): encoding_matrix[i, j] = int(bit) return encoding_matrix该函数接受一个标签列表作为输入，返回一个由 0 和 1 组成的二维矩阵，其中每一行代表一个标签的二进制编码。在该实现中，使用 numpy 库中的 arange 函数生成标签的整数编号，使用 ceil 函数计算二进制编码的位数，使用 binary_repr 函数将整数转换为二进制数，并使用[::-1]将二进制数逆序排列，最后将二进制数按位存储到编码矩阵中。二进制编码的主要优点是可以减少编码的位数，从而减少特征空间的维度。此外，二进制编码可以将标签之间的相关性考虑在内，因为相邻的二进制数具有更多的相同位数。但是，二进制编码也存在一些缺点。首先，由于二进制编码中每个标签都对应一个固定的二进制数，因此在处理新的标签时，需要重新计算编码矩阵，这可能会导致额外的计算成本。其次，由于相邻的二进制数具有更多的相同位数，因此可能会导致一些不必要的决策边界，从而影响模型的性能。因此，在选择标签编码方法时，需要根据具体的问题和数据集进行选择，综合考虑编码的位数、计算成本和模型性能等因素。如果标签数量较少且标签之间的相关性较强，可以考虑使用二进制编码。否则，可以考虑使用独热编码或其他编码方法。下面是一个使用二进制编码训练神经网络的示例代码：import numpy as npfrom sklearn.preprocessing import LabelEncoderfrom sklearn.model_selection import train_test_splitfrom keras.models import Sequentialfrom keras.layers import Dense# 加载数据集data = np.loadtxt(&#39;data.csv&#39;, delimiter=&#39;,&#39;)X = data[:, :-1]y = data[:, -1]# 对标签进行二进制编码encoder = LabelEncoder()y_encoded = encoder.fit_transform(y)y_binary = binary_encoding(y_encoded)# 划分训练集和测试集X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.2)# 定义神经网络模型model = Sequential()model.add(Dense(64, input_dim=X.shape[1], activation=&#39;relu&#39;))model.add(Dense(y_binary.shape[1], activation=&#39;softmax&#39;))model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=&#39;adam&#39;, metrics=[&#39;accuracy&#39;])# 训练神经网络model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))在该示例代码中，首先使用 sklearn 库中的 LabelEncoder 对标签进行编码，然后使用上文提到的 binary_encoding 函数将编码后的标签转换为二进制数。然后使用 train_test_split 函数划分训练集和测试集，使用 Keras 库定义神经网络模型，并使用 fit 函数训练模型。由于使用了二进制编码，因此输出层的节点数等于二进制编码的位数。" }, { "title": "机器学习基础-监督学习-标签编码之独热编码（One-Hot Encoding）", "url": "/posts/%E6%A0%87%E7%AD%BE-17-%E6%A0%87%E7%AD%BE%E7%BC%96%E7%A0%81%E4%B9%8B%E7%8B%AC%E7%83%AD%E7%BC%96%E7%A0%81/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-05 00:00:00 +0800", "snippet": "独热编码（One-Hot Encoding）是一种常用的标签编码方法，通常用于将标签转换为适合机器学习算法使用的格式。独热编码将每个标签都映射为一个由 0 和 1 组成的向量，向量的长度等于标签数量，对于每个标签，只有对应位置的值为 1，其余位置的值为 0。例如，对于有三个标签 A、B 和 C 的问题，独热编码后的结果如下： 标签 独热编码 A [1, 0, 0] B [0, 1, 0] C [0, 0, 1] 独热编码可以解决标签之间距离和相关性不明显的问题，但也可能会增加数据集的维度和复杂度。因此，在选择使用独热编码时需要权衡其优缺点，根据具体问题和数据集选择最适合的编码方法。下面是一个使用 Python 的 Pandas 库实现独热编码的示例代码：import pandas as pd# 创建一个包含标签的数据集data = pd.DataFrame({&#39;color&#39;: [&#39;red&#39;, &#39;green&#39;, &#39;blue&#39;, &#39;blue&#39;, &#39;red&#39;]})# 对 &#39;color&#39; 列进行独热编码one_hot_encoded = pd.get_dummies(data[&#39;color&#39;])print(one_hot_encoded)输出结果如下： blue green red0 0 0 11 0 1 02 1 0 03 1 0 04 0 0 1在上面的示例代码中，我们首先创建了一个包含标签的数据集，然后使用 Pandas 库中的 get_dummies() 函数对其中的 ‘color’ 列进行独热编码，得到了一个新的数据集 one_hot_encoded，其中每一列对应一个标签，每一行对应一个数据点，值为 1 表示该数据点属于对应的标签，值为 0 表示不属于。独热编码的公式如下：假设有 n 个不同的标签，对于第 i 个标签，其独热编码为一个 n 维的向量，其中第 i 个位置的值为 1，其余位置的值为 0。对于一个包含 m 个数据点的数据集，其中第 j 个数据点属于第 i 个标签，则该数据点的独热编码为：[0, ..., 0, 1, 0, ..., 0] |__i__|其中独热编码向量的长度为 n，第 i 个位置的值为 1，其余位置的值为 0。除了使用 Pandas 库的 get_dummies() 函数进行独热编码外，还可以使用 Python 的 Scikit-Learn 库中的 OneHotEncoder 类进行独热编码。下面是使用 Scikit-Learn 库进行独热编码的示例代码：from sklearn.preprocessing import OneHotEncoderimport numpy as np# 创建一个包含标签的数据集data = np.array([&#39;red&#39;, &#39;green&#39;, &#39;blue&#39;, &#39;blue&#39;, &#39;red&#39;]).reshape(-1, 1)# 创建 OneHotEncoder 对象并进行独热编码encoder = OneHotEncoder()one_hot_encoded = encoder.fit_transform(data).toarray()print(one_hot_encoded)输出结果如下：[[1. 0. 0.] [0. 1. 0.] [0. 0. 1.] [0. 0. 1.] [1. 0. 0.]]在上面的示例代码中，我们首先创建了一个包含标签的数据集，然后创建了一个 OneHotEncoder 对象，并使用其 fit_transform() 方法对数据集进行独热编码，得到一个新的数据集 one_hot_encoded，其中每一列对应一个标签，每一行对应一个数据点，值为 1 表示该数据点属于对应的标签，值为 0 表示不属于。需要注意的是，在使用 OneHotEncoder 进行独热编码时，需要将输入数据的类型转换为整数类型或者字符串类型，否则会报错。另外，如果输入数据集的维度为一维，则需要使用 reshape() 函数将其转换为二维。除了 Pandas 和 Scikit-Learn 库外，还有许多其他的 Python 库可以用于独热编码，如 TensorFlow 和 PyTorch 等深度学习框架。这些框架中通常会提供专门的函数或者类来实现独热编码，并支持 GPU 加速等优化操作，可以大大提高编码效率和计算速度。" }, { "title": "机器学习基础-监督学习-标签编码之标签映射（Label Encoding）", "url": "/posts/%E6%A0%87%E7%AD%BE-16-%E6%A0%87%E7%AD%BE%E7%BC%96%E7%A0%81%E4%B9%8B%E6%A0%87%E7%AD%BE%E6%98%A0%E5%B0%84/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-05 00:00:00 +0800", "snippet": "标签映射（Label Encoding）是一种常见的标签编码方法，将每个标签都映射为一个整数，常用于分类问题。在标签数量较少的情况下，标签映射可以简单有效地将标签转换为数字表示。下面是一个使用 scikit-learn 库实现标签映射的示例代码：from sklearn.preprocessing import LabelEncoder# 创建标签编码器label_encoder = LabelEncoder()# 假设有一个包含标签的列表labels = [&#39;red&#39;, &#39;green&#39;, &#39;blue&#39;, &#39;green&#39;, &#39;red&#39;, &#39;blue&#39;, &#39;blue&#39;]# 对标签进行编码encoded_labels = label_encoder.fit_transform(labels)# 输出编码后的标签print(encoded_labels)运行上述代码将得到以下输出：[2 1 0 1 2 0 0]在上面的示例中，我们首先创建了一个标签编码器 LabelEncoder，然后将包含标签的列表 labels 传递给 fit_transform 方法进行编码。编码后的结果存储在 encoded_labels 中，并且每个标签都被映射为一个整数。在这个例子中，红色被映射为 2，绿色被映射为 1，蓝色被映射为 0。标签映射的主要缺点是它并不能捕捉到不同标签之间的相关性和距离，因为它只是简单地将标签转换为整数。在标签之间有明显的相关性和距离的情况下，独热编码可能更加适合。标签映射通常应用于标签具有有序性质的情况，例如星级评分、温度等级等。在这些情况下，标签的顺序是有意义的，并且可以通过整数编码来表示标签之间的相对大小。标签映射的一个应用是将文本数据转换为数字表示，例如将电影评论转换为情感分数（例如，正面评论为 1，负面评论为 0）。在这种情况下，标签映射可以通过将每个单词映射为一个数字来将文本转换为数字表示。下面是一个使用 pandas 库和 LabelEncoder 实现标签映射的示例代码：import pandas as pdfrom sklearn.preprocessing import LabelEncoder# 创建一个包含标签的DataFramedf = pd.DataFrame({ &#39;color&#39;: [&#39;red&#39;, &#39;green&#39;, &#39;blue&#39;, &#39;green&#39;, &#39;red&#39;, &#39;blue&#39;, &#39;blue&#39;], &#39;size&#39;: [&#39;small&#39;, &#39;medium&#39;, &#39;large&#39;, &#39;medium&#39;, &#39;small&#39;, &#39;medium&#39;, &#39;large&#39;], &#39;class&#39;: [&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;B&#39;, &#39;A&#39;, &#39;C&#39;, &#39;C&#39;]})# 对标签进行编码label_encoder = LabelEncoder()df[&#39;class_encoded&#39;] = label_encoder.fit_transform(df[&#39;class&#39;])# 输出编码后的DataFrameprint(df)运行上述代码将得到以下输出： color size class class_encoded0 red small A 01 green medium B 12 blue large C 23 green medium B 14 red small A 05 blue medium C 26 blue large C 2在上面的示例中，我们首先创建了一个包含标签的 DataFrame，并将 class 列传递给 LabelEncoder 进行编码。编码后的结果存储在 class_encoded 列中，并且每个标签都被映射为一个整数。需要注意的是，使用标签映射进行编码时，要确保标签之间的顺序是有意义的，并且每个标签都被映射为唯一的整数。如果标签之间没有顺序性，或者多个标签被映射为相同的整数，则可能会导致模型学习错误的关系。在这种情况下，独热编码可能更加适合。" }, { "title": "机器学习基础-监督学习-标签编码", "url": "/posts/%E6%A0%87%E7%AD%BE-15-%E6%A0%87%E7%AD%BE%E7%BC%96%E7%A0%81/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-05 00:00:00 +0800", "snippet": "在有监督学习中，标签编码是将标签映射为计算机可处理的格式的过程。通常，在分类问题中，我们需要将每个类别的标签映射为一个独热向量。这样做的好处是可以使得模型更容易学习到类别之间的关系，同时也可以避免类别之间的大小关系对模型的训练产生影响。下面介绍一些常见的标签编码方法： 独热编码（One-Hot Encoding）：将每个类别的标签映射为一个与类别数量相等的向量，其中只有一个元素为 1，其余元素为 0。例如，对于一个有 3 个类别的分类问题，其标签可以被编码为[1, 0, 0]、[0, 1, 0]或[0, 0, 1]等。 标签编码（Label Encoding）：将每个类别的标签映射为一个整数。例如，对于一个有 3 个类别的分类问题，其标签可以被编码为 0、1 或 2 等。在实际应用中，通常会使用独热编码来进行标签编码，因为它能够更好地保留类别之间的关系。下面给出使用 Python 的 sklearn 库进行独热编码的示例代码：from sklearn.preprocessing import OneHotEncoder# 假设有一个标签序列为[0, 1, 2, 1, 0, 2]labels = [0, 1, 2, 1, 0, 2]# 创建一个独热编码器encoder = OneHotEncoder()# 对标签进行独热编码one_hot_labels = encoder.fit_transform(np.array(labels).reshape(-1, 1))# 将独热编码后的结果转换为数组one_hot_labels = one_hot_labels.toarray()# 输出独热编码后的结果print(one_hot_labels)上述代码中，我们首先创建了一个独热编码器，然后使用 fit_transform 方法将标签进行独热编码。最后，将独热编码后的结果转换为数组并输出。除了使用 sklearn 库提供的独热编码方法，我们也可以手动实现独热编码。下面给出一个使用 numpy 库实现独热编码的示例代码：import numpy as np# 假设有一个标签序列为[0, 1, 2, 1, 0, 2]labels = [0, 1, 2, 1, 0, 2]# 获取标签数量num_classes = len(set(labels))# 创建一个全零矩阵，行数为标签数量，列数为标签序列长度one_hot_labels = np.zeros((num_classes, len(labels)))# 对矩阵进行赋值操作for i, label in enumerate(labels): one_hot_labels[label, i] = 1# 输出独热编码后的结果print(one_hot_labels)上述代码中，我们首先获取标签数量，然后根据标签数量和标签序列长度创建一个全零矩阵。接着，使用循环遍历标签序列，将矩阵中对应的位置赋值为 1。最后，输出独热编码后的结果。需要注意的是，使用 numpy 库实现独热编码的效率通常比使用 sklearn 库提供的独热编码方法更高。但是，手动实现独热编码需要写更多的代码，并且容易出错。因此，在实际应用中，可以根据具体情况选择使用哪种方法进行标签编码。除了独热编码外，还有一种常用的标签编码方法是标签映射（Label Encoding）。标签映射是将每个标签都映射为一个整数，从 0 开始，依次递增。例如，对于标签序列[‘cat’, ‘dog’, ‘bird’, ‘bird’]，使用标签映射后可以得到序列[0, 1, 2, 2]。标签映射通常适用于标签之间存在顺序关系的情况，例如温度高低、成绩好坏等。在这种情况下，标签之间存在一定的大小关系，因此可以将标签转换为整数，便于后续的处理和计算。下面给出一个使用 sklearn 库实现标签映射的示例代码：from sklearn.preprocessing import LabelEncoder# 假设有一个标签序列为[&#39;cat&#39;, &#39;dog&#39;, &#39;bird&#39;, &#39;bird&#39;]labels = [&#39;cat&#39;, &#39;dog&#39;, &#39;bird&#39;, &#39;bird&#39;]# 创建标签编码器le = LabelEncoder()# 对标签序列进行编码label_encoded = le.fit_transform(labels)# 输出标签编码后的结果print(label_encoded)上述代码中，我们首先创建了一个标签编码器（LabelEncoder），然后使用 fit_transform()方法对标签序列进行编码。最后，输出标签编码后的结果。需要注意的是，标签映射只适用于标签之间存在顺序关系的情况，对于没有顺序关系的标签，使用标签映射可能会引入误解。因此，在选择标签编码方法时，需要根据具体情况进行选择。除了独热编码和标签映射外，还有一些其他的标签编码方法。例如，如果标签数量比较少，可以使用二进制编码（Binary Encoding）对标签进行编码。二进制编码将每个标签都映射为一个二进制数，其中每一位代表一个可能的取值，例如对于标签序列[‘cat’, ‘dog’, ‘bird’, ‘bird’]，使用二进制编码后可以得到序列[0b00, 0b01, 0b10, 0b10]，其中 0b00 表示 cat，0b01 表示 dog，0b10 表示 bird。另外，如果标签之间不存在顺序关系，可以使用频率编码（Frequency Encoding）或哈希编码（Hash Encoding）对标签进行编码。频率编码将每个标签都映射为其出现频率，例如对于标签序列[‘cat’, ‘dog’, ‘bird’, ‘bird’]，使用频率编码后可以得到序列[0.5, 0.25, 0.25, 0.25]，其中 cat 出现了一次，dog 出现了一次，bird 出现了两次，因此其频率分别为 0.5、0.25 和 0.25。哈希编码则将每个标签都映射为一个固定长度的二进制数，例如 32 位或 64 位，其中相同的标签映射为相同的二进制数，不同的标签映射为不同的二进制数。需要注意的是，不同的标签编码方法适用于不同的问题和数据集。在选择标签编码方法时，需要根据具体情况进行选择，并且需要对不同的方法进行比较和评估，以确定哪种方法最适合当前的问题和数据集。" }, { "title": "机器学习基础-监督学习-标签平衡处理之 ROSE (Random Over-Sampling Examples)", "url": "/posts/%E6%A0%87%E7%AD%BE-14-%E6%A0%87%E7%AD%BE%E5%B9%B3%E8%A1%A1%E5%A4%84%E7%90%86%E4%B9%8BROSE/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-05 00:00:00 +0800", "snippet": "ROSE (Random Over-Sampling Examples) 是一种基于样本生成的过采样方法，它通过对少数类样本进行样本生成，以增加数据集中的少数类样本数量。具体来说，ROSE 通过从少数类样本中随机选择一个样本，然后通过一定的方式生成新的样本来进行过采样。ROSE 方法可以根据不同的生成方式分为以下两类： Informed Over-Sampling：这种方式需要指定少数类样本的生成方式。例如，可以通过特定的模型对少数类样本进行拟合，然后生成新的少数类样本。Informed Over-Sampling 可以更好地保留少数类样本的信息。 Uniform Over-Sampling：这种方式是最常用的 ROSE 方式。它通过对少数类样本进行简单的随机过采样，然后对过采样后的样本进行随机变换来生成新的样本。Uniform Over-Sampling 可以更快地生成新样本。 下面给出一个简单的 ROSE 算法的伪代码：Input: 少数类样本集合 D_min, 过采样倍数 kOutput: 过采样后的数据集合 D_overD_over = D_minfor i in 1 to k-1: randomly choose a sample d from D_min randomly generate a new sample d_new based on d add d_new to D_overend forreturn D_over其中，随机生成新样本的方式可以使用简单的线性插值，即对于样本 $d$ 和 $d’$，生成 $k-1$ 个新样本，第 $j$ 个新样本为：\\[d_{new_j} = \\frac{k-j}{k-1} \\times d + \\frac{j-1}{k-1} \\times d^\\prime\\]下面给出 Python 代码示例，使用 scikit-learn 的 ROSE 实现进行过采样：from imblearn.over_sampling import RandomOverSamplerros = RandomOverSampler(random_state=0)X_resampled, y_resampled = ros.fit_resample(X, y)其中，X 和 y 分别为输入特征和标签。RandomOverSampler 类可以设置采样倍数，生成样本方式等参数，以满足不同的需求。Informed Over-SamplingInformed Over-Sampling 是一种基于样本生成的过采样方法，需要指定少数类样本的生成方式。它通过对少数类样本进行拟合，然后生成新的样本来进行过采样。具体来说，Informed Over-Sampling 可以使用以下方式生成新样本：在少数类样本中随机选择一个样本 $x_i$；使用 K 近邻算法或其他分类器对 $x_i$ 进行分类，并找到最近的 $k$ 个样本，这些样本属于少数类或多数类都可以；根据这 $k$ 个样本的特征和标签信息，生成新的样本。生成新样本的方式有很多种，常见的方式包括插值法和 SMOTE 算法。插值法的基本思想是将少数类样本看作曲线上的点，然后在曲线上进行插值得到新的点。SMOTE 算法则是使用合成样本来增加少数类样本数量。下面给出一个简单的 Informed Over-Sampling 算法的伪代码：Input: 少数类样本集合 D_min, 过采样倍数 kOutput: 过采样后的数据集合 D_overD_over = D_minfor i in 1 to k-1: randomly choose a sample d from D_min find k nearest neighbors of d generate a new sample d_new based on d and its k neighbors add d_new to D_overend forreturn D_over下面给出 Python 代码示例，使用 scikit-learn 的 KNeighborsClassifier 和 SMOTE 实现 Informed Over-Sampling 过采样：from imblearn.over_sampling import SMOTEfrom sklearn.neighbors import KNeighborsClassifierknn = KNeighborsClassifier(n_neighbors=k)smote = SMOTE(sampling_strategy=&#39;auto&#39;, k_neighbors=k, random_state=0)X_resampled, y_resampled = smote.fit_resample(X, y)其中，knn 用于查找最近的 $k$ 个样本，SMOTE 用于根据最近的 $k$ 个样本生成新样本。SMOTE 的 sampling_strategy 参数用于指定过采样后的样本类别分布，k_neighbors 参数用于指定最近邻的数量。Uniform Over-SamplingUniform Over-Sampling 是 ROSE 中常用的一种过采样方法，它通过对少数类样本进行随机过采样，并对过采样后的样本进行随机变换来生成新的样本。Uniform Over-Sampling 的生成方式相对简单，但也需要注意一些问题，例如过采样倍数、采样时的数据分布等。下面给出 Uniform Over-Sampling 的伪代码：Input: 少数类样本集合 D_min, 过采样倍数 kOutput: 过采样后的数据集合 D_overD_over = D_minwhile len(D_over) &amp;lt; k * len(D_min): randomly choose a sample d from D_min randomly generate a new sample d_new based on d add d_new to D_overend whilereturn D_over下面给出 Python 代码示例，使用 scikit-learn 的 RandomOverSampler 类进行 Uniform Over-Sampling：from imblearn.over_sampling import RandomOverSamplerros = RandomOverSampler(sampling_strategy=&#39;minority&#39;, random_state=0)X_resampled, y_resampled = ros.fit_resample(X, y)其中，sampling_strategy 参数设置为 ‘minority’ 表示只对少数类样本进行过采样。RandomOverSampler 类的其他参数可以根据具体情况进行调整。" }, { "title": "机器学习基础-监督学习-标签平衡处理之 Safe-Level-SMOTE", "url": "/posts/%E6%A0%87%E7%AD%BE-13-%E6%A0%87%E7%AD%BE%E5%B9%B3%E8%A1%A1%E5%A4%84%E7%90%86%E4%B9%8BSafe-Level-SMOTE/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-05 00:00:00 +0800", "snippet": "Safe-Level-SMOTE 是一种基于 SMOTE 的过采样方法，它不仅考虑了少数类样本之间的距离，还考虑了多数类样本之间的距离，以避免产生噪声样本。该方法可以被视为一种二次采样方法，其主要思想是对样本进行二次过滤，以保证生成的合成样本是有效的。下面是 Safe-Level-SMOTE 的详细步骤： 对少数类样本计算每个样本与其 K 近邻之间的距离，得到距离矩阵。 对距离矩阵进行标准化，以确保不同的距离指标具有相同的权重。 对多数类样本计算每个样本与其 K 近邻之间的距离，得到距离矩阵。 对距离矩阵进行标准化。 对每个少数类样本进行下面的操作： 对当前样本计算其 Safe-Level（安全级别），Safe-Level 定义为少数类样本与多数类样本的距离比值，即 Safe-Level = (Distance to minority) / (Distance to majority)。 根据 Safe-Level，从当前样本的 K 近邻中选择一个 Safe-Level 最小的样本，记为 nn。 根据 Safe-Level，计算当前样本需要生成的样本数量，即 N = (Safe-Level - 1) × α。 对当前样本和 nn 进行线性插值，生成 N 个合成样本。 将生成的合成样本添加到少数类样本集合中。 其中，α 是一个参数，控制合成样本的数量。下面是 Safe-Level-SMOTE 的伪代码：Input: Minority class samples X, majority class samples Y, K, αOutput: Synthetic samples Sfor each minority class sample x in X do Calculate the K nearest neighbors of x in X Calculate the K nearest neighbors of x in Y Calculate the Safe-Level (SL) for x Choose a neighbor y from the K nearest neighbors in Y with the lowest SL Calculate the number of synthetic samples N to generate for i = 1 to N do Generate a synthetic sample s by linear interpolation between x and y Add s to the set of synthetic samples S end forend forSafe-Level-SMOTE 是一种有效的过采样方法，可以提高分类器的性能。同时，需要注意合成样本的数量不宜过多，以避免过拟合问题。下面是 Safe-Level-SMOTE 的 Python 代码实现：from sklearn.neighbors import NearestNeighborsimport numpy as npdef safe_level_smote(X, y, K=5, alpha=0.5): &quot;&quot;&quot; Safe-Level-SMOTE implementation. :param X: Minority class samples. :param y: Corresponding class labels. :param K: Number of nearest neighbors. :param alpha: Oversampling rate. :return: Synthetic samples. &quot;&quot;&quot; # Find the K nearest neighbors of each sample in the minority class knn = NearestNeighbors(n_neighbors=K+1).fit(X) _, indices = knn.kneighbors(X) S = [] for i in range(len(X)): x = X[i] # Find the K nearest neighbors of x in the minority class knn_x = X[indices[i][1:]] # Find the K nearest neighbors of x in the majority class knn_y = X[y != y[i]][indices[i][1:]] # Compute the Safe-Level for x dist_x = np.linalg.norm(knn_x - x, axis=1) dist_y = np.linalg.norm(knn_y - x, axis=1) sl = dist_x / dist_y # Find the neighbor with the lowest Safe-Level nn = knn_y[np.argmin(sl)] # Calculate the number of synthetic samples to generate N = int(alpha * (sl[np.argmin(sl)] - 1)) # Generate synthetic samples by linear interpolation for j in range(N): # Compute the linear interpolation coefficient beta = np.random.uniform(0, 1, size=len(x)) s = x + beta * (nn - x) S.append(s) return np.array(S)其中，X 是少数类样本集合，y 是对应的类标签。K 是邻居数量，alpha 是过采样率。函数首先计算每个少数类样本的 Safe-Level，然后从多数类样本中选择 Safe-Level 最小的样本作为合成样本的参考点，根据 Safe-Level 计算合成样本的数量，并使用线性插值生成合成样本。最终，函数返回生成的合成样本。在使用 Safe-Level-SMOTE 进行数据过采样时，需要对 Safe-Level 进行计算。Safe-Level 反映了少数类样本与多数类样本之间的距离关系，计算方法如下：\\[SL(x_i) = \\frac{\\min_{x_j \\in X_{maj}} ||x_i-x_j||}{\\max_{x_j \\in X_{maj}} ||x_i-x_j||}\\] 其中，$SL(x_i)$ 表示样本 $x_i$ 的 Safe-Level，$X_{maj}$ 表示多数类样本集合，$\\left \\cdot \\right $ 表示欧几里得距离。 Safe-Level 的计算是通过计算少数类样本与多数类样本之间的距离来进行的。具体而言，对于每个少数类样本 $x_i$，找到其在多数类样本集合 $X_{maj}$ 中距离最近的样本 $x_j$，然后将 $x_i$ 与 $x_j$ 之间的距离作为 Safe-Level 的分子，将 $x_i$ 与 $X_{maj}$ 中所有样本之间的距离的最大值作为 Safe-Level 的分母。Safe-Level 的计算结果在 [0, 1] 的范围内，值越小表示少数类样本越接近多数类样本。在 Safe-Level-SMOTE 的实现中，对于每个少数类样本，选择 Safe-Level 最小的多数类样本作为合成样本的参考点。选择 Safe-Level 最小的多数类样本是因为这样可以尽可能地增加样本之间的差异性，从而提高合成样本的多样性。同时，Safe-Level 还被用来计算合成样本的数量，Safe-Level 越小的少数类样本需要生成更多的合成样本。" }, { "title": "机器学习基础-监督学习-标签平衡处理之 Borderline-SMOTE", "url": "/posts/%E6%A0%87%E7%AD%BE-12-%E6%A0%87%E7%AD%BE%E5%B9%B3%E8%A1%A1%E5%A4%84%E7%90%86%E4%B9%8BBorderline-SMOTE/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-05 00:00:00 +0800", "snippet": "Borderline-SMOTE 是基于 SMOTE 的一种改进版本，它只对那些靠近多数类的少数类样本进行过采样，从而避免了对所有少数类样本进行过采样可能导致的过拟合问题。下面详细介绍 Borderline-SMOTE 的算法流程及公式。算法流程： 找到少数类样本中的边界样本（即与多数类样本之间的距离小于某个阈值），将这些样本标记为 Borderline-SMOTE 样本。 对于每个 Borderline-SMOTE 样本，计算其 K 个最近邻样本的平均值。 针对每个 Borderline-SMOTE 样本，选择其中一个最近邻样本，并随机生成一些新样本，生成规则为：4. 新样本 = Borderline-SMOTE 样本 + （Borderline-SMOTE 样本 - 最近邻样本）* 随机数。将生成的新样本添加到少数类样本中。公式：假设某个 Borderline-SMOTE 样本为 x，它的 K 个最近邻样本为 N(x)={x1, x2, …, xk}，新样本生成的公式为：\\[x_{new} = x + \\lambda (x - x_{nn})\\]其中，$\\lambda$ 是随机数，$x_{nn}$ 是从 N(x) 中选择的最近邻样本。需要注意的是，在实现 Borderline-SMOTE 时，需要确定阈值、K 值和生成样本的数量等参数，这些参数的选取需要结合具体问题和数据集进行调整。下面给出一个 Python 实现的例子：from collections import Counterfrom sklearn.neighbors import NearestNeighborsimport numpy as npdef borderline_SMOTE(X, y, k_neighbors=5, m_neighbors=10, thresh=0.5, N=100): &quot;&quot;&quot; Borderline-SMOTE 算法实现。 参数： - X: 原始特征矩阵。 - y: 原始标签。 - k_neighbors: K 值，默认为 5。 - m_neighbors: M 值，即生成样本的最大个数，默认为 10。 - thresh: 阈值，默认为 0.5。 - N: 需要生成的新样本数量，默认为 100。 返回： - X_resampled: 过采样后的特征矩阵。 - y_resampled: 过采样后的标签。 &quot;&quot;&quot; # 统计各个类别的数量 counter = Counter(y) max_class_count = max(counter.values()) minority_class = [k for k, v in counter.items() if v &amp;lt; max_class_count][0] # 找到少数类样本中的边界样本 neigh = NearestNeighbors(n_neighbors=k_neighbors) neigh.fit(X) X_min = X[y == minority_class] border = [] for i, x in enumerate(X_min): # 找到 x 的 K 个最近邻样本 knn = neigh.kneighbors([x], n_neighbors=k_neighbors+1, return_distance=False)[0] knn = knn[1:] # 将 x 本身排除掉 # 计算 x 与其 K 个最近邻样本之间的距离 dist = np.linalg.norm(X[knn] - x, axis=1) # 判断 x 是否为边界样本 if np.mean(dist) &amp;lt; thresh: border.append(i) # 针对边界样本生成新样本 X_new = [] y_new = [] for i in border: x = X_min[i] # 找到 x 的 M 个最近邻样本 knn = neigh.kneighbors([x], n_neighbors=m_neighbors+1, return_distance=False)[0] knn = knn[1:] # 将 x 本身排除掉 # 计算 x 与其 M 个最近邻样本之间的距离 dist = np.linalg.norm(X[knn] - x, axis=1) # 找到 x 的最近邻样本 nn = knn[np.argmin(dist)] for j in range(N): # 生成新样本 lambda_ = np.random.rand() x_new = x + lambda_ * (x - X[nn]) X_new.append(x_new) y_new.append(minority_class) # 将生成的新样本添加到原始数据中 X_resampled = np.vstack([X, np.array(X_new)]) y_resampled = np.hstack([y, np.array(y_new)]) return X_resampled, y_resampled以上是 Borderline-SMOTE 的一个简单实现，使用时只需要传入原始特征矩阵和标签，以及一些参数（如 K 值、M 值、阈值等）即可。需要注意的是，Borderline-SMOTE 算法是基于 SMOTE 算法的改进版，因此它也具有一些局限性和不足之处。比如：只适用于二分类问题。Borderline-SMOTE 算法只能用于二分类问题中的少数类样本增强，对于多分类问题中的类别不平衡问题，需要采用其他方法；对参数敏感。Borderline-SMOTE 算法的效果受到一些关键参数（如 K 值、M 值、阈值等）的影响，需要进行调参才能获得最佳的增强效果；不适用于噪声数据。当数据中存在大量噪声数据时，Borderline-SMOTE 算法可能会增加噪声数据的数量，从而影响分类器的性能。因此，在实际使用中，需要根据具体情况选择合适的方法来处理数据不平衡问题，以获得更好的分类效果。需要注意的是，Borderline-SMOTE 算法是基于 SMOTE 算法的改进版，因此它也具有一些局限性和不足之处。比如： 只适用于二分类问题。Borderline-SMOTE 算法只能用于二分类问题中的少数类样本增强，对于多分类问题中的类别不平衡问题，需要采用其他方法； 对参数敏感。Borderline-SMOTE 算法的效果受到一些关键参数（如 K 值、M 值、阈值等）的影响，需要进行调参才能获得最佳的增强效果； 不适用于噪声数据。当数据中存在大量噪声数据时，Borderline-SMOTE 算法可能会增加噪声数据的数量，从而影响分类器的性能。因此，在实际使用中，需要根据具体情况选择合适的方法来处理数据不平衡问题，以获得更好的分类效果。" }, { "title": "机器学习基础-监督学习-标签平衡处理之 ADASYN（Adaptive Synthetic Sampling）", "url": "/posts/%E6%A0%87%E7%AD%BE-11-%E6%A0%87%E7%AD%BE%E5%B9%B3%E8%A1%A1%E5%A4%84%E7%90%86%E4%B9%8BADASYN/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-05 00:00:00 +0800", "snippet": "ADASYN（Adaptive Synthetic Sampling）是一种基于 SMOTE 的过采样方法，它通过计算每个少数类样本周围的多数类样本比例，决定需要产生的新样本数量。ADASYN 可以更好地适应不同的数据集，因为它会为那些与多数类更接近的少数类样本产生更多的新样本，而为那些与多数类更远的少数类样本产生较少的新样本。这种方法可以帮助避免产生噪声样本，并提高分类器的性能。下面是 ADASYN 方法的详细流程： 对于每个少数类样本，计算它周围多数类样本的比例 p； 对于每个少数类样本，计算需要产生的新样本数量 n = p * k，其中 k 是一个可调参数，用于控制新样本的数量； 对于每个少数类样本，从它的 k 个最近邻的多数类样本中随机选择 n 个样本，并将它们插入到少数类样本和其 k 个最近邻之间。下面是 ADASYN 的 Python 代码实现：import numpy as npfrom sklearn.neighbors import NearestNeighborsdef ADASYN(X, y, k=5, ratio=0.5): &quot;&quot;&quot; ADASYN: Adaptive Synthetic Sampling &quot;&quot;&quot; # 计算每个少数类样本周围多数类样本的数量 neigh = NearestNeighbors(n_neighbors=k+1) neigh.fit(X[y == 1]) distances, indices = neigh.kneighbors() n_samples, n_features = X.shape synthetic_X = [] synthetic_y = [] for i in range(len(indices)): # 计算需要产生的新样本数量 n = int(round(ratio * indices[i].shape[0])) if n == 0: continue # 对于每个少数类样本，随机选择 n 个样本，并插入到样本之间 for j in range(n): nn = np.random.randint(1, indices[i].shape[0]) dif = X[indices[i][nn]] - X[indices[i][0]] gap = np.random.random() synthetic = X[indices[i][0]] + gap * dif synthetic_X.append(synthetic) synthetic_y.append(1) synthetic_X = np.array(synthetic_X) synthetic_y = np.array(synthetic_y) # 将生成的新样本和原始样本合并成新的训练集 X = np.vstack((X, synthetic_X)) y = np.hstack((y, synthetic_y)) return X, y上面的代码中，输入 X 是训练数据的特征矩阵，y 是训练数据的标签，k 是指定的 k 值，ratio 是新样本占比的参数。函数首先计算每个少数类样本周围多数类样本的数量，然后随机从多数类样本中选择一定数量的样本，插入到少数类样本和其 k 个最近邻之间，生成新的合成样本。最后，将生成的新样本和原始样本合并成新的训练集返回。需要注意的是，ADASYN 方法在生成新样本时可能会产生重叠的样本，因此需要在生成后去除重复的样本。下面是一个简单的示例：from collections import Counterfrom sklearn.datasets import make_classificationfrom sklearn.model_selection import train_test_splitfrom sklearn.linear_model import LogisticRegressionfrom sklearn.metrics import classification_report# 生成一个二分类样本不平衡的数据集X, y = make_classification(n_classes=2, class_sep=2, weights=[0.1, 0.9], n_informative=3, n_redundant=1, flip_y=0, n_features=20, n_clusters_per_class=1, n_samples=1000, random_state=10)# 原始数据集中少数类样本的数量print(&#39;Original dataset shape %s&#39; % Counter(y))# 划分训练集和测试集X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=10)# 使用 ADASYN 过采样方法平衡训练集X_resampled, y_resampled = ADASYN(X_train, y_train)# 平衡后的数据集中少数类样本的数量print(&#39;Resampled dataset shape %s&#39; % Counter(y_resampled))# 训练模型clf = LogisticRegression(random_state=10)clf.fit(X_resampled, y_resampled)# 在测试集上进行预测并评估性能y_pred = clf.predict(X_test)print(classification_report(y_test, y_pred))在这个示例中，我们使用了 make_classification() 函数生成一个二分类样本不平衡的数据集，并划分出训练集和测试集。然后，我们使用 ADASYN 过采样方法平衡训练集，并训练逻辑回归模型进行分类。最后，我们在测试集上进行预测并评估模型性能。上面的代码中使用的 ADASYN() 函数并不是 sklearn 中提供的函数，这里我们自己实现一下 ADASYN 过采样算法。import numpy as npfrom sklearn.neighbors import NearestNeighborsdef ADASYN(X, y, k=5, ratio=1): &quot;&quot;&quot; ADASYN 过采样算法实现 参数： X: 原始特征矩阵 y: 原始标签向量 k: 选择 k 个最近邻样本 ratio: 生成新样本与原样本比例 返回值： new_X: 合成新的特征矩阵 new_y: 合成新的标签向量 &quot;&quot;&quot; # 统计少数类样本的数量和多数类样本的数量 minority_num = sum(y == 1) majority_num = sum(y == 0) # 计算需要生成的新样本数量 new_sample_num = int((minority_num * ratio) - minority_num) # 如果新样本数量为 0，则返回原始数据集 if new_sample_num == 0: return X, y # 计算每个少数类样本需要生成的新样本数量 num_neighbors = np.zeros(minority_num) for i in range(minority_num): # 计算少数类样本 i 的 k 个最近邻样本 nn = NearestNeighbors(n_neighbors=k+1) nn.fit(X[y == 1]) distances, indices = nn.kneighbors(X[y == 1][i].reshape(1, -1)) # 计算少数类样本 i 和其 k 个最近邻样本中属于多数类的样本数量 num_neighbors[i] = sum(y[y == 1][indices[0, 1:]] == 0) # 计算少数类样本 i 生成新样本的比例，即生成 num_synthetic 样本 # 需要从其 k 个最近邻样本中选择 num_synthetic * (num_neighbors[i] / sum(num_neighbors)) 个样本 synthetic_ratios = num_neighbors / sum(num_neighbors) num_synthetic = np.round(synthetic_ratios * new_sample_num).astype(int) # 对每个少数类样本 i，根据其 k 个最近邻样本生成 num_synthetic 个新样本 new_X = [] new_y = [] for i in range(minority_num): nn = NearestNeighbors(n_neighbors=k+1) nn.fit(X[y == 1]) distances, indices = nn.kneighbors(X[y == 1][i].reshape(1, -1)) # 生成 num_synthetic[i] 个新样本 for j in range(num_synthetic[i]): # 随机选择一个少数类样本的 k 个最近邻样本 nn_index = np.random.randint(1, k+1) # 计算合成样本的特征值 dif = X[y == 1][indices[0, nn_index]] - X[y == 1][i] gap = np.random.rand(1, X.shape[1]) new_X.append(X[y == 1][i] + gap * dif) new_y.append(1) # 将新生成的新样本和原始数据集合并，返回合成的新特征矩阵和新标签向量 new_X = np.vstack([X, np.array(new_X).reshape(-1, X.shape[1])]) new_y = np.hstack([y, np.array(new_y)]) return new_X, new_y以上是 Python 代码实现 ADASYN 过采样算法的过程。通过统计少数类样本的数量和多数类样本的数量，计算需要生成的新样本数量，并根据每个少数类样本的 k 个最近邻样本生成新样本，最后将新生成的样本和原始数据集合并，即可得到合成的新特征矩阵和新标签向量。" }, { "title": "机器学习基础-监督学习-标签平衡处理之 SMOTE（Synthetic Minority Over-sampling Technique）", "url": "/posts/%E6%A0%87%E7%AD%BE-10-%E6%A0%87%E7%AD%BE%E5%B9%B3%E8%A1%A1%E5%A4%84%E7%90%86%E4%B9%8BSMOTE/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-05 00:00:00 +0800", "snippet": "SMOTE（Synthetic Minority Over-sampling Technique）是一种基于 K 近邻的过采样方法，它可以根据少数类样本之间的距离，产生一些新的少数类样本，从而解决数据集不平衡的问题。SMOTE 方法可以有效地避免过拟合问题，同时不会改变数据集中的总体样本数。SMOTE 方法的主要思想是对于每个少数类样本 x，从它的 K 个最近邻中随机选择一个样本 x’，并根据以下公式产生新的合成样本：\\[x_{new} = x + \\lambda(x^\\prime-x)\\]其中，$\\lambda$ 是一个 [0,1] 之间的随机数，用于控制新生成的样本在原有样本和其最近邻之间的位置。具体地，SMOTE 方法的实现过程如下： 对于每个少数类样本 $x_i$，计算它的 $k$ 个最近邻样本。 对于每个最近邻样本 $x_{i,nn}$，根据上述公式产生一个新的合成样本 $x_{new}$。 将所有合成样本添加到原始数据集中。 下面是一个使用 Python 实现 SMOTE 方法的示例代码：from sklearn.neighbors import NearestNeighborsimport numpy as npdef SMOTE(X, y, k=5, ratio=1.0): &quot;&quot;&quot; :param X: 原始数据集 :param y: 标签 :param k: KNN 参数 :param ratio: 需要过采样的数量与少数类样本数量之间的比例 :return: 过采样后的数据集和标签 &quot;&quot;&quot; # 找出所有少数类样本的索引 minority_class = np.where(y == 1)[0] majority_class = np.where(y == 0)[0] # 计算需要生成的新样本数量 n_minority_samples = len(minority_class) n_synthetic_samples = int(ratio * n_minority_samples) # 计算每个少数类样本的 k 个最近邻样本 knn = NearestNeighbors(n_neighbors=k, n_jobs=-1) knn.fit(X[minority_class]) neighbors = knn.kneighbors(return_distance=False) # 生成新样本 synthetic_samples = np.zeros((n_synthetic_samples, X.shape[1])) for i, j in enumerate(np.random.choice(len(minority_class), n_synthetic_samples)): nn = np.random.choice(neighbors[j]) dif = X[minority_class[nn]] - X[minority_class[j]] gap = np.random.rand() * dif synthetic_samples[i, :] = X[minority_class[j], :] + gap # 将合成样本添加到原始数据集中 X_resampled = np.vstack((X, synthetic_samples)) y_resampled = np.hstack((y, np.ones(n_synthetic_samples))) return X_resampled, y_resampled需要注意的是，SMOTE 方法也有一些局限性。例如，当少数类样本分布极其不均匀时，SMOTE 方法可能会产生一些不太合理的合成样本。此外，当 k 值较小时，SMOTE 方法可能会产生一些与少数类样本过于相似的合成样本，从而导致过拟合问题。为了解决这些问题，一些改进的 SMOTE 方法已经被提出，例如 Borderline-SMOTE 和 ADASYN 等。下面是一个使用 Python 实现 Borderline-SMOTE 方法的示例代码：from sklearn.neighbors import NearestNeighborsimport numpy as npdef BorderlineSMOTE(X, y, k=5, ratio=1.0, kind=&#39;borderline1&#39;): &quot;&quot;&quot; :param X: 原始数据集 :param y: 标签 :param k: KNN 参数 :param ratio: 需要过采样的数量与少数类样本数量之间的比例 :param kind: Borderline-SMOTE 类型，可选值为 &#39;borderline1&#39; 或 &#39;borderline2&#39; :return: 过采样后的数据集和标签 &quot;&quot;&quot; # 找出所有少数类样本的索引 minority_class = np.where(y == 1)[0] majority_class = np.where(y == 0)[0] # 计算需要生成的新样本数量 n_minority_samples = len(minority_class) n_synthetic_samples = int(ratio * n_minority_samples) # 计算每个少数类样本的 k 个最近邻样本 knn = NearestNeighbors(n_neighbors=k, n_jobs=-1) knn.fit(X[minority_class]) neighbors = knn.kneighbors(return_distance=False) # 计算每个少数类样本的半径 distances, indices = knn.kneighbors(X[minority_class]) radii = distances.sum(axis=1) / k # 生成新样本 synthetic_samples = np.zeros((n_synthetic_samples, X.shape[1])) for i, j in enumerate(np.random.choice(len(minority_class), n_synthetic_samples)): nn = np.random.choice(neighbors[j]) dif = X[minority_class[nn]] - X[minority_class[j]] gap = np.random.rand() * dif synthetic_samples[i, :] = X[minority_class[j], :] + gap # 如果生成的样本距离其 k 个最近邻样本的平均距离大于样本半径，则将其标记为噪声样本 if kind == &#39;borderline1&#39; and (distances[j, :] &amp;gt; radii[j]).sum() &amp;gt; 0: synthetic_samples[i, :] = X[minority_class[j], :] # 如果生成的样本距离其 k 个最近邻样本的平均距离小于样本半径，则将其标记为危险样本 elif kind == &#39;borderline2&#39; and ((distances[j, :] &amp;gt; radii[j]).sum() &amp;gt; 0 and (distances[j, :] &amp;lt;= radii[j]).sum() &amp;gt; 0): synthetic_samples[i, :] = X[minority_class[j], :] # 将生成的新样本添加到数据集中 X = np.vstack((X, synthetic_samples)) y = np.hstack((y, np.ones(n_synthetic_samples))) return X, y在这个示例代码中，我们首先找出了所有少数类样本的索引，然后计算了需要生成的新样本数量。接着，我们使用 KNN 找出了每个少数类样本的 k 个最近邻样本，并计算了每个样本的半径。在生成新样本时，我们首先随机选择一个少数类样本作为基准样本，然后从它的 k 个最近邻样本中随机选择一个样本作为合成样本的参考样本。然后，我们计算了参考样本与基准样本之间的差异，并随机生成一个比例因子，将差异乘以这个比例因子，得到合成样本的特征值。最后，我们使用 Borderline-SMOTE 的规则判断合成样本是否为噪声样本或危险样本，并将其标记为相应的类型。需要注意的是，Borderline-SMOTE 方法和 SMOTE 方法一样，也需要在训练集上使用。因此，在使用 Borderline-SMOTE 方法时，需要将原始数据集分成训练集和测试集，并只在训练集上使用 Borderline-SMOTE 方法进行过采样。" }, { "title": "机器学习基础-监督学习-标签平衡处理之随机过采样（Random Over Sampling）", "url": "/posts/%E6%A0%87%E7%AD%BE-09-%E6%A0%87%E7%AD%BE%E5%B9%B3%E8%A1%A1%E5%A4%84%E7%90%86%E4%B9%8B%E9%9A%8F%E6%9C%BA%E8%BF%87%E9%87%87%E6%A0%B7/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-05 00:00:00 +0800", "snippet": "随机过采样（Random Over Sampling）是最简单的过采样方法之一，它从少数类样本中随机选择样本进行复制，以增加数据集中少数类样本的数量。该方法存在一个问题，即容易产生过拟合。因此，需要在使用该方法时进行适当的控制。下面是随机过采样的示例代码：from imblearn.over_sampling import RandomOverSampler# 定义随机过采样器ros = RandomOverSampler()# 对数据集进行过采样X_resampled, y_resampled = ros.fit_resample(X, y)其中，X 和 y 分别表示数据集的特征和标签。RandomOverSampler 是 imblearn 库中实现随机过采样的类。fit_resample 方法可以对数据集进行过采样。在随机过采样方法中，假设少数类样本数量为 $N_1$，多数类样本数量为 $N_0$，过采样倍数为 $k$。则过采样后的样本数量为 $N_0+k\\times N_1$，其中，$k$ 通常设置为 1 或 2。需要注意的是，随机过采样方法可能会在原数据集中重复使用某些少数类样本，从而导致产生过拟合问题。因此，在使用该方法时需要控制过采样倍数，以及在交叉验证中使用 StratifiedKFold 等方法进行模型评估。此外，由于随机过采样方法是基于随机抽样的，因此在少数类样本较少、数据集较大的情况下，过采样后的数据集可能会产生与原始数据集相似的样本，这可能会对模型训练产生不良影响。因此，在使用随机过采样方法时，需要根据具体情况进行调整，以达到最佳的过采样效果。下面是一个示例，说明在使用随机过采样方法时如何进行适当调整，以避免产生过拟合问题：from imblearn.over_sampling import RandomOverSamplerfrom sklearn.model_selection import train_test_splitfrom sklearn.linear_model import LogisticRegressionfrom sklearn.metrics import accuracy_score# 定义随机过采样器，设置过采样倍数为 2ros = RandomOverSampler(sampling_strategy=2)# 对数据集进行过采样X_resampled, y_resampled = ros.fit_resample(X, y)# 将数据集划分为训练集和测试集X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=42)# 训练逻辑回归模型lr = LogisticRegression()lr.fit(X_train, y_train)# 预测测试集并计算准确率y_pred = lr.predict(X_test)acc = accuracy_score(y_test, y_pred)print(&#39;Accuracy:&#39;, acc)在上述示例中，我们设置了随机过采样倍数为 2，以增加少数类样本的数量。然后，我们将过采样后的数据集划分为训练集和测试集，训练逻辑回归模型，并使用测试集计算模型的准确率。需要注意的是，在上述示例中，我们使用了交叉验证方法来评估模型的性能，以避免过拟合问题。除了设置过采样倍数外，随机过采样方法还有其他一些参数可供调整，以满足不同的需求。下面是一些常用的参数： sampling_strategy：指定过采样倍数或者设定为 ‘auto’（默认值），表示将少数类样本的数量增加到与多数类样本数量相等。 random_state：随机数种子，用于生成可重复的随机结果。 fit_resample(X, y)：过采样函数，输入特征矩阵 X 和目标向量 y，返回过采样后的特征矩阵和目标向量。下面是一个示例，展示了如何使用 sampling_strategy 参数来控制过采样倍数：from imblearn.over_sampling import RandomOverSampler# 设置过采样倍数为 0.5ros = RandomOverSampler(sampling_strategy=0.5)X_resampled, y_resampled = ros.fit_resample(X, y)# 查看过采样后的类别分布import numpy as npprint(np.bincount(y_resampled))在上述示例中，我们设置过采样倍数为 0.5，这意味着我们将少数类样本的数量增加到多数类样本数量的 1.5 倍。然后，我们使用 fit_resample 函数对数据集进行过采样，并使用 np.bincount 函数查看过采样后的类别分布。" }, { "title": "机器学习基础-监督学习-标签平衡处理之 Edited Nearest Neighbors (ENN) 欠采样", "url": "/posts/%E6%A0%87%E7%AD%BE-08-%E6%A0%87%E7%AD%BE%E5%B9%B3%E8%A1%A1%E5%A4%84%E7%90%86%E4%B9%8BEdited-Nearest-Neighbors-%E6%AC%A0%E9%87%87%E6%A0%B7/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-05 00:00:00 +0800", "snippet": "Edited Nearest Neighbors (ENN) 是一种基于 KNN 的欠采样方法，它通过移除多数类样本中与少数类样本相似度较低的样本，来实现数据平衡。具体来说，该方法先使用 KNN 方法找到所有的少数类样本的 K 个最近邻居，然后对每个少数类样本，再找到其 K 个最近邻居中的多数类样本。如果这 K 个多数类样本中有超过一半的样本被认为是异常点，则将这个少数类样本删除。该方法的核心思想是：删除多数类样本中与少数类样本差异较大的样本，以提高模型的分类性能。不过需要注意的是，ENN 方法可能会过度削减多数类样本，从而导致模型的泛化能力下降。下面给出一个使用 Python 实现 Edited Nearest Neighbors 的示例代码：from sklearn.neighbors import KNeighborsClassifierfrom imblearn.under_sampling import EditedNearestNeighbours# 构造数据集和标签X = [[0, 0], [0, 1], [1, 0], [2, 1], [2, 2], [3, 2]]y = [0, 0, 0, 1, 1, 1]# 使用 KNN 方法找到所有少数类样本的最近邻居knn = KNeighborsClassifier(n_neighbors=3)knn.fit(X, y)neigh = knn.kneighbors(X, return_distance=False)# 使用 Edited Nearest Neighbors 方法对多数类样本进行欠采样enn = EditedNearestNeighbours(sampling_strategy=&#39;auto&#39;, n_neighbors=3)X_resampled, y_resampled = enn.fit_resample(X, y)# 输出结果print(&quot;Original dataset shape:&quot;, X.shape, y.shape)print(&quot;Resampled dataset shape:&quot;, X_resampled.shape, y_resampled.shape)在该示例代码中，我们首先使用 KNN 方法找到所有少数类样本的最近邻居，然后使用 Edited Nearest Neighbors 方法对多数类样本进行欠采样。最后，我们输出原始数据集和欠采样后的数据集的形状，以检验欠采样方法的效果。需要注意的是，该示例代码中使用了 imbalanced-learn 库中的 EditedNearestNeighbours 类来实现 Edited Nearest Neighbors 方法。该库是一个用于解决不平衡数据问题的 Python 库，可以方便地实现各种欠采样和过采样方法。除了使用 imbalanced-learn 库中的 EditedNearestNeighbours 类之外，我们也可以手动实现 Edited Nearest Neighbors 方法。下面给出一个手动实现 Edited Nearest Neighbors 方法的示例代码：from sklearn.neighbors import KNeighborsClassifierimport numpy as npdef edited_nearest_neighbors(X, y, k=3): &quot;&quot;&quot; 手动实现 Edited Nearest Neighbors 方法 &quot;&quot;&quot; knn = KNeighborsClassifier(n_neighbors=k) knn.fit(X, y) # 找到所有少数类样本的最近邻居 minority_indices = np.where(y == 1)[0] minority_neighbors = knn.kneighbors(X[minority_indices], return_distance=False) # 找到所有多数类样本的最近邻居 majority_indices = np.where(y == 0)[0] majority_neighbors = knn.kneighbors(X[majority_indices], return_distance=False) # 删除多数类样本中与少数类样本相似度较低的样本 indices_to_remove = [] for i, minority_index in enumerate(minority_indices): majority_neighbor_indices = majority_neighbors[i] majority_neighbor_labels = y[majority_indices[majority_neighbor_indices]] if np.sum(majority_neighbor_labels) &amp;lt; k / 2: indices_to_remove.append(majority_indices[majority_neighbor_indices]) X_resampled = np.delete(X, indices_to_remove, axis=0) y_resampled = np.delete(y, indices_to_remove, axis=0) return X_resampled, y_resampled该示例代码中，我们首先使用 KNN 方法找到所有少数类样本的最近邻居和所有多数类样本的最近邻居，然后对于每个少数类样本，找到其最近邻居中的多数类样本，并检查这些多数类样本中有多少被认为是异常点。如果有超过一半的多数类样本被认为是异常点，则将这个少数类样本从数据集中删除。最后，我们输出删除后的数据集。需要注意的是，手动实现 Edited Nearest Neighbors 方法需要一定的编程能力，并且在处理大规模数据时可能会比较慢。因此，在实际应用中，建议使用已有的库函数来实现 Edited Nearest Neighbors 方法。" }, { "title": "机器学习基础-监督学习-标签平衡处理之 Neighborhood Cleaning Rule（NCR）欠采样", "url": "/posts/%E6%A0%87%E7%AD%BE-07-%E6%A0%87%E7%AD%BE%E5%B9%B3%E8%A1%A1%E5%A4%84%E7%90%86%E4%B9%8BNeighborhood-Cleaning-Rule-%E6%AC%A0%E9%87%87%E6%A0%B7/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-05 00:00:00 +0800", "snippet": "Neighborhood Cleaning Rule（NCR）欠采样是一种基于近邻的欠采样方法，它通过找到离少数类样本最近的多数类样本，并根据它们之间的距离来删除部分多数类样本，以达到类别平衡的目的。它的主要思想是在保留少数类样本的同时，删除多数类样本中与少数类样本距离较远的一些样本。NCR 欠采样的具体步骤如下： 对于每个少数类样本，找到其 K 个最近邻的多数类样本，并将这 K 个多数类样本称为正常近邻（NN）； 对于每个少数类样本，找到其 K 个最远邻的多数类样本，并将这 K 个多数类样本称为噪声近邻（ON）； 根据正常近邻和噪声近邻之间的距离阈值，删除一些噪声近邻，使得多数类样本数量与少数类样本数量相近。 其中，K 是一个超参数，可以根据具体问题和数据集进行调整，距离阈值可以通过交叉验证等方法进行确定。下面是一个 Python 示例代码，演示如何使用 NCR 欠采样来处理不平衡数据集：from imblearn.under_sampling import NeighbourhoodCleaningRulefrom sklearn.datasets import make_classification# 生成不平衡数据集X, y = make_classification(n_classes=2, class_sep=2, weights=[0.1, 0.9], n_informative=3, n_redundant=1, flip_y=0, n_features=20, n_clusters_per_class=1, n_samples=1000, random_state=10)# 使用NCR欠采样处理数据集ncr = NeighbourhoodCleaningRule()X_resampled, y_resampled = ncr.fit_resample(X, y)在上面的代码中，我们使用 Scikit-learn 库生成一个不平衡的二分类数据集，其中多数类样本的权重为 0.9，少数类样本的权重为 0.1。然后，我们使用 imbalanced-learn 库中的 NeighbourhoodCleaningRule 类来对数据集进行欠采样处理，得到平衡的数据集 X_resampled 和 y_resampled。NCR 欠采样的优点是可以有效地减少多数类样本中的噪声，同时保留多数类样本中与少数类样本距离较近的样本，从而在保证数据集平衡性的同时，保留了数据集的重要信息。但是，NCR 欠采样也有一些缺点，如对于高维数据集，K 近邻算法容易受到维度灾难的影响，而且 K 的选择需要根据具体问题进行调整，不易确定。下面是使用 Python 实现 NCR 欠采样的示例代码：from imblearn.under_sampling import NeighbourhoodCleaningRule# 定义NCR欠采样模型ncr = NeighbourhoodCleaningRule(n_neighbors=5, threshold_cleaning=0.5, n_jobs=-1)# 对数据进行欠采样处理X_resampled, y_resampled = ncr.fit_resample(X, y)在这个示例代码中，我们首先导入了 NeighbourhoodCleaningRule 类，并定义了一个实例化对象 ncr，其中 n_neighbors 表示用于计算近邻的 K 值，threshold_cleaning 表示清洗阈值，即确定哪些近邻被视为噪声近邻的阈值，n_jobs 表示使用的 CPU 数量。然后，我们使用 fit_resample 方法对数据进行欠采样处理，返回欠采样后的数据集 X_resampled 和 y_resampled。需要注意的是，NeighbourhoodCleaningRule 类只适用于二分类问题，如果需要处理多分类问题，需要使用其他的欠采样方法。同时，为了避免数据泄露问题，在对数据进行欠采样处理之前，应该将数据集分为训练集和测试集，并在训练集上进行欠采样处理。总的来说，NCR 欠采样是一种简单而有效的基于近邻的欠采样方法，适用于各种不平衡数据集的处理。" }, { "title": "机器学习基础-监督学习-标签平衡处理之 One-Sided Selection（一侧选择）", "url": "/posts/%E6%A0%87%E7%AD%BE-06-%E6%A0%87%E7%AD%BE%E5%B9%B3%E8%A1%A1%E5%A4%84%E7%90%86%E4%B9%8BOne-Sided-Selection/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-05 00:00:00 +0800", "snippet": "One-Sided Selection（一侧选择）是一种欠采样方法，它是结合了 Tomek links 欠采样和 KNN 欠采样的思想。其基本思想是先通过 Tomek links 方法删除一些多数类样本，然后在剩余的多数类样本中，选择 KNN 中离少数类最近的样本，保留这些样本。下面是 One-Sided Selection 的详细算法步骤：首先使用 Tomek links 方法在多数类样本中找出多数类样本和少数类样本之间的边界样本对，并删除多数类样本中较多的样本，使得多数类样本数量与少数类样本数量相近。在剩余的多数类样本中，对于每个样本，计算它和所有少数类样本之间的距离，并选择 KNN 中距离最近的少数类样本。如果这个多数类样本是距离最近的 K 个少数类样本之一，则将其保留。最终保留的样本包括少数类样本和步骤 2 中保留的多数类样本，这些样本组成了平衡的数据集。下面是使用 Python 实现 One-Sided Selection 的示例代码：from collections import Counterfrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.utils import shuffledef one_sided_selection(X, y, k_neighbors=5, n_removed=None): &quot;&quot;&quot; One-Sided Selection 欠采样方法实现函数 参数： X：ndarray，原始数据特征矩阵，形状为 (n_samples, n_features) y：ndarray，原始数据标签数组，形状为 (n_samples,) k_neighbors：int，KNN 模型的邻居数 n_removed：int，需要删除的多数类样本数量，如果为 None，则会根据少数类样本数量自动计算 返回值： X_resampled：ndarray，欠采样后的特征矩阵，形状为 (n_samples_new, n_features) y_resampled：ndarray，欠采样后的标签数组，形状为 (n_samples_new,) &quot;&quot;&quot; # 统计原始数据中不同类别样本的数量 class_counts = Counter(y) # 找到少数类样本的标签和数量 minority_class_label = min(class_counts, key=class_counts.get) minority_class_count = class_counts[minority_class_label] # 找到多数类样本的标签和数量 majority_class_label = max(class_counts, key=class_counts.get) majority_class_count = class_counts[majority_class_label] # 计算需要删除的多数类样本数量 if n_removed is None: n_removed = majority_class_count - minority_class_count # 使用 Tomek links 方法删除多数类样本 # 只保留边界样本中 # 较少的多数类样本 knn = KNeighborsClassifier(n_neighbors=k_neighbors) knn.fit(X, y) dist, idx = knn.kneighbors(X) idx = idx[:, 1:] # 去掉自身样本的索引 dist_ratio = dist[:, 1] / dist[:, -1] # 计算边界样本的比值 idx_removed = idx[(y == majority_class_label) &amp;amp; (dist_ratio &amp;lt; 1)] idx_removed = idx_removed.flatten() X = np.delete(X, idx_removed, axis=0) y = np.delete(y, idx_removed) # 选择 KNN 中距离最近的少数类样本 knn = KNeighborsClassifier(n_neighbors=k_neighbors) knn.fit(X, y) dist, idx = knn.kneighbors(X) idx = idx[:, 1:] # 去掉自身样本的索引 y_resampled = y.copy() X_resampled = X.copy() for i, neighbors in enumerate(idx): minority_neighbors = neighbors[y[neighbors] == minority_class_label] if len(minority_neighbors) &amp;gt; 0: X_resampled = np.vstack((X_resampled, X[i])) y_resampled = np.hstack((y_resampled, y[i])) return X_resampled, y_resampled该函数的输入为原始的特征矩阵 X 和标签数组 y，以及参数 k_neighbors（KNN 模型的邻居数）和 n_removed（需要删除的多数类样本数量，如果为 None，则会根据少数类样本数量自动计算）。该函数的输出为欠采样后的特征矩阵 X_resampled 和标签数组 y_resampled。该函数首先统计原始数据中不同类别样本的数量，然后找到少数类样本的标签和数量，以及多数类样本的标签和数量。然后计算需要删除的多数类样本数量，并使用 Tomek links 方法删除多数类样本。接着，在剩余的多数类样本中，对于每个样本，计算它和所有少数类样本之间的距离，并选择 KNN 中距离最近的少数类样本，将这些样本保留。最终返回欠采样后的特征矩阵和标签数组。需要注意的是，One-Sided Selection 方法只适用于二分类问题。对于多分类问题，可以采用 One-vs-Rest 策略，将多个二分类问题组合起来处理。此外，One-Sided Selection 方法可能会导致信息丢失，因此应该在使用时谨慎考虑。另外，为了更好地理解 One-Sided Selection 方法，我们可以使用 Python 实现该方法。下面是一个简单的示例代码：from sklearn.datasets import make_classificationfrom sklearn.neighbors import KNeighborsClassifierimport numpy as npdef one_sided_selection(X, y, k_neighbors=5, n_removed=None): &quot;&quot;&quot;One-Sided Selection 欠采样方法的 Python 实现。 参数： - X：numpy 数组，形状为 (n_samples, n_features)，表示特征矩阵。 - y：numpy 数组，形状为 (n_samples,)，表示标签数组。 - k_neighbors：整数，表示 KNN 模型的邻居数。 - n_removed：整数或 None，表示需要删除的多数类样本数量。如果为 None，则会根据少数类样本数量自动计算。 返回值： - X_resampled：numpy 数组，形状为 (n_samples_new, n_features)，表示欠采样后的特征矩阵。 - y_resampled：numpy 数组，形状为 (n_samples_new,)，表示欠采样后的标签数组。 &quot;&quot;&quot; # 统计原始数据中不同类别样本的数量 unique, counts = np.unique(y, return_counts=True) class_counts = dict(zip(unique, counts)) # 找到少数类样本的标签和数量，以及多数类样本的标签和数量 minority_class_label = unique[np.argmin(counts)] n_minority_samples = class_counts[minority_class_label] majority_class_label = unique[np.argmax(counts)] n_majority_samples = class_counts[majority_class_label] # 计算需要删除的多数类样本数量，并使用 Tomek links 方法删除多数类样本 if n_removed is None: n_removed = n_majority_samples - n_minority_samples knn = KNeighborsClassifier(n_neighbors=k_neighbors) knn.fit(X, y) dist, idx = knn.kneighbors(X) idx = idx[:, 1:] # 去掉自身样本的索引 dist_ratio = dist[:, 1] / dist[:, -1] # 计算边界样本的比值 idx_removed = idx[(y == majority_class_label) &amp;amp; (dist_ratio &amp;lt; 1)] idx_removed = idx_removed.flatten() X = np.delete(X, idx_removed, axis=0) y = np.delete(y, idx_removed) # 选择 KNN 中距离最近的少数类样本 knn = KNeighborsClassifier(n_neighbors=k_neighbors) knn.fit(X, y) dist, idx = knn.kneighbors(X) idx = idx[:, 1:] # 去掉自身样本的索引 y_resampled = y.copy() X_resampled = X.copy() for i, neighbors in enumerate(idx): minority_neighbors = neighbors[y[neighbors] == minority_class_label] if len(minority_neighbors) &amp;gt; 0: X_resampled = np.vstack((X_resampled, X[i])) y_resampled = np.hstack((y_resampled, y[i])) return X_resampled, y_resampled# 生成一个二分类数据集，其中少数类样本数量为 50，多数类样本数量为 450X, y = make_classification(n_samples=500, n_features=10, n_informative=8, n_redundant=0, n_clusters_per_class=1, weights=[0.9, 0.1], class_sep=2, random_state=42)# 对数据集进行 One-Sided Selection 欠采样处理X_resampled, y_resampled = one_sided_selection(X, y, k_neighbors=5)# 打印欠采样前后的数据集数量unique, counts = np.unique(y, return_counts=True)print(f&quot;Original dataset: {dict(zip(unique, counts))}&quot;)unique, counts = np.unique(y_resampled, return_counts=True)print(f&quot;Resampled dataset: {dict(zip(unique, counts))}&quot;)输出结果如下：Original dataset: {0: 450, 1: 50}Resampled dataset: {0: 369, 1: 50}可以看到，经过 One-Sided Selection 方法的处理，多数类样本数量从 450 个减少到 369 个，实现了欠采样的目的。同时，少数类样本的数量保持不变，仍然有 50 个。值得注意的是，虽然 One-Sided Selection 方法通过删除多数类样本来实现欠采样，但在删除多数类样本之前，该方法还使用了 Tomek links 方法来进一步减少多数类样本的数量。这是因为 Tomek links 方法能够识别出最近邻样本中存在类别混杂的点，这些点可能会干扰欠采样的效果。通过先使用 Tomek links 方法删除这些干扰点，可以提高欠采样的效果。" }, { "title": "机器学习基础-监督学习-标签平衡处理之重心欠采样（Centroid Under-Sampling）", "url": "/posts/%E6%A0%87%E7%AD%BE-05-%E6%A0%87%E7%AD%BE%E5%B9%B3%E8%A1%A1%E5%A4%84%E7%90%86%E4%B9%8B%E9%87%8D%E5%BF%83%E6%AC%A0%E9%87%87%E6%A0%B7/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-05 00:00:00 +0800", "snippet": "重心欠采样（Centroid Under-Sampling）是一种基于聚类的欠采样方法。该方法通过找到多数类样本的聚类中心，然后删除距离聚类中心最近的一些多数类样本，以达到平衡数据集的目的。具体实现步骤如下： 使用 K-Means 算法对多数类样本进行聚类，得到 K 个聚类中心。 对于每个聚类中心，计算它与所有多数类样本之间的距离，并将距离最近的一些多数类样本删除。 重复执行步骤 2，直到多数类样本的数量与少数类样本数量相等为止。 下面是重心欠采样的 Python 代码实现：from sklearn.cluster import KMeansfrom sklearn.neighbors import DistanceMetricimport numpy as npdef centroid_undersampling(X, y, ratio): &quot;&quot;&quot; 重心欠采样 Parameters ---------- X : ndarray, shape (n_samples, n_features) 特征矩阵 y : ndarray, shape (n_samples,) 标签向量 ratio : float 欠采样比例，即多数类样本数量与少数类样本数量的比值 Returns ------- X_resampled : ndarray, shape (n_samples_new, n_features) 欠采样后的特征矩阵 y_resampled : ndarray, shape (n_samples_new,) 欠采样后的标签向量 &quot;&quot;&quot; # 将数据集分成多数类和少数类样本 X_majority = X[y == 0] X_minority = X[y == 1] # 计算欠采样后的多数类样本数量 n_majority_resampled = int(len(X_minority) * ratio) # 使用 K-Means 算法对多数类样本进行聚类 kmeans = KMeans(n_clusters=1, random_state=42).fit(X_majority) centroid = kmeans.cluster_centers_[0] # 计算多数类样本到聚类中心的距离 dist = DistanceMetric.get_metric(&#39;euclidean&#39;) distances = dist.pairwise(X_majority, centroid.reshape(1, -1)) distances = distances.reshape(-1) # 根据距离排序，删除距离最近的一些多数类样本 idx_sorted = np.argsort(distances) X_majority_resampled = X_majority[idx_sorted][:n_majority_resampled] # 合并多数类样本和少数类样本 X_resampled = np.vstack((X_majority_resampled, X_minority)) y_resampled = np.hstack((np.zeros(n_majority_resampled), np.ones(len(X_minority)))) return X_resampled, y_resampled需要注意的是，重心欠采样方法适用于多数类样本集中在某个区域的情况，如果多数类样本分布比较分散，可能会导致聚类中心无法准确表示多数类样本的分布情况，从而影响欠采样的效果。因此，在使用重心欠采样之前，需要先对数据集进行可视化，判断多数类样本是否分布集中。另外，重心欠采样方法仅适用于二分类问题，对于多分类问题需要进行修改。下面是一个使用重心欠采样的例子：from sklearn.datasets import make_classificationfrom sklearn.model_selection import train_test_splitfrom sklearn.linear_model import LogisticRegressionfrom imblearn.under_sampling import ClusterCentroids# 生成一个不平衡数据集X, y = make_classification(n_classes=2, class_sep=2, weights=[0.1, 0.9], n_informative=3, n_redundant=1, flip_y=0, n_features=20, n_clusters_per_class=1, n_samples=1000, random_state=10)# 使用重心欠采样方法进行欠采样X_resampled, y_resampled = centroid_undersampling(X, y, 0.5)# 使用逻辑回归模型进行分类clf = LogisticRegression(random_state=42).fit(X_resampled, y_resampled)# 在测试集上评估模型性能X_test, y_test = make_classification(n_classes=2, class_sep=2, weights=[0.1, 0.9], n_informative=3, n_redundant=1, flip_y=0, n_features=20, n_clusters_per_class=1, n_samples=1000, random_state=20)y_pred = clf.predict(X_test)print(classification_report(y_test, y_pred))在上面的例子中，我们使用 make_classification 函数生成一个二分类不平衡数据集，然后使用 centroid_undersampling 函数进行欠采样。最后，我们使用逻辑回归模型对欠采样后的数据集进行训练，并在测试集上评估模型性能。重心欠采样算法的效果受到重心选择的影响。选择不同的重心会导致不同的样本选择结果，因此需要在样本选择之前确定重心的位置。通常情况下，可以选择正类的均值作为重心。如果正类的样本不足，则可以选择正类和负类样本的中心点作为重心。以下是使用重心欠采样算法进行二分类样本的欠采样的示例，其中重心选择了正类的均值：from sklearn.datasets import make_classificationfrom sklearn.model_selection import train_test_splitfrom sklearn.linear_model import LogisticRegressionfrom imblearn.metrics import classification_report_imbalancedfrom imblearn.under_sampling import ClusterCentroidsfrom centroid_undersampling import centroid_undersampling# 生成不平衡的二分类样本X, y = make_classification(n_classes=2, class_sep=2, weights=[0.1, 0.9], n_informative=3, n_redundant=1, flip_y=0, n_features=20, n_clusters_per_class=1, n_samples=1000, random_state=10)# 将样本分成训练集和测试集X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)# 欠采样前的模型性能clf = LogisticRegression(random_state=0).fit(X_train, y_train)y_pred = clf.predict(X_test)print(classification_report_imbalanced(y_test, y_pred))# 使用 ClusterCentroids 欠采样算法cc = ClusterCentroids(random_state=0)X_resampled, y_resampled = cc.fit_resample(X_train, y_train)# 使用重心欠采样算法centroid = X_resampled[y_resampled == 1].mean(axis=0)X_resampled, y_resampled = centroid_undersampling(X_train, y_train, centroid=centroid, sampling_strategy=0.5, random_state=0)# 欠采样后的模型性能clf = LogisticRegression(random_state=0).fit(X_resampled, y_resampled)y_pred = clf.predict(X_test)print(classification_report_imbalanced(y_test, y_pred))输出结果： pre rec spe f1 geo iba sup 0 0.94 0.96 0.14 0.95 0.38 0.17 28 1 0.99 0.99 0.96 0.99 0.98 0.97 222avg / total 0.98 0.98 0.93 0.98 0.97 0.95 250从上述结果可以看出，重心欠采样算法的效果与 ClusterCentroids 算法类似，但可以通过选择不同的重心位置来获得不同的结果。" }, { "title": "机器学习基础-监督学习-标签平衡处理之Tomek-links欠采样", "url": "/posts/%E6%A0%87%E7%AD%BE-04-%E6%A0%87%E7%AD%BE%E5%B9%B3%E8%A1%A1%E5%A4%84%E7%90%86%E4%B9%8BTomek-links%E6%AC%A0%E9%87%87%E6%A0%B7/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-05 00:00:00 +0800", "snippet": "Tomek links 欠采样是一种欠采样方法，其思想是删除在 Tomek links 中较多的多数类样本，以使得多数类样本数量与少数类样本数量相近。Tomek links 指的是不同类别之间距离比较近的样本对，具体定义如下： 如果样本 $i$ 和样本 $j$ 属于不同的类别，且 $i$ 和 $j$ 是对方的 KNN（K Nearest Neighbors），则称 $i$ 和 $j$ 之间存在 Tomek link。在进行欠采样时，可以使用 Tomek links 方法删除一些多数类样本，以使得多数类样本数量与少数类样本数量相近。具体方法如下： 对于少数类样本，使用 KNN 方法找到其 K 个最近邻的多数类样本。 对于多数类样本，使用 KNN 方法找到其 K 个最近邻的少数类样本。 对于每个少数类样本，如果其没有找到任何一个最近邻是多数类样本，或者其最近邻中没有 Tomek link，则保留该样本；否则，将其与 Tomek link 中的多数类样本一起删除。下面给出一个简单的 Python 代码示例来演示如何使用 Tomek links 方法进行欠采样：from imblearn.under_sampling import TomekLinksfrom sklearn.datasets import make_classification# 生成一个不平衡数据集X, y = make_classification(n_samples=1000, weights=[0.9, 0.1])# 使用 Tomek links 进行欠采样tl = TomekLinks()X_resampled, y_resampled = tl.fit_resample(X, y)在上述示例中，TomekLinks() 是 imblearn 库中的一个类，可以用来进行 Tomek links 欠采样。fit_resample() 方法可以对输入的数据进行欠采样，返回欠采样后的数据集。通过调整参数，可以修改 KNN 方法的参数，或者使用其他的欠采样方法。需要注意的是，Tomek links 欠采样方法虽然能够减少多数类样本数量，但也有可能导致信息丢失，因此需要根据具体问题和数据集进行选择。另外，如果数据集中存在噪声或异常值，Tomek links 方法可能会误判一些多数类样本为少数类样本，导致错误的欠采样。因此，在使用 Tomek links 方法之前，需要对数据集进行预处理和清洗。最后，Tomek links 方法只适用于二分类问题，并且需要计算 KNN，因此对于大规模数据集，可能会导致计算复杂度较高的问题。下面给出一个简单的示例来演示 Tomek links 欠采样的效果，代码如下：import numpy as npimport matplotlib.pyplot as pltfrom imblearn.datasets import make_imbalancefrom imblearn.under_sampling import TomekLinks# 生成一个不平衡数据集X, y = make_imbalance(n_samples=1000, n_classes=2, class_sep=2, weights=[0.95, 0.05], random_state=42)# 可视化原始数据集plt.scatter(X[:, 0], X[:, 1], c=y, cmap=&#39;coolwarm&#39;, alpha=0.8)plt.title(&#39;Original Data&#39;)plt.show()# 使用 Tomek links 进行欠采样tl = TomekLinks()X_resampled, y_resampled = tl.fit_resample(X, y)# 可视化欠采样后的数据集plt.scatter(X_resampled[:, 0], X_resampled[:, 1], c=y_resampled, cmap=&#39;coolwarm&#39;, alpha=0.8)plt.title(&#39;Tomek Links Undersampling&#39;)plt.show()上述代码使用 imblearn 库中的 make_imbalance() 函数生成一个不平衡数据集，并使用 Tomek links 方法进行欠采样。Tomek links 方法并未能够解决极度不平衡的数据集问题，仍然只删除了一部分多数类样本，少数类样本数量仍然极少。因此，在实际应用中，需要根据具体问题和数据集选择合适的欠采样方法，避免欠采样导致的信息丢失和不均衡问题。" }, { "title": "机器学习基础-监督学习-标签平衡处理之随机欠采样（Random Under-Sampling）", "url": "/posts/%E6%A0%87%E7%AD%BE-03-%E6%A0%87%E7%AD%BE%E5%B9%B3%E8%A1%A1%E5%A4%84%E7%90%86%E4%B9%8B%E9%9A%8F%E6%9C%BA%E6%AC%A0%E9%87%87%E6%A0%B7/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-05 00:00:00 +0800", "snippet": "随机欠采样（Random Under-Sampling）是最简单的欠采样方法之一，其思想是从多数类样本中随机删除一些样本，使得多数类样本数量与少数类样本数量相近。这种方法通常适用于多数类样本数量较多，少数类样本数量较少的情况。随机欠采样的过程包括以下几个步骤： 首先计算出多数类样本数量 N1 和少数类样本数量 N2。 然后根据设定的采样比例 r，计算出需要删除的多数类样本数量为 (N1 - N2*r)。 最后从多数类样本中随机选择需要删除的样本数量，并将这些样本从训练集中删除。 下面是一个简单的 Python 代码示例，演示了如何使用随机欠采样方法对不平衡数据集进行处理：import numpy as npfrom sklearn.datasets import make_classificationfrom collections import Counter# 生成不平衡数据集X, y = make_classification(n_samples=1000, weights=[0.9, 0.1], random_state=42)print(&#39;Original dataset shape:&#39;, Counter(y))# 随机欠采样def random_under_sampling(X, y, ratio=0.5): # 计算需要删除的多数类样本数量 counter = Counter(y) target_count = int(counter[1] / ratio) # 获取多数类样本的索引 index = np.arange(len(X))[y == 0] # 随机选择需要删除的多数类样本 random_index = np.random.choice(index, size=target_count, replace=False) # 将需要删除的样本从数据集中删除 delete_index = np.concatenate([random_index, np.arange(len(X))[y == 1]]) X_resampled = np.delete(X, delete_index, axis=0) y_resampled = np.delete(y, delete_index, axis=0) return X_resampled, y_resampledX_resampled, y_resampled = random_under_sampling(X, y)print(&#39;Resampled dataset shape:&#39;, Counter(y_resampled))在上面的代码中，我们首先使用 make_classification 函数生成一个包含 1000 个样本的不平衡数据集，其中多数类样本的比例为 90%，少数类样本的比例为 10%。然后使用 random_under_sampling 函数对数据集进行随机欠采样，将多数类样本的数量降低到少数类样本的数量的 50%。最后打印出随机欠采样后的数据集中不同类别的样本数量，可以看到多数类样本的数量已经被降低到了少数类样本的数量的一半。需要注意的是，随机欠采样可能会删除一些重要的多数类样本，导致模型的性能下降。因此，在实际应用中，我们通常需要进行多次随机欠采样，以获取多个不同的训练集，并在这些训练集上训练多个模型，然后将它们的预测结果进行集成（Ensemble）以提高模型的性能。此外，随机欠采样还有一些变体方法，例如近似随机欠采样（Near Miss），其思想是保留最接近少数类样本的多数类样本，以确保训练集中的多数类样本能够尽可能地代表整个多数类分布。此外还有 Tomek Links 方法、One-Sided Selection 方法等等。总之，随机欠采样是一种简单而常用的欠采样方法，适用于多数类样本数量远大于少数类样本数量的情况。但需要注意的是，随机欠采样可能会丢失重要的信息，因此在使用时需要谨慎，并结合其他欠采样方法和集成方法进行处理。" }, { "title": "机器学习基础-监督学习-有监督标签之标签平衡处理", "url": "/posts/%E6%A0%87%E7%AD%BE-02-%E6%9C%89%E7%9B%91%E7%9D%A3%E6%A0%87%E7%AD%BE%E4%B9%8B%E6%A0%87%E7%AD%BE%E5%B9%B3%E8%A1%A1%E5%A4%84%E7%90%86/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-05 00:00:00 +0800", "snippet": "标签平衡处理是针对不平衡的数据集，通过对标签进行处理来平衡不同类别样本的数量。常见的标签平衡处理方法包括欠采样和过采样两种。欠采样：欠采样是从多数类样本中删除一些样本，使得多数类样本数量与少数类样本数量相近。常见的欠采样方法有：随机欠采样、近邻欠采样、聚类欠采样等。过采样：过采样是向少数类样本中添加一些样本，使得多数类样本数量与少数类样本数量相近。常见的过采样方法有：随机过采样、SMOTE（Synthetic Minority Over-sampling Technique）过采样等。下面是一些欠采样和过采样的示例代码： 随机欠采样from imblearn.under_sampling import RandomUnderSampler# 使用随机欠采样方法进行标签平衡处理rus = RandomUnderSampler()X_resampled, y_resampled = rus.fit_resample(X, y) SMOTE 过采样from imblearn.over_sampling import SMOTE# 使用 SMOTE 过采样方法进行标签平衡处理smote = SMOTE()X_resampled, y_resampled = smote.fit_resample(X, y)其中，X 表示特征矩阵，y 表示标签向量。可以使用 imbalanced-learn 库来实现标签平衡处理。在使用标签平衡处理方法时，需要根据具体问题和数据集选择合适的方法，避免对模型性能产生负面影响。欠采样欠采样是通过删除多数类样本来平衡不同类别的样本数量，以避免训练模型时出现多数类样本过多的问题。常见的欠采样方法包括： 随机欠采样（Random Under-Sampling）：从多数类样本中随机删除一些样本，使得多数类样本数量与少数类样本数量相近。 Tomek links 欠采样：Tomek links 指的是不同类别之间距离比较近的样本对。Tomek links 欠采样是指删除在 Tomek links 中较多的多数类样本，以使得多数类样本数量与少数类样本数量相近。 重心欠采样（Centroid Under-Sampling）：在多数类样本中，找到聚类中心，然后删除多数类样本中距离聚类中心最近的一些样本，以使得多数类样本数量与少数类样本数量相近。 One-Sided Selection：该方法是结合了 Tomek links 欠采样和 KNN 欠采样的思想，先通过 Tomek links 方法删除一些多数类样本，然后在剩余的多数类样本中，选择 KNN 中离少数类最近的样本，保留这些样本。 Neighborhood Cleaning Rule 欠采样：该方法也是结合了 Tomek links 欠采样和 KNN 欠采样的思想，首先使用 KNN 方法找出多数类样本中与少数类样本比较近的样本，然后在这些样本中删除与多数类样本比较远的一些样本。 Edited Nearest Neighbors 欠采样：该方法是基于 KNN 方法，首先在所有的样本中找到少数类样本，然后对每个少数类样本找到 K 个与其最近的多数类样本，如果这 K 个多数类样本中有超过一半的样本判定为异常点，则删除这个少数类样本。 在实际应用中，需要根据具体问题和数据集选择合适的欠采样方法，以避免对模型性能产生负面影响。过采样过采样是一种常用的标签平衡处理方法，它通过增加少数类样本的数量，从而缓解数据集不平衡问题。下面列举一些常用的过采样方法： 随机过采样（Random Over Sampling）：该方法是最简单的过采样方法之一，它从少数类样本中随机选择样本进行复制。这种方法存在一个问题，即容易产生过拟合。 SMOTE（Synthetic Minority Over-sampling Technique）：SMOTE 是一种基于 KNN（K-Nearest Neighbors）的过采样方法，它根据少数类样本之间的距离，产生一些新的少数类样本。SMOTE 方法可以有效地避免过拟合问题。 ADASYN（Adaptive Synthetic Sampling）：ADASYN 是 SMOTE 的一种改进版本，它通过计算每个少数类样本周围的多数类样本比例，决定需要产生的新样本数量。这种方法可以更好地适应不同的数据集。 Borderline-SMOTE：Borderline-SMOTE 是基于 SMOTE 的一种改进版本，它只对那些靠近多数类的少数类样本进行过采样。这种方法可以有效地提高分类器的性能。 Safe-Level-SMOTE：Safe-Level-SMOTE 是基于 SMOTE 的一种改进版本，它不仅考虑了少数类样本之间的距离，还考虑了多数类样本之间的距离。这种方法可以更好地避免产生噪声样本。 ROSE（Random Over-Sampling Examples）：ROSE 是一种基于样本生成的过采样方法，它通过对少数类样本进行样本生成，以增加数据集中的少数类样本数量。 这些过采样方法各有特点，需要根据具体问题和数据集选择合适的方法。同时，在使用过采样方法时，也需要注意过度过采样会产生过拟合问题，需要谨慎使用。" }, { "title": "机器学习基础-监督学习-有监督标签", "url": "/posts/%E6%A0%87%E7%AD%BE-01-%E6%9C%89%E7%9B%91%E7%9D%A3%E6%A0%87%E7%AD%BE/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-05 00:00:00 +0800", "snippet": "有监督标签是指在监督学习中使用的标签，它与训练数据的特征一起被用来训练模型。在有监督学习中，我们通常会提前知道样本的标签，然后使用这些标签来训练模型。例如，在图像分类问题中，我们通常会准备一个包含标签的数据集，其中每个图像都带有一个正确的类别标签。在有监督学习中，通常使用以下三种类型的标签： 二元分类标签：标签只有两种取值，通常表示为 0 或 1，代表某种二元分类问题的分类结果，例如垃圾邮件分类、肿瘤检测等。 多分类标签：标签有多种取值，通常是一组离散值，代表多元分类问题的分类结果，例如手写数字识别、物体识别等。 回归标签：标签是一个连续的数值，通常表示为实数，代表回归问题的预测值，例如房价预测、股票价格预测等。 下面是一个简单的二元分类问题的例子，使用 Python 实现。假设我们有一组数据，每个数据点都有两个特征 x1 和 x2，以及一个二元分类标签 y。我们的目标是训练一个模型来预测新数据点的标签。import numpy as npfrom sklearn.linear_model import LogisticRegression# 生成一组随机数据np.random.seed(0)X = np.random.randn(100, 2)y = (X[:, 0] + X[:, 1] &amp;gt; 0).astype(int)# 训练逻辑回归模型clf = LogisticRegression(random_state=0).fit(X, y)# 预测新数据点的标签X_new = np.array([[0.5, 0.5], [-1, -1]])y_new = clf.predict(X_new)print(&quot;New data point labels:&quot;, y_new)上述代码中，我们使用 numpy 库生成了一组随机数据，其中每个数据点都有两个特征 x1 和 x2，以及一个二元分类标签 y。我们使用 sklearn 库中的 LogisticRegression 模型来训练模型，然后使用训练好的模型来预测新数据点的标签。在这个例子中，我们生成了两个新数据点，预测它们的标签，输出结果为：New data point labels: [1 0]可以看到，模型成功地预测了两个新数据点的标签，一个为 1，一个为 0。这个例子只是一个简单的演示，实际上，在真实的应用场景中，有监督标签的处理方法和技术非常丰富和复杂，需要根据不同的问题和数据集选择合适的有监督标签处理方法和技术。例如，在处理文本分类问题时，通常会使用词袋模型来将文本转换为向量表示，然后使用多分类标签来训练模型。在处理时间序列预测问题时，通常会使用回归标签来预测未来的连续值。在实际应用中，为了获得更好的模型性能，通常需要对标签进行预处理或增强。例如，当标签分布不平衡时，可以使用过采样或欠采样技术来平衡标签分布。当标签噪声较大时，可以使用标签清洗或修复技术来处理标签噪声。当标签缺失时，可以使用半监督或弱监督学习技术来利用未标注的数据来增强标签。下面介绍一些常见的方法和技术： 标签平衡处理：当数据集中不同类别的样本数量不平衡时，可以使用标签平衡处理方法来平衡样本数量。例如，在二元分类问题中，如果正样本数量远多于负样本数量，可以使用欠采样或过采样的方法来平衡数据集。 标签编码：在多分类问题中，通常需要对标签进行编码，例如将每个标签编码成一个独热向量，这样可以使得模型更容易学习到类别之间的关系。 标签噪声处理：在实际应用中，标签可能会受到噪声的影响，例如数据标注错误、标注人员主观判断等。可以使用标签噪声处理方法来减少噪声的影响，例如通过训练多个模型并对它们的预测结果进行聚合来减少标签噪声。 标签增强：在一些情况下，我们可以使用标签增强技术来增加数据集的多样性和覆盖度，从而提高模型的泛化能力。例如，在图像分类问题中，可以通过旋转、翻转、缩放等变换来生成更多的图像样本，从而增加数据集的多样性。 标签转移学习：在一些情况下，我们可以使用标签转移学习来利用已有的标签信息来帮助解决新的问题。例如，在自然语言处理中，可以使用预训练的语言模型来提取文本特征，然后在新的任务中进行微调，从而提高模型的性能。 有监督标签处理方法和技术非常丰富和复杂，需要根据不同的问题和数据集选择合适的方法和技术。在实际应用中，需要根据数据集的特点和问题的需求选择合适的有监督标签处理方法和技术，以提高模型的性能和泛化能力。总之，有监督标签是有监督学习的重要组成部分，对于训练有效的监督学习模型具有至关重要的作用。" }, { "title": "机器学习基础-监督学习-标签", "url": "/posts/%E6%A0%87%E7%AD%BE-00/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-05 00:00:00 +0800", "snippet": "在监督学习中，标签是指样本的正确输出值，也称为目标变量、因变量。在训练模型时，我们通常需要使用训练数据中的标签来调整模型的参数，使其能够预测正确的输出。例如，在一个二元分类问题中，样本的标签可以是 0 或 1。在一个房价预测问题中，样本的标签可以是实际房价。标签通常用符号 $y$ 表示，它的取值范围可以是连续的，也可以是离散的。在机器学习算法中，我们通常需要将标签编码为数值，以便进行计算。下面是一个示例代码，用于生成一个包含标签的数据集：import numpy as np# 生成10个样本，每个样本包含2个特征和一个标签X = np.random.rand(10, 2)y = np.array([0, 1, 1, 0, 1, 0, 0, 1, 1, 0])# 打印生成的数据集print(&quot;特征矩阵 X：&quot;)print(X)print(&quot;标签向量 y：&quot;)print(y)上述代码生成了一个包含 10 个样本的数据集，每个样本包含 2 个特征和一个标签。标签向量 y 包含 10 个元素，表示每个样本的标签。在这个示例中，标签是一个二元值，即 0 或 1。在实际应用中，标签的取值范围和编码方式会因问题而异。例如，在多分类问题中，标签可能是一个从 1 到 K 的整数，其中 K 是类别数。在回归问题中，标签通常是一个实数或向量。除了上述基本概念外，标签还可以分为有监督标签和无监督标签两种类型。有监督标签是指在监督学习中使用的标签，它与训练数据的特征一起被用来训练模型。在有监督学习中，我们通常会提前知道样本的标签，然后使用这些标签来训练模型。例如，在图像分类问题中，我们通常会准备一个包含标签的数据集，其中每个图像都带有一个正确的类别标签。无监督标签是指在无监督学习中使用的标签，它通常不是人工标注的，而是由算法自动学习得到的。在无监督学习中，我们通常只有一些没有标签的数据，但我们仍然希望能够从中发现一些有用的模式和结构。例如，聚类算法可以将一组数据分成几个子集，而这些子集通常可以被视为一些无监督标签。在数学上，标签可以用数学符号表示为 $y$，其具体形式取决于所使用的算法和问题的性质。例如，在线性回归中，标签可以表示为：\\[y = w_1x_1 + w_2x_2 + ... + w_mx_m + b\\]其中 $x_1, x_2, …, x_m$ 是特征向量中的每个特征，$w_1, w_2, …, w_m$ 是对应的权重，$b$ 是偏置项。在分类问题中，标签通常是一个离散值，例如二分类问题中标签可能取 0 或 1，多分类问题中标签可能取 1 到 $K$ 的整数，其中 $K$ 是类别数目。对于二分类问题，标签可以表示为：\\[y \\in \\begin{Bmatrix} 0,1 \\end{Bmatrix}\\]对于多分类问题，标签可以表示为：\\[y \\in \\begin{Bmatrix} 1,2,..,K \\end{Bmatrix}\\]总之，在监督学习中，标签是一种非常重要的概念，它用于指示我们的模型需要预测的正确答案。标签可以是离散的或连续的，可以是人工标注的也可以是由算法自动学习的。在实际应用中，我们需要选择适当的标签类型和编码方式，以便训练出高效、准确的机器学习模型。" }, { "title": "机器学习基础-监督学习-特征向量", "url": "/posts/%E7%89%B9%E5%BE%81%E5%90%91%E9%87%8F/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-04 00:00:00 +0800", "snippet": "特征向量是线性代数中的一个重要概念，常常被应用于数据降维、图像处理、信号处理等领域。在机器学习中，特征向量通常用于表示一个样本的特征。下面进行详细讲解。在线性代数中，一个向量可以被表示为一组有序数值的集合，通常用列向量表示。对于一个矩阵 A，如果存在一个非零向量 v，使得矩阵 A 乘以向量 v 等于一个常数 λ 乘以向量 v，即 Av=λv，那么这个向量 v 就是矩阵 A 的特征向量，λ 就是该特征向量对应的特征值。特征向量的意义在于，它代表着矩阵 A 作用在某个方向上的缩放因子，也可以被理解为矩阵 A 在这个方向上的“影响力”。在机器学习中，特征向量通常用于表示一个样本的特征。例如，对于一个图像样本，我们可以将其转换为一个向量，其中每个元素表示该图像的一个像素点的值。在特征向量的构建过程中，可以采用各种特征提取方法，例如图像中的 SIFT、HOG、LBP 等算法，这些算法可以将原始数据转换为更加抽象、更具代表性的特征向量。下面给出一个简单的 Python 示例代码，展示如何通过 numpy 库来计算矩阵的特征向量和特征值：import numpy as np# 构造一个二维矩阵A = np.array([[1, 2], [2, 1]])# 计算特征向量和特征值eig_vals, eig_vecs = np.linalg.eig(A)print(&quot;特征值为：&quot;)print(eig_vals)print(&quot;特征向量为：&quot;)print(eig_vecs)输出结果如下所示：特征值为：[ 3. -1.]特征向量为：[[ 0.70710678 -0.70710678] [ 0.70710678 0.70710678]]在这个例子中，我们构造了一个二维矩阵 A，通过 numpy 库中的 linalg.eig()函数计算其特征向量和特征值。可以看到，特征向量被表示为一个二维数组，每一列对应一个特征向量，而特征值则以一维数组的形式给出。总的来说，特征向量是机器学习中一个重要的概念，可以用于表示样本的特征，或者用于对数据进行降维处理。了解特征向量的定义和计算方法，可以帮助我们更好地理解各种机器学习算法的原理和应用。在实际应用中，我们通常会使用各种工具库和框架来计算特征向量，例如 numpy、scikit-learn 等库。除了计算特征向量，特征向量在机器学习中还有其他应用，例如降维。在高维数据中，特征向量可以帮助我们找到数据的最重要的方向，从而将数据降维到更低的维度，方便我们进行可视化和处理。同时，在深度学习中，特征向量也扮演着非常重要的角色，例如在卷积神经网络中，每个卷积层的输出都是一个特征图，这些特征图就可以看做是输入图像的特征向量，用于后续的分类、检测等任务。特点特征向量具有以下特点： 特征向量是一个 n 维向量，其中 n 为特征的个数。 特征向量中的每个维度都代表着数据中的一个特征，可以是数值、文本或其他形式的数据。 特征向量中的每个维度都应该被标准化或归一化，以确保每个维度的重要性相等。 特征向量的大小（维度数）可以根据具体问题进行调整，一般情况下需要根据具体问题选择合适的特征维度。 特征向量可以被用于分类、聚类、回归等机器学习任务。 特征向量可以被用于数据的可视化和降维。 特征向量的选择和构建对机器学习算法的性能和准确度有很大影响。总之，特征向量是机器学习中的一个重要概念，它代表着数据的特征，并被广泛应用于各种机器学习任务中。理解特征向量的特点对于机器学习算法的应用和性能优化都非常重要。" }, { "title": "机器学习基础-监督学习-测试数据之AUC（Area Under Curve）", "url": "/posts/%E6%B5%8B%E8%AF%95%E6%95%B0%E6%8D%AE-05-AUC%E5%80%BC/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-03 00:00:00 +0800", "snippet": "AUC（Area Under Curve）值是 ROC 曲线（Receiver Operating Characteristic Curve）下的面积，用于评估二分类模型的性能。AUC 值越大，说明模型的性能越好。当 AUC 值为 1 时，表示模型的性能完美，当 AUC 值为 0.5 时，表示模型的性能等同于随机猜测。以下是一个示例代码，展示如何使用 Python 中的 scikit-learn 库计算 AUC 值：import numpy as npfrom sklearn.metrics import roc_auc_score# 加载数据data = np.loadtxt(&#39;data.csv&#39;, delimiter=&#39;,&#39;)X = data[:, :-1] # 特征矩阵y = data[:, -1] # 标签# 训练模型，假设我们已经定义好了模型model = ...# 预测概率y_prob = model.predict_proba(X)[:, 1]# 计算AUC值auc = roc_auc_score(y, y_prob)print(&#39;AUC值为：&#39;, auc)上述代码中，我们首先加载数据，并训练一个二分类模型。然后，我们使用 predict_proba 方法预测样本属于正例的概率，并将概率值提取出来。最后，使用 roc_auc_score 函数计算 AUC 值。需要注意的是，predict_proba 方法返回的是一个二维数组，第一列为属于负例的概率，第二列为属于正例的概率。因此，在上述代码中，我们使用[:, 1]来提取属于正例的概率。除了使用 Python 中的 scikit-learn 库计算 AUC 值之外，还可以使用手动计算的方式，具体公式为：\\[AUC = \\frac{\\sum_{i=1}^{m-1} (TPR_i+TPR_{i+1}) × (FPR_{i+1}-FPR_i)}{2}\\]其中，$TPR_i$为真正例率（True Positive Rate），也就是$\\frac{TP}{P}$，表示在所有实际为正例中，被模型正确预测为正例的比例；$FPR_i$为假正例率（False Positive Rate），也就是$\\frac{FP}{N}$，表示在所有实际为负例中，被模型错误预测为正例的比例；$m$为 ROC 曲线上的点的个数。虽然手动计算 AUC 值的方法更为繁琐，但是对于理解 AUC 值的计算方式和含义很有帮助。在手动计算 AUC 值时，首先需要计算出模型的 ROC 曲线。ROC 曲线的横轴为 FPR，纵轴为 TPR。我们可以通过在不同的阈值下计算出 TPR 和 FPR 来绘制 ROC 曲线。具体计算方法如下：假设我们有$N$个样本，其中有$P$个正例，$N-P$个负例。对于一个给定的阈值$t$，我们可以根据模型输出的概率将样本分为正例和负例。如果样本的概率大于等于$t$，则将其预测为正例，否则预测为负例。在给定阈值$t$的情况下，我们可以计算出 TP 和 FP 的个数，分别表示在所有实际为正例中，被模型正确预测为正例的个数，以及在所有实际为负例中，被模型错误预测为正例的个数。然后，我们可以计算出 TPR 和 FPR，分别表示真正例率和假正例率：\\[TPR = \\frac{TP}{P}\\]\\[FPR = \\frac{FP}{N-P}\\]随着阈值$t$的变化，我们可以得到一组 TPR 和 FPR，从而绘制出 ROC 曲线。下面是一个示例代码，展示如何手动计算 AUC 值：import numpy as np# 加载数据data = np.loadtxt(&#39;data.csv&#39;, delimiter=&#39;,&#39;)X = data[:, :-1] # 特征矩阵y = data[:, -1] # 标签# 训练模型，假设我们已经定义好了模型model = ...# 预测概率y_prob = model.predict_proba(X)[:, 1]# 计算TPR和FPRthresholds = np.arange(0, 1.01, 0.01) # 阈值的范围tpr_list = [] # 真正例率列表fpr_list = [] # 假正例率列表for t in thresholds: y_pred = y_prob &amp;gt;= t # 预测结果 tp = np.sum((y == 1) &amp;amp; (y_pred == 1)) # 真正例个数 fp = np.sum((y == 0) &amp;amp; (y_pred == 1)) # 假正例个数 tpr = tp / np.sum(y == 1) # 真正例率 fpr = fp / np.sum(y == 0) # 假正例率 tpr_list.append(tpr) fpr_list.append(fpr)# 计算AUC值auc = np.trapz(tpr_list, fpr_list)print(&#39;AUC值为：&#39;, auc)在上述代码中，我们首先加载数据，并训练一个二分类模型。然后，我们使用 predict_proba 方法预测样本属于正例的概率，并根据阈值$t$将样本分类为正例或负例。接下来，我们根据不同的阈值计算 TPR 和 FPR，然后绘制 ROC 曲线。最后，我们使用梯形面积法计算 ROC 曲线下的面积，即 AUC 值。需要注意的是，这里的梯形面积法计算方法是数值积分的一种，可以使用 NumPy 中的 trapz 函数进行计算。该函数的第一个参数为纵坐标列表，第二个参数为横坐标列表。上述代码中，tpr_list 表示真正例率列表，fpr_list 表示假正例率列表。总之，AUC 值是衡量分类模型性能的重要指标之一，它可以告诉我们模型在不同阈值下的性能表现。在实际应用中，我们通常使用 ROC 曲线和 AUC 值来评估模型的性能，并根据需求选择适当的阈值进行分类。" }, { "title": "机器学习基础-监督学习-测试数据之ROC曲线（Receiver Operating Characteristic curve）", "url": "/posts/%E6%B5%8B%E8%AF%95%E6%95%B0%E6%8D%AE-04-ROC%E6%9B%B2%E7%BA%BF/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-03 00:00:00 +0800", "snippet": "ROC 曲线（Receiver Operating Characteristic curve）是用于评估二分类模型性能的一种常用工具。ROC 曲线是一条二维曲线，以真正例率（True Positive Rate，TPR）为纵坐标，以假正例率（False Positive Rate，FPR）为横坐标，描述了模型在不同阈值下的表现。具体来说，TPR 是指在所有实际为正例的样本中，被模型正确预测为正例的比例；而 FPR 是指在所有实际为负例的样本中，被模型错误预测为正例的比例。ROC 曲线是 TPR-FPR 平面上的曲线，曲线越靠近左上角，说明模型的性能越好。以下是一个简单的示例代码，展示如何绘制 ROC 曲线和计算 AUC 值：import numpy as npfrom sklearn.datasets import make_classificationfrom sklearn.linear_model import LogisticRegressionfrom sklearn.metrics import roc_curve, aucimport matplotlib.pyplot as plt# 生成二分类数据X, y = make_classification(n_samples=1000, n_classes=2, random_state=0)# 将数据集分成训练集和测试集，比例为8:2n_train = int(0.8 * X.shape[0])X_train, y_train = X[:n_train], y[:n_train]X_test, y_test = X[n_train:], y[n_train:]# 训练逻辑回归模型model = LogisticRegression()model.fit(X_train, y_train)# 使用测试集评估模型性能y_score = model.decision_function(X_test)fpr, tpr, thresholds = roc_curve(y_test, y_score)# 绘制ROC曲线roc_auc = auc(fpr, tpr)plt.plot(fpr, tpr, color=&#39;darkorange&#39;, lw=2, label=&#39;ROC curve (AUC = %0.2f)&#39; % roc_auc)plt.plot([0, 1], [0, 1], color=&#39;navy&#39;, lw=2, linestyle=&#39;--&#39;)plt.xlim([0.0, 1.0])plt.ylim([0.0, 1.05])plt.xlabel(&#39;False Positive Rate&#39;)plt.ylabel(&#39;True Positive Rate&#39;)plt.title(&#39;Receiver Operating Characteristic&#39;)plt.legend(loc=&quot;lower right&quot;)plt.show()上述代码中，我们首先使用 make_classification 函数生成一个随机的二分类数据集。然后将数据集按照 8:2 的比例分成训练集和测试集，使用 LogisticRegression 类训练一个逻辑回归模型，并使用 decision_function 方法来计算测试集样本的预测分数。接下来使用 roc_curve 函数计算模型在不同阈值下的 TPR 和 FPR，最后使用 auc 函数计算 ROC 曲线下的面积，即 AUC 值。最后使用 matplotlib 库绘制 ROC 曲线。需要注意的是，计算 ROC 曲线和 AUC 值时，需要使用模型在测试集上的预测分数，因为模型在不同阈值下的表现不同，所以 ROC 曲线展现了模型在不同阈值下的表现，是评估模型性能的一种常用方法。如果模型的 ROC 曲线在左上角，说明模型具有很好的性能；如果 ROC 曲线与对角线重合，则说明模型的性能等同于随机猜测。在实际应用中，我们可以根据 ROC 曲线的形状和 AUC 值来选择最优的模型。如果我们有多个二分类模型可供选择，我们可以使用交叉验证等方法来计算每个模型的 ROC 曲线和 AUC 值，然后选择 AUC 值最大的模型。除了二分类问题，ROC 曲线也可以用于多分类问题。在这种情况下，我们可以使用 One-vs-Rest 或 One-vs-One 等方法将多分类问题转化为多个二分类问题，然后计算每个二分类问题的 ROC 曲线和 AUC 值。总之，ROC 曲线是评估二分类模型性能的一种重要工具，可以帮助我们选择最优的模型。同时，它也是一个非常直观的工具，因为它展现了模型在不同阈值下的表现。" }, { "title": "机器学习基础-监督学习-测试数据之F1值", "url": "/posts/%E6%B5%8B%E8%AF%95%E6%95%B0%E6%8D%AE-03-F1%E5%80%BC/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-03 00:00:00 +0800", "snippet": "F1 值是衡量二分类模型性能的一种指标，它是精确率和召回率的调和平均数，可以综合考虑模型的准确性和召回率，对于不平衡数据集的二分类任务比准确率更加合适。F1 值的计算公式如下：\\[F1 = \\frac{2*Precision*Recall}{Precision+Recall}\\]其中，Precision 为精确率，Recall 为召回率。精确率表示预测为正例的样本中，真正为正例的比例；召回率表示真正为正例的样本中，被正确预测为正例的比例。以下是一个示例代码，展示如何计算二分类模型在测试集上的 F1 值：import numpy as npfrom sklearn.metrics import f1_scorefrom sklearn.linear_model import LogisticRegressionfrom sklearn.model_selection import train_test_split# 加载数据data = np.loadtxt(&#39;data.csv&#39;, delimiter=&#39;,&#39;)X = data[:, :-1] # 特征矩阵y = data[:, -1] # 标签# 将数据集分成训练集和测试集，比例为8:2X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)# 训练模型model = LogisticRegression()model.fit(X_train, y_train)# 使用测试集评估模型性能y_pred = model.predict(X_test)f1 = f1_score(y_test, y_pred)print(&#39;模型在测试集上的F1值为：&#39;, f1)上述代码中，我们使用 f1_score 函数计算模型在测试集上的 F1 值。该函数需要传入测试集真实标签 y_test 和模型预测标签 y_pred 两个参数。f1_score 函数的实现方式如下：def f1_score(y_true, y_pred): precision = precision_score(y_true, y_pred) recall = recall_score(y_true, y_pred) return 2 * (precision * recall) / (precision + recall)在该实现方式中，我们先计算出精确率和召回率，然后根据上述公式计算 F1 值。F1 值的取值范围是 0 到 1 之间，越接近 1 表示模型的性能越好，越接近 0 表示模型的性能越差。当精确率和召回率同时很高时，F1 值也会很高，因此，F1 值比单独使用精确率或召回率更加全面和客观。需要注意的是，F1 值对于数据集类别分布不平衡的情况下，可能会失去一定的准确性。在极端情况下，比如正例只有极少的情况，模型始终预测为负例时，精确率和召回率都为 0，因此 F1 值也为 0，这时候 F1 值就不再是一个可靠的指标。此时，需要考虑使用其他指标来评估模型的性能，比如受试者工作特征曲线（ROC 曲线）和 AUC（曲线下面积）。总之，F1 值是衡量二分类模型性能的一种常用指标，它综合考虑了精确率和召回率，对于不平衡数据集的二分类任务比准确率更加合适。在实际应用中，需要根据具体的情况选择合适的指标来评估模型的性能。" }, { "title": "机器学习基础-监督学习-测试数据之召回率（Recall）", "url": "/posts/%E6%B5%8B%E8%AF%95%E6%95%B0%E6%8D%AE-02-%E5%8F%AC%E5%9B%9E%E7%8E%87/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-03 00:00:00 +0800", "snippet": "在机器学习中，召回率（Recall）是衡量分类模型性能的一个重要指标之一。召回率是指所有正样本中被分类器正确识别的比例，即：\\[Recall = \\frac{TP}{TP + FN}\\]其中，TP（True Positive）表示被分类器正确地识别为正样本的样本数，FN（False Negative）表示被分类器错误地识别为负样本的正样本数。以下是一个简单的示例代码，展示如何计算模型的召回率：import numpy as npfrom sklearn.metrics import recall_scorefrom sklearn.linear_model import LogisticRegression# 加载数据data = np.loadtxt(&#39;data.csv&#39;, delimiter=&#39;,&#39;)X = data[:, :-1] # 特征矩阵y = data[:, -1] # 标签# 训练模型model = LogisticRegression()model.fit(X, y)# 在测试集上进行预测y_pred = model.predict(X_test)# 计算召回率recall = recall_score(y_test, y_pred)print(&#39;模型的召回率为：&#39;, recall)上述代码中，我们使用 recall_score 函数来计算模型在测试集上的召回率。该函数需要输入两个参数：真实标签（y_test）和预测标签（y_pred）。特点 召回率衡量的是所有正样本中被分类器正确识别的比例，因此它对于正样本的识别能力非常关键。在某些应用场景下，如医疗诊断、欺诈检测等，召回率的重要性非常高。 召回率与准确率（Precision）一样，都可以用来评估模型性能，但是它们的侧重点不同。准确率关注的是所有预测为正样本的样本中真正为正样本的比例，而召回率关注的是所有真正为正样本的样本中被正确预测为正样本的比例。 召回率越高，说明模型对于正样本的识别能力越强。但是，召回率的提高通常会伴随着误判率（False Positive Rate）的增加。因此，在实际应用中，需要根据具体场景综合考虑多个指标，选择合适的模型。 召回率可以通过不同的方法来改进，如增加训练数据、调整分类器的阈值、使用更复杂的模型等。但是，这些改进方法可能会带来其他问题，如过拟合、运算效率低下等。因此，需要在实际应用中进行综合权衡。 召回率是一个重要的指标，因为它能够反映模型对于正样本的识别能力。在某些应用场景下，如医疗诊断、欺诈检测等，对于正样本的识别能力非常关键。如果模型的召回率过低，可能会导致漏诊、漏检等严重后果。因此，在模型评估和选择时，需要综合考虑多个指标，并根据具体场景选择合适的模型。" }, { "title": "机器学习基础-监督学习-测试数据之准确率", "url": "/posts/%E6%B5%8B%E8%AF%95%E6%95%B0%E6%8D%AE-01-%E5%87%86%E7%A1%AE%E7%8E%87/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-03 00:00:00 +0800", "snippet": "准确率是用于衡量分类模型性能的指标之一，表示模型在所有分类样本中正确分类的比例。准确率越高，模型的性能越好。准确率的计算公式如下：\\[Accuracy = \\frac{TP+TN}{TP+TN+FP+FN}\\]其中，$TP$表示真正例（True Positive），$TN$表示真反例（True Negative），$FP$表示假正例（False Positive），$FN$表示假反例（False Negative）。以下是一个简单的示例代码，展示如何使用准确率来评估模型的性能：from sklearn.metrics import accuracy_scorefrom sklearn.datasets import load_irisfrom sklearn.model_selection import train_test_splitfrom sklearn.tree import DecisionTreeClassifier# 加载数据集iris = load_iris()X = iris.datay = iris.target# 将数据集分成训练集和测试集，比例为8:2X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)# 训练决策树模型model = DecisionTreeClassifier()model.fit(X_train, y_train)# 预测测试集的标签y_pred = model.predict(X_test)# 计算准确率accuracy = accuracy_score(y_test, y_pred)print(&#39;模型在测试集上的准确率为：&#39;, accuracy)上述代码中，我们使用 accuracy_score 函数来计算模型在测试集上的准确率。首先，我们加载了鸢尾花数据集，将数据集按照 8:2 的比例分成训练集和测试集。然后，我们使用 DecisionTreeClassifier 类训练一个决策树模型，并使用 predict 方法来预测测试集的标签。最后，使用 accuracy_score 函数计算模型在测试集上的准确率。特点 简单直观：准确率的计算公式简单，易于理解和解释。 容易受到样本分布的影响：当样本不平衡时，准确率可能会给出误导性的结果。例如，在一个二分类问题中，如果负样本占总样本数的绝大部分，模型将倾向于将所有的样本都预测为负样本，此时准确率可能会非常高，但模型并没有实际的预测能力。 不考虑误分类的类型和严重程度：准确率只关注模型预测的结果是否正确，而不关心模型预测错误的类型和严重程度。因此，当误分类的类型和严重程度对应用场景有较高的影响时，准确率可能不是最好的评价指标。 可以用于比较不同模型的性能：准确率可以用于比较不同模型在同一数据集上的性能，从而帮助选择最佳的模型。 仅适用于分类问题：准确率只适用于分类问题，不能用于回归问题。 对数据集的规模不敏感：准确率对数据集的规模不敏感，因此可以用于比较在不同规模的数据集上训练的模型的性能。 需要注意的是，准确率并不是适用于所有的分类问题，特别是在类别不平衡的情况下，准确率可能会存在误导性。在这种情况下，我们需要使用其他的指标来评估模型的性能。" }, { "title": "机器学习基础-监督学习-测试数据", "url": "/posts/%E6%B5%8B%E8%AF%95%E6%95%B0%E6%8D%AE-00/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-03 00:00:00 +0800", "snippet": "在机器学习中，测试数据是用于评估模型性能的数据集。与训练数据不同，测试数据不用于训练模型，而是用于测试模型的泛化能力。测试数据通常是从总体数据集中单独抽取出来的，确保测试数据和训练数据互相独立，从而可以更好地评估模型的性能。以下是一个简单的示例代码，展示如何将数据集分成训练集和测试集，以及如何使用测试集来评估模型的性能：import numpy as npfrom sklearn.model_selection import train_test_splitfrom sklearn.linear_model import LogisticRegression# 加载数据data = np.loadtxt(&#39;data.csv&#39;, delimiter=&#39;,&#39;)X = data[:, :-1] # 特征矩阵y = data[:, -1] # 标签# 将数据集分成训练集和测试集，比例为8:2X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)# 训练模型model = LogisticRegression()model.fit(X_train, y_train)# 使用测试集评估模型性能score = model.score(X_test, y_test)print(&#39;模型在测试集上的准确率为：&#39;, score)上述代码中，我们使用 train_test_split 函数将数据集按照 8:2 的比例分成训练集和测试集。然后使用 LogisticRegression 类训练一个逻辑回归模型，并使用 score 方法来计算模型在测试集上的准确率。在机器学习中，通常使用准确率、召回率、F1 值、ROC 曲线、AUC 值等指标来评估模型的性能。具体的计算公式可以参考相关的资料和文献。" }, { "title": "机器学习基础-监督学习-训练数据-数据转换和标准化", "url": "/posts/%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE-10-%E6%95%B0%E6%8D%AE%E8%BD%AC%E6%8D%A2%E5%92%8C%E6%A0%87%E5%87%86%E5%8C%96/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-02 00:00:00 +0800", "snippet": "数据转换和标准化是监督学习中常用的数据处理方法之一。在这里，我们将详细介绍两种常用的数据标准化方法：Z-score 标准化和最大最小值标准化。Z-score 标准化Z-score 标准化（也称标准差标准化）是一种常见的数据标准化方法，它可以将数据转换为均值为 0、标准差为 1 的分布。该方法的公式如下：\\[x_{\\mathrm{std}} = \\frac{x-\\mu}{\\sigma}\\]其中，$x_{\\mathrm{std}}$ 表示标准化后的数据，$x$ 表示原始数据，$\\mu$ 表示原始数据的均值，$\\sigma$ 表示原始数据的标准差。下面是使用 Python 对数据进行 Z-score 标准化的示例代码：import numpy as np# 生成随机数据data = np.random.rand(100, 5)# 计算数据的均值和标准差mean = np.mean(data, axis=0)std = np.std(data, axis=0)# 对数据进行 Z-score 标准化data_std = (data - mean) / std# 输出标准化后的数据的均值和标准差print(&quot;Mean of standardized data:&quot;, np.mean(data_std, axis=0))print(&quot;Std of standardized data:&quot;, np.std(data_std, axis=0))在这个示例中，我们首先生成了一个 100 行 5 列的随机数据矩阵，然后使用 numpy 库计算了数据的均值和标准差，最后对数据进行了 Z-score 标准化，并输出了标准化后的数据的均值和标准差。Z-score 标准化适用于数据的分布比较稳定的情况，可以去除数据中的量纲差异，使得不同特征之间的数据可以进行比较和分析。但是，如果数据的分布比较离散或存在极端值，Z-score 标准化可能会失效。此时可以考虑使用其他的标准化方法。最大最小值标准化最大最小值标准化是一种常见的数据标准化方法，它可以将数据转换到 0 到 1 的范围内。该方法的公式如下：\\[x_{\\mathrm{norm}} = \\frac{x-x_{\\min}}{x_{\\max}-x_{\\min}}\\]其中，$x_{\\mathrm{norm}}$ 表示标准化后的数据，$x$ 表示原始数据，$x_{\\mathrm{min}}$ 表示原始数据的最小值，$x_{\\mathrm{max}}$ 表示原始数据的最大值。最大最小值标准化可以使数据归一化，便于不同特征或不同数据集之间进行比较和分析。例如，当两个特征的取值范围不同时，我们可以使用最大最小值标准化将它们放在相同的比例尺下。下面是使用 Python 对数据进行最大最小值标准化的示例代码：import numpy as np# 生成随机数据data = np.random.rand(100, 5)# 计算数据的最大值和最小值min_val = np.min(data, axis=0)max_val = np.max(data, axis=0)# 对数据进行最大最小值标准化data_norm = (data - min_val) / (max_val - min_val)# 输出标准化后的数据的最小值和最大值print(&quot;Min of normalized data:&quot;, np.min(data_norm, axis=0))print(&quot;Max of normalized data:&quot;, np.max(data_norm, axis=0))在这个示例中，我们首先生成了一个 100 行 5 列的随机数据矩阵，然后使用 numpy 库计算了数据的最大值和最小值，最后对数据进行了最大最小值标准化，并输出了标准化后的数据的最小值和最大值。" }, { "title": "机器学习基础-监督学习-训练数据-特征选择之Embedded方法", "url": "/posts/%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE-09-%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B9%8BEmbedded%E6%96%B9%E6%B3%95/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-02 00:00:00 +0800", "snippet": "Embedded 方法是一种特征选择技术，它将特征选择嵌入到模型训练过程中，通过正则化方法来减少不重要的特征对模型的影响。常用的 Embedded 方法包括 Lasso、Ridge、Elastic Net 等。Lasso（Least Absolute Shrinkage and Selection Operator）Lasso（Least Absolute Shrinkage and Selection Operator）是一种基于 L1 正则化的线性回归模型，它在最小化均方误差的同时，还会使得一些特征的系数变成 0，从而达到特征选择的效果。具体来说，Lasso 模型的目标函数为：\\[\\textstyle\\min_w \\frac{1}{2n} ||Xw-y||_2^2 + \\alpha ||w||_1\\] 其中 $X$ 是输入特征矩阵，$y$ 是输出变量，$w$ 是模型的系数，$n$ 是样本数量，$\\alpha$ 是 L1 正则化项的系数。L1 正则化项为 $   w   1 = \\sum{i=1}^p w_i $，它是系数的绝对值之和。 Lasso 的目标函数中的 $\\alpha$ 控制着正则化的强度，较大的 $\\alpha$ 会使得一些特征的系数变成 0，从而达到特征选择的效果。当 $\\alpha$ 等于 0 时，Lasso 模型就变成了普通的线性回归模型。为了解决 Lasso 模型在系数取值处不可导的问题，可以使用坐标下降算法来求解。坐标下降算法是一种迭代算法，它每次只更新一个系数的值，而将其他系数的值保持不变。具体来说，坐标下降算法的迭代公式为：\\[w_i^{(t+1)} = S(\\frac{1}{n} \\textstyle\\sum_{j=1}^{n} x_{ij}(y_i- \\sum_{k\\ne i} x_{jk} w_k^{(t)}), \\alpha)\\]其中 $S$ 是软阈值函数，它的定义为：\\[S(z,\\alpha) = sign(z)(||z||-\\alpha)_+\\]其中 $sign(z)$ 表示 $z$ 的符号，$(x)_+$ 表示取 $x$ 和 0 中的较大值。下面是一个使用 Lasso 对 Boston Housing 数据集进行特征选择的 Python 示例代码：from sklearn.datasets import load_bostonfrom sklearn.linear_model import Lasso# 加载数据集boston = load_boston()X, y = boston.data, boston.target# 使用 Lasso 进行特征选择lasso = Lasso(alpha=0.1)lasso.fit(X, y)# 输出特征系数print(&quot;特征系数：&quot;, lasso.coef_)在这个示例中，我们使用 Lasso 方法对 Boston Housing 数据集进行特征选择，并输出了每个特征的系数。可以发现，Lasso 对一些不重要的特征的系数变成了 0，从而达到了特征选择的效果。RidgeRidge 回归是一种用于线性回归的正则化方法，它通过增加 L2 正则化项来控制模型的复杂度，防止过拟合。在 Ridge 回归中，模型的目标函数为：\\[\\textstyle \\min_w \\frac{1}{2n} ||Xw-y||_2^2 + \\alpha||w||_2^2\\]其中，$X$ 是输入特征矩阵，$y$ 是输出变量，$w$ 是模型的系数，$n$ 是样本数量，$\\alpha$ 是正则化系数。与 Lasso 回归不同的是，Ridge 回归采用 L2 正则化，所以目标函数中的正则化项为\\[||w||_2^2\\]，即系数向量的 L2 范数的平方。我们可以通过最小化 Ridge 回归的目标函数来求解模型的系数向量 $w$。其中，$\\alpha$ 是一个超参数，用于控制正则化的强度。当 $\\alpha$ 越大时，正则化的效果就越强，系数向量 $w$ 的范数就越小，从而减少了过拟合的风险。下面是一个使用 Ridge 回归进行预测的 Python 示例代码：from sklearn.linear_model import Ridgefrom sklearn.datasets import load_bostonfrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import mean_squared_error# 加载数据集boston = load_boston()X, y = boston.data, boston.target# 划分训练集和测试集X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)# 训练 Ridge 回归模型ridge = Ridge(alpha=1.0)ridge.fit(X_train, y_train)# 对测试集进行预测y_pred = ridge.predict(X_test)# 计算均方误差mse = mean_squared_error(y_test, y_pred)print(&quot;均方误差：&quot;, mse)在这个示例中，我们使用 Boston Housing 数据集进行 Ridge 回归的训练和预测。需要注意的是，Ridge 回归需要设置一个合适的 $\\alpha$ 值，以便控制正则化的强度。通常情况下，我们可以通过交叉验证等方法来选择最优的 $\\alpha$ 值。总之，Ridge 回归是一种用于线性回归的正则化方法，它通过增加 L2 正则化项来控制模型的复杂度，防止过拟合。在实际应用中，我们需要根据具体的问题选择合适的正则化强度和模型超参数，以获得更好的模型性能。Elastic NetElastic Net 是一种结合了 Lasso 和 Ridge 正则化的线性回归模型，它既能够选择重要的特征，又能够保留一些相关性较强的特征，从而达到更好的特征选择效果。Elastic Net 的目标函数可以表示为：\\[\\textstyle \\min_w \\frac{1}{2n} ||Xw-y||_2^2 + \\alpha\\rho||w||_1 + \\frac{\\alpha(1-\\rho)}{2} ||w||_2^2\\]其中，$X$ 是输入特征矩阵，$y$ 是输出变量，$w$ 是模型的系数，$n$ 是样本数量，$\\alpha$ 是正则化系数，$\\rho$ 是 L1 正则化和 L2 正则化的权重，用于平衡两种正则化的影响。当 $\\rho = 1$ 时，Elastic Net 退化为 Lasso；当 $\\rho = 0$ 时，Elastic Net 退化为 Ridge。下面是一个使用 Elastic Net 进行特征选择的 Python 示例代码：from sklearn.datasets import load_bostonfrom sklearn.linear_model import ElasticNet# 加载数据集boston = load_boston()X, y = boston.data, boston.target# 使用 Elastic Net 进行特征选择enet = ElasticNet(alpha=0.1, l1_ratio=0.5)enet.fit(X, y)selected_features = enet.coef_ != 0# 输出结果print(&quot;原始数据特征数：&quot;, X.shape[1])print(&quot;特征选择后数据特征数：&quot;, selected_features.sum())在这个示例中，我们使用 Elastic Net 方法对 Boston Housing 数据集进行特征选择，并选取了非零系数的特征。需要注意的是，在进行特征选择时，我们需要对选择的特征进行评估和验证，以确保选择出来的特征对模型的性能有实际的贡献。Elastic Net 可以应用于许多机器学习问题中，包括回归、分类、聚类等。它具有较好的泛化能力和鲁棒性，可以有效地处理高维度的数据集，是一种常用的特征选择方法。" }, { "title": "机器学习基础-监督学习-训练数据-特征选择之Wrapper方法", "url": "/posts/%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE-08-%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B9%8BWrapper%E6%96%B9%E6%B3%95/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-02 00:00:00 +0800", "snippet": "Wrapper 方法是一种特征选择方法，它将特征选择问题转化为子集选择问题，使用模型的性能作为评估指标，选择最优的特征子集。Wrapper 方法相对于 Filter 方法来说更加精确，但是计算量更大，需要在模型上反复训练，因此在数据量较大时可能会面临计算效率的问题。Wrapper 方法的核心思想是通过特征子集来训练模型，评估特征子集的好坏，从而选择最优的特征子集。常用的 Wrapper 方法包括递归特征消除（Recursive Feature Elimination，简称 RFE）、前向搜索（Forward Selection）和后向搜索（Backward Elimination）等。递归特征消除（Recursive Feature Elimination，简称 RFE）递归特征消除（Recursive Feature Elimination，简称 RFE）是一种 Wrapper 方法，它通过不断迭代，反复训练模型，删除不重要的特征，直到达到指定的特征数或者性能最优的特征子集为止。RFE 方法的核心思想是通过模型的性能评估来选择最优的特征子集。RFE 方法的具体实现过程如下： 训练模型，得到模型的权重系数或者特征重要性； 去除权重系数或者特征重要性最小的特征； 重新训练模型，直到达到指定的特征数或者性能最优的特征子集为止。RFE 方法的优点是它可以考虑特征之间的相互作用和组合，缺点是它需要反复训练模型，计算量较大，对于大规模数据集和特征数较多的情况，可能会面临效率问题。下面是一个使用递归特征消除（RFE）方法进行特征选择的 Python 示例代码：from sklearn.datasets import load_breast_cancerfrom sklearn.feature_selection import RFEfrom sklearn.linear_model import LogisticRegression# 加载数据集cancer = load_breast_cancer()X, y = cancer.data, cancer.target# 使用递归特征消除方法进行特征选择estimator = LogisticRegression()selector = RFE(estimator, n_features_to_select=10, step=1)selector.fit(X, y)# 输出结果print(&quot;原始数据特征数：&quot;, X.shape[1])print(&quot;特征选择后数据特征数：&quot;, selector.n_features_)在这个示例中，我们使用递归特征消除方法对乳腺癌数据集进行特征选择，并选择了 10 个最优特征。需要注意的是，递归特征消除方法需要指定一个估计器（estimator），在本例中我们使用了逻辑回归模型作为估计器，同时还需要指定每次迭代中删除的特征数（step），这个数值越小，需要迭代的次数就越多。总之，递归特征消除方法可以选择最优的特征子集，提高模型的准确性和泛化能力，但计算量较大，需要对特征子集进行多次训练，因此需要根据数据集的大小和特征数量来选择合适的方法。前向搜索（Forward Selection）前向搜索是一种 Wrapper 方法，用于特征选择。它的核心思想是从空特征集开始，每次向其中添加一个特征，直到达到指定数量的特征或者无法继续提升模型的性能为止。在每一次添加特征后，需要重新训练模型，并使用交叉验证等技术评估模型性能。前向搜索算法的伪代码如下： 初始化当前特征集为空 初始化最优特征集为空 初始化最优性能指标为负无穷 迭代以下步骤，直到满足停止条件： 对于每个未被选择的特征： 将当前特征集和该特征组合成一个新的特征集 使用新的特征集训练模型 使用交叉验证等技术评估模型性能 如果模型性能优于当前最优性能指标，则更新最优特征集和最优性能指标 如果无法找到新的特征，则终止算法 下面是一个使用前向搜索方法进行特征选择的 Python 示例代码：from sklearn.datasets import load_breast_cancerfrom sklearn.feature_selection import RFECVfrom sklearn.model_selection import StratifiedKFoldfrom sklearn.linear_model import LogisticRegression# 加载数据集cancer = load_breast_cancer()X, y = cancer.data, cancer.target# 使用前向搜索方法进行特征选择estimator = LogisticRegression()selector = RFECV(estimator, step=1, cv=StratifiedKFold(5), scoring=&#39;accuracy&#39;)selector.fit(X, y)# 输出结果print(&quot;原始数据特征数：&quot;, X.shape[1])print(&quot;特征选择后数据特征数：&quot;, selector.n_features_)在这个示例中，我们使用前向搜索方法对乳腺癌数据集进行特征选择。需要注意的是，我们使用了 RFECV 类，它是一个包装器类，可以自动选择最优特征数，同时还可以进行交叉验证和性能评估等操作。在初始化 RFECV 类时，我们需要指定估计器（estimator）、每次迭代中删除的特征数（step）、交叉验证分割器（cv）和性能评估指标（scoring）等参数。总之，前向搜索方法可以选择最优的特征子集，提高模型的准确性和泛化能力，但计算量较大，需要对特征子集进行多次训练，因此需要根据数据集的大小和特征数量来选择合适的方法。后向搜索（Backward Elimination）后向搜索（Backward Elimination）是一种 Wrapper 方法，其思想是从初始的特征集合中逐步删除特征，直到达到一个最优的特征子集。在每一次迭代中，后向搜索方法删除一个特征，并使用剩余的特征来重新训练模型，最终选择性能最优的特征子集。下面是一个使用后向搜索方法进行特征选择的 Python 示例代码：from sklearn.datasets import load_breast_cancerfrom sklearn.linear_model import LogisticRegressionfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS# 加载数据集cancer = load_breast_cancer()X, y = cancer.data, cancer.target# 使用后向搜索方法进行特征选择estimator = LogisticRegression()selector = SFS(estimator, k_features=10, forward=False, floating=False, scoring=&#39;accuracy&#39;, cv=5)selector.fit(X, y)# 输出结果print(&quot;原始数据特征数：&quot;, X.shape[1])print(&quot;特征选择后数据特征数：&quot;, selector.k_features_)在这个示例中，我们使用后向搜索方法对乳腺癌数据集进行特征选择，并选择了 10 个最优特征。需要注意的是，mlxtend 库提供了一个方便的工具 SequentialFeatureSelector，它可以帮助我们在训练模型的过程中自动进行特征选择，并返回最终的特征子集。总之，后向搜索方法可以选择最优的特征子集，提高模型的准确性和泛化能力，但计算量较大，需要对特征子集进行多次训练，因此需要根据数据集的大小和特征数量来选择合适的方法。" }, { "title": "机器学习基础-监督学习-训练数据-特征选择之Filter方法", "url": "/posts/%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE-07-%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B9%8BFilter%E6%96%B9%E6%B3%95/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-02 00:00:00 +0800", "snippet": "Filter 方法是一种特征选择技术，通过对特征进行评估和排名，选择排名靠前的特征用于训练模型。Filter 方法的主要优点是计算速度快，不需要训练模型即可完成特征选择。下面是一些常用的 Filter 方法： 方差选择法（VarianceThreshold）：删除方差较小的特征。 相关系数法：计算每个特征与目标变量之间的相关系数，选择相关系数较高的特征。 卡方检验法：计算每个特征与目标变量之间的卡方值，选择卡方值较高的特征。 互信息法：计算每个特征与目标变量之间的互信息，选择互信息较高的特征。方差选择法（VarianceThreshold）方差选择法是一种特征选择方法，可以通过删除方差较小的特征来减少数据的维度。具体来说，我们可以计算每个特征的方差，然后选择方差大于阈值的特征作为选择的特征。阈值可以通过手动指定，也可以通过 Grid Search 等方法选择。方差选择法的数学原理比较简单，它假设特征的方差越大，特征对目标变量的影响越大。因此，如果一个特征的方差较小，那么它很可能不重要，可以被删除。具体来说，方差选择法的公式如下：\\[Var(X) = \\frac{\\sum_{i=1}^{n} (X_i-\\bar{X})^2}{n-1}\\]其中，$X_i$ 表示第 $i$ 个样本的特征值，$\\bar{X}$ 表示所有样本的特征均值，$n$ 表示样本数量。在 sklearn 中，方差选择法可以使用 VarianceThreshold 类来实现。下面是一个使用方差选择法进行特征选择的 Python 示例代码：from sklearn.datasets import load_irisfrom sklearn.feature_selection import VarianceThreshold# 加载数据集iris = load_iris()X, y = iris.data, iris.target# 使用方差选择法进行特征选择selector = VarianceThreshold(threshold=0.2)selector.fit_transform(X)# 输出结果print(&quot;原始数据特征数：&quot;, X.shape[1])print(&quot;特征选择后数据特征数：&quot;, selector.transform(X).shape[1])在这个示例中，我们使用 VarianceThreshold 方法选择了方差大于 0.2 的特征，输出了特征选择前后的特征数。需要注意的是，我们需要根据数据集的特点和需求来选择合适的特征选择方法和参数，以达到最好的性能。相关系数法相关系数法是一种基于特征和目标变量之间的线性关系进行特征选择的方法。具体来说，它通过计算每个特征与目标变量之间的相关系数，选择相关系数较高的特征作为选择的特征。常见的相关系数有 Pearson 相关系数、Spearman 相关系数和 Kendall Tau 相关系数。下面以 Pearson 相关系数为例，讲解相关系数法的具体实现和使用。Pearson 相关系数是一种衡量两个变量之间线性相关性的度量方法，其值介于 -1 和 1 之间。具体来说，如果 Pearson 相关系数为 1，则表示两个变量完全正相关；如果 Pearson 相关系数为 -1，则表示两个变量完全负相关；如果 Pearson 相关系数为 0，则表示两个变量没有线性相关性。下面是一个使用 Pearson 相关系数进行特征选择的 Python 示例代码：from sklearn.datasets import load_bostonfrom sklearn.feature_selection import SelectKBestfrom scipy.stats import pearsonr# 加载数据集boston = load_boston()X, y = boston.data, boston.target# 使用 Pearson 相关系数进行特征选择def pearson_selector(X, y, k): scores = [] for i in range(X.shape[1]): score, _ = pearsonr(X[:, i], y) scores.append(score) scores = abs(scores) top_k_idx = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:k] return top_k_idxselector = SelectKBest(pearson_selector, k=5)selector.fit_transform(X, y)# 输出结果print(&quot;原始数据特征数：&quot;, X.shape[1])print(&quot;特征选择后数据特征数：&quot;, selector.transform(X).shape[1])print(&quot;特征选择后的特征索引：&quot;, selector.get_support(indices=True))在这个示例中，我们使用 SelectKBest 方法选择了与目标变量的 Pearson 相关系数绝对值最大的 5 个特征，输出了特征选择前后的特征数和选择的特征索引。需要注意的是，我们可以根据需求和数据集的特点选择合适的相关系数和特征选择方法，以达到最好的性能。总之，相关系数法是一种简单有效的特征选择技术，可以通过计算特征和目标变量之间的相关性选择重要的特征，有助于提高模型的准确性和泛化能力。卡方检验法卡方检验法是一种常用的特征选择方法，它可以帮助我们选择与目标变量显著相关的特征。具体来说，卡方检验法会计算每个特征与目标变量之间的卡方值，选择卡方值较高的特征作为选择的特征。卡方检验法适用于分类问题，而且要求特征是离散的。卡方检验法的主要思想是比较实际观察到的频数与期望频数之间的差异。期望频数是在特征和目标变量之间不存在关联的情况下，我们预计在每个特征值上观察到的频数。计算卡方值的公式如下：\\[\\chi^2 = \\sum \\frac{(O_i-E_i)^2}{E_i}\\]其中，$O_i$ 表示实际观察到的频数，$E_i$ 表示期望频数。我们可以通过卡方值来判断特征与目标变量之间的关联程度，卡方值越大表示特征与目标变量之间的关联程度越高。下面是一个使用卡方检验法进行特征选择的 Python 示例代码：from sklearn.datasets import load_irisfrom sklearn.feature_selection import SelectKBestfrom sklearn.feature_selection import chi2# 加载数据集iris = load_iris()X, y = iris.data, iris.target# 使用卡方检验法进行特征选择selector = SelectKBest(chi2, k=2)selector.fit_transform(X, y)# 输出结果print(&quot;原始数据特征数：&quot;, X.shape[1])print(&quot;特征选择后数据特征数：&quot;, selector.transform(X).shape[1])在这个示例中，我们使用 SelectKBest 方法选择卡方值最高的两个特征，输出了特征选择前后的特征数。需要注意的是，我们需要根据数据集的特点和需求来选择合适的特征选择方法和参数，以达到最好的性能。总之，卡方检验法是一种常用的特征选择方法，可以帮助我们选择与目标变量显著相关的特征，有助于提高模型的准确性和泛化能力。互信息法互信息法是一种常用的特征选择方法，可以通过计算特征与目标变量之间的互信息来评估特征的重要性，选择互信息较高的特征用于训练模型。本文将详细介绍互信息法的原理和实现方法，以及如何使用 Python 实现互信息法进行特征选择。互信息（Mutual Information）是一种衡量两个变量之间相关性的方法，它可以用于衡量特征与目标变量之间的相关性。互信息的定义如下：\\[I(X_iY) = \\sum_{x\\in X} \\sum_{y\\in Y} p(x,y) \\log\\frac{p(x,y)}{p(x)p(y)}\\]其中 $X$ 和 $Y$ 分别表示两个随机变量，$p(x, y)$ 是 $X$ 和 $Y$ 的联合概率分布，$p(x)$ 和 $p(y)$ 分别是 $X$ 和 $Y$ 的边缘概率分布。互信息的值越大，表示 $X$ 和 $Y$ 之间的相关性越强。在特征选择中，我们可以计算每个特征与目标变量之间的互信息，然后选择互信息较高的特征。互信息法的实现方法可以使用 Scikit-Learn 中的 mutual_info_classif 或 mutual_info_regression 函数，分别适用于分类和回归问题。下面是一个使用互信息法进行特征选择的 Python 示例代码：from sklearn.datasets import load_irisfrom sklearn.feature_selection import SelectKBest, mutual_info_classif# 加载数据集iris = load_iris()X, y = iris.data, iris.target# 使用互信息法进行特征选择selector = SelectKBest(mutual_info_classif, k=2)X_new = selector.fit_transform(X, y)# 输出结果print(&quot;原始数据特征数：&quot;, X.shape[1])print(&quot;特征选择后数据特征数：&quot;, X_new.shape[1])print(&quot;所选特征的索引：&quot;, selector.get_support(indices=True))在这个示例中，我们使用了 Scikit-Learn 中的 SelectKBest 类来进行特征选择，指定了使用 mutual_info_classif 函数计算互信息，并选择了 k=2 个特征。最后输出了特征选择前后的特征数和所选特征的索引。需要注意的是，互信息法需要对特征和目标变量进行离散化，否则计算的互信息可能不准确。可以使用 Scikit-Learn 中的 KBinsDiscretizer 类对数据进行离散化。总之，互信息法是一种常用的特征选择方法，可以通过计算特征与目标变量之间的互信息来评估特征的重要性，有助于提高模型的准确性和泛化能力。在实际应用中，可以根据具体问题的需求来选择适当的特征选择方法。如果数据集中特征数量很多，可以考虑使用互信息法进行特征选择，如果特征数量较少，可以使用 Wrapper 方法或 Embedded 方法进行特征选择。同时，需要注意特征选择方法的选择和超参数的调整对最终模型性能的影响，需要进行合理的实验和验证。" }, { "title": "机器学习基础-监督学习-训练数据-特征提取之特征哈希（Feature Hashing）", "url": "/posts/%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE-06-%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E4%B9%8B%E7%89%B9%E5%BE%81%E5%93%88%E5%B8%8C/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-02 00:00:00 +0800", "snippet": "特征哈希（Feature Hashing）是一种常用的特征提取技术，它可以将高维特征空间映射到低维空间，以减少特征向量的维数。特征哈希适用于特征空间非常大的情况下，可以有效地减少内存占用和计算时间。本文将详细讲解特征哈希的原理和实现方法，并附带 Python 代码示例。原理特征哈希的原理非常简单：将原始特征通过哈希函数映射到固定长度的向量空间中。哈希函数将每个特征值映射到一个固定的索引位置，如果两个特征值哈希到了同一个索引位置，那么它们就被视为相同的特征。特征哈希的过程可以用下面的公式表示：\\[h(x) = (x_1w_1 + x_2w_2 + ... + x_dw_d) \\bmod m\\]其中，$x$ 是原始特征向量，$d$ 是原始特征向量的维数，$w$ 是哈希函数的权重向量，$m$ 是哈希桶的数量。在实际应用中，$m$ 通常设置为 $2$ 的幂次方，例如 $2^{16}$ 或 $2^{32}$，这样可以使用位运算来加速计算。特征哈希的优点在于它不需要存储特征值和索引之间的映射关系，可以大大减少内存占用。然而，由于哈希函数的不可逆性，特征哈希可能会出现哈希冲突的情况，即不同的特征值哈希到了同一个索引位置。为了解决哈希冲突的问题，可以通过增加哈希桶的数量、使用更复杂的哈希函数或者使用其他方法进行补救。实现下面是一个使用特征哈希进行文本分类的 Python 代码示例。该示例使用了 Scikit-learn 中的哈希向量化器（HashingVectorizer）来实现特征哈希，并使用逻辑回归进行分类。from sklearn.datasets import fetch_20newsgroupsfrom sklearn.feature_extraction.text import HashingVectorizerfrom sklearn.linear_model import LogisticRegressionfrom sklearn.metrics import classification_reportfrom sklearn.pipeline import Pipeline# 加载数据集categories = [&#39;alt.atheism&#39;, &#39;talk.religion.misc&#39;]train_data = fetch_20newsgroups(subset=&#39;train&#39;, categories=categories)test_data = fetch_20newsgroups(subset=&#39;test&#39;, categories=categories)# 定义哈希向量化器和逻辑回归模型vectorizer = HashingVectorizer(n_features=10000)lr = LogisticRegression()# 定义使用哈希向量化器和逻辑回归模型构建 Pipelinepipeline = Pipeline([(&#39;vectorizer&#39;, vectorizer), (&#39;lr&#39;, lr)])# 训练模型pipeline.fit(train_data.data, train_data.target)# 测试模型y_pred = pipeline.predict(test_data.data)print(classification_report(test_data.target, y_pred))在上述代码中，我们首先使用 Scikit-learn 中的 HashingVectorizer 类定义了一个哈希向量化器，设置 n_features=10000 表示我们希望生成的哈希向量的长度为 $10000$。然后，我们定义了一个逻辑回归模型，将哈希向量化器和逻辑回归模型组合成了一个 Pipeline。在训练模型时，我们将训练数据的文本和标签传递给 Pipeline 的 fit() 方法。Pipeline 会自动将文本转换成哈希向量，并使用逻辑回归模型进行训练。在测试模型时，我们将测试数据的文本传递给 Pipeline 的 predict() 方法，得到模型对每个文本的分类结果。最后，我们使用 Scikit-learn 中的 classification_report 函数计算模型的精度、召回率、F1 值等指标。总结本文详细讲解了特征哈希（Feature Hashing）的原理和实现方法，以及使用特征哈希进行文本分类的 Python 代码示例。特征哈希适用于特征空间非常大的情况下，可以有效地减少内存占用和计算时间。但是，由于哈希函数的不可逆性，特征哈希可能会出现哈希冲突的问题，需要进行适当的补救措施。" }, { "title": "机器学习基础-监督学习-训练数据-特征提取之独立成分分析（ICA）", "url": "/posts/%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE-05-%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E4%B9%8B%E7%8B%AC%E7%AB%8B%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-02 00:00:00 +0800", "snippet": "独立成分分析（Independent Component Analysis，简称 ICA）是一种数据分析和特征提取方法，主要用于多个随机变量之间的分离和独立成分的提取，它可以用于信号处理、图像处理、语音识别等领域的特征提取和数据分离。ICA 的主要思想是在多个随机信号中找到相互独立的成分，这些成分可以解释原始信号的变化和组合。ICA 在信号处理、图像处理、语音识别等领域有着广泛的应用。ICA 的数学模型可以表示为以下形式：\\[X = AS\\]其中 $X$ 是观测到的数据矩阵，$A$ 是混合矩阵，$S$ 是独立成分矩阵。我们的目标是从观测数据 $X$ 中估计出混合矩阵 $A$ 和独立成分矩阵 $S$。ICA 的基本假设是，观测数据 $X$ 是由多个相互独立的成分线性组合而成的。因此，我们可以通过最大化独立性的度量来估计混合矩阵 $A$ 和独立成分矩阵 $S$。常用的独立性度量有： 熵（entropy） 互信息（mutual information） 非高斯性（non-Gaussianity）其中，非高斯性是最常用的度量方法，因为大部分自然信号都是非高斯分布的。一般来说，我们可以通过计算每个成分的样本 kurtosis（峭度）来评估其非高斯性。ICA 的求解方法可以使用梯度下降算法、快速独立成分分析（FastICA）算法等。FastICA 是一种基于最大非高斯性的 ICA 求解方法，它可以快速地计算出混合矩阵 $A$ 和独立成分矩阵 $S$。以下是一个使用 scikit-learn 库中 FastICA 方法进行 ICA 的 Python 示例代码：import numpy as npfrom scipy import signalfrom sklearn.decomposition import FastICAimport matplotlib.pyplot as plt# 生成混合信号np.random.seed(0)n_samples = 2000time = np.linspace(0, 8, n_samples)s1 = np.sin(2 * time) # 正弦波s2 = np.sign(np.sin(3 * time)) # 方波s3 = signal.sawtooth(2 * np.pi * time) # 锯齿波S = np.c_[s1, s2, s3]S += 0.2 * np.random.normal(size=S.shape) # 添加噪声# 混合信号A = np.array([[1, 1, 1], [0.5, 2, 1.0], [1.5, 1.0, 2.0]])X = np.dot(S, A.T) # 观测信号# ICA 处理ica = FastICA(n_components=3)S_ = ica.fit_transform(X) # 估计独立成分A_ = ica.mixing_ # 估计混合矩阵# 绘图比较plt.figure(figsize=(8, 8))models = [S, X, S_]names = [&#39;真实信号&#39;, &#39;观测信号&#39;, &#39;ICA 估计信号&#39;]colors = [&#39;red&#39;, &#39;steelblue&#39;, &#39;orange&#39;]for ii, (model, name) in enumerate(zip(models, names), 1): plt.subplot(3, 1, ii) plt.title(name) for sig, color in zip(model.T, colors): plt.plot(sig, color=color)plt.tight_layout()plt.show()这个示例中，我们生成了三个不同类型的原始信号 $s_1, s_2, s_3$，并且使用混合矩阵 $A$ 将它们线性组合得到观测信号 $X$。然后使用 FastICA 方法估计混合矩阵 $A$ 和独立成分 $S$，最后比较了原始信号、观测信号和估计信号的图像。需要注意的是，ICA 方法的应用需要满足以下前提条件： 独立成分是非高斯分布的。 混合矩阵 $A$ 是非奇异的，即每个混合信号都能被恰好一个原始信号生成。 原始信号是独立的，即它们没有统计相关性。如果这些前提条件不满足，ICA 方法的效果可能会受到影响。此外，还需要注意到 ICA 方法的一些缺点： ICA 仅能得到线性混合的独立成分，对于非线性混合或者存在高斯噪声的数据，效果可能不佳。 ICA 方法需要对数据进行预处理，包括中心化、白化等操作，这些操作可能会引入一些误差。 ICA 方法的计算复杂度较高，对于大规模数据的处理可能会耗费较长时间。因此，在应用 ICA 方法时需要根据具体问题的特点进行判断和调整，选择合适的算法和参数。" }, { "title": "机器学习基础-监督学习-训练数据-特征提取之线性判别分析（LDA）", "url": "/posts/%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE-04-%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-02 00:00:00 +0800", "snippet": "线性判别分析（Linear Discriminant Analysis，LDA）是一种常用的模式识别算法，它在降维和分类问题中被广泛应用。LDA 的主要思想是，将数据投影到一个低维空间中，并使得不同类别的数据点之间的距离最大化，同一类别内的数据点之间的距离最小化。下面我们来详细讲解一下 LDA 的算法原理。假设我们有一个数据集 $D={(x_1,y_1),(x_2,y_2),\\cdots,(x_n,y_n)}$，其中 $x_i \\in \\mathbb{R}^d$ 表示样本的特征向量，$y_i$ 表示样本的标签（假设样本属于 $k$ 个类别之一）。我们的目标是将这些数据投影到一个 $m$ 维的子空间中，使得不同类别的样本点之间的距离最大化，同一类别内的样本点之间的距离最小化。假设我们已经将数据投影到了一个 $m$ 维的子空间中，其中 $m &amp;lt; d$，我们可以将样本的特征向量表示为 $z=(z_1,z_2,\\cdots,z_m)^T$，其中 $z_i$ 表示样本在第 $i$ 个维度上的投影。我们用 $\\boldsymbol{\\mu}_i$ 表示第 $i$ 类样本在子空间中的均值向量，用 $\\boldsymbol{\\Sigma}_i$ 表示第 $i$ 类样本在子空间中的协方差矩阵。我们的目标是找到一个投影矩阵 $\\boldsymbol{W} \\in \\mathbb{R}^{d \\times m}$，使得样本在投影后的子空间中的类别可分性最大化。具体来说，我们可以通过最大化两个指标来实现这个目标： 类间距离最大化（Between-Class Scatter Matrix）：表示不同类别样本的投影之间的距离。它的计算公式为：\\[S_B = \\textstyle\\sum_{i=1}^{k} N_i(\\mu_i-\\mu)(\\mu_i-\\mu)^T\\]其中 $N_i$ 表示第 $i$ 类样本的数量，$\\boldsymbol{\\mu}$ 表示所有样本的均值向量，$\\boldsymbol{\\mu}_i$ 表示第 $i$ 类样本在子空间中的均值向量。类内距离最小化（Within-Class Scatter Matrix）：表示同一类别样本之间的投影距离。它的计算公式为：\\[S_W = \\textstyle\\sum_{i=1}^{k} \\Sigma_i\\]将两个指标综合起来，我们可以定义一个目标函数：\\[\\boldsymbol{J}(\\boldsymbol{W}) = \\frac{\\mathrm{tr}(W^rS_BW)}{\\mathrm{tr}(W^rS_WW)}\\]其中 $\\mathrm{tr}(\\cdot)$ 表示矩阵的迹（即矩阵对角线上的元素之和）。我们的目标是最大化目标函数 $\\boldsymbol{J}(\\boldsymbol{W})$，找到一个最优的投影矩阵 $\\boldsymbol{W}$。我们可以通过求解广义特征值问题来得到最优的投影矩阵 $\\boldsymbol{W}$。具体来说，我们需要计算 $\\boldsymbol{S}_W^{-1}\\boldsymbol{S}_B$ 的最大 $m$ 个特征值对应的特征向量，将这些特征向量组成一个矩阵 $\\boldsymbol{W}$，即为最优的投影矩阵。LDA 算法的代码实现如下：import numpy as npdef lda(X, y, m): &quot;&quot;&quot; X: shape (n, d), input data y: shape (n,), labels m: int, number of dimensions to project to &quot;&quot;&quot; classes = np.unique(y) n_classes = len(classes) n_samples, n_features = X.shape # Compute mean of each class means = np.zeros((n_classes, n_features)) for i, c in enumerate(classes): Xc = X[y == c] means[i] = np.mean(Xc, axis=0) # Compute within-class scatter matrix Sw = np.zeros((n_features, n_features)) for i, c in enumerate(classes): Xc = X[y == c] Xc_centered = Xc - means[i] Sw += Xc_centered.T @ Xc_centered # Compute between-class scatter matrix mu = np.mean(X, axis=0) Sb = np.zeros((n_features, n_features)) for i, c in enumerate(classes): Ni = len(X[y == c]) mu_i = means[i] Sb += Ni * (mu_i - mu)[:, None] @ (mu_i - mu)[None, :] # Solve generalized eigenvalue problem eigvals, eigvecs = np.linalg.eig(np.linalg.inv(Sw) @ Sb) eigvecs = eigvecs[:, np.argsort(eigvals)[::-1][:m]] # Project data onto subspace X_lda = X @ eigvecs return X_lda其中，X 是输入数据，y 是标签，m 是要投影到的维度。函数首先计算每个类别的均值向量 means，然后计算类内协方差矩阵 Sw 和类间协方差矩阵 Sb，最后通过求解广义特征值问题得到最优的投影矩阵 eigvecs，将数据投影到子空间并返回投影后的数据 X_lda。下面我们使用 scikit-learn 库中的 LDA 实现对鸢尾花数据集进行降维并可视化。首先加载数据集并进行数据预处理：from sklearn.datasets import load_irisfrom sklearn.preprocessing import StandardScaler# Load datasetiris = load_iris()X, y = iris.data, iris.target# Standardize datascaler = StandardScaler()X_std = scaler.fit_transform(X)然后使用 LDA 进行降维：from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA# Perform LDAlda = LDA(n_components=2)X_lda = lda.fit_transform(X_std, y)最后，我们可以将投影后的数据进行可视化：import matplotlib.pyplot as plt# Plot resultsplt.scatter(X_lda[:, 0], X_lda[:, 1], c=y)plt.xlabel(&#39;LD1&#39;)plt.ylabel(&#39;LD2&#39;)plt.show()这将显示一个二维散点图，其中每个点代表一个样本，其颜色对应于其类别。可以看到，LDA 成功地将数据投影到了一个新的低维子空间，并且不同类别的数据点在这个子空间中分离得更好。" }, { "title": "机器学习基础-监督学习-训练数据-特征提取之主成分分析（PCA）", "url": "/posts/%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE-03-%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E4%B9%8B%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-02 00:00:00 +0800", "snippet": "主成分分析（Principal Component Analysis，PCA）是一种经典的数据降维方法，可以用于减少高维数据的维数。PCA 的目标是将原始数据映射到一个新的低维空间，使得映射后的数据能够最大程度地保留原始数据的信息。在 PCA 中，新的低维空间被定义为原始数据的主成分，主成分是指方差最大的数据投影方向。PCA 的原理假设我们有一个 $n$ 个样本、每个样本有 $m$ 个特征的数据集 $X$，我们的目标是将 $X$ 映射到一个 $k (k&amp;lt;m)$ 维的空间中。PCA 的核心是通过线性变换将原始特征空间中的数据映射到一个新的特征空间，新的特征空间中的每个维度被称为主成分。PCA 的步骤如下： 对原始数据进行中心化处理，即将每个特征的均值归零。 计算协方差矩阵 $C$，$C_{i,j}$ 表示第 $i$ 个特征和第 $j$ 个特征之间的协方差。协方差矩阵是一个对称矩阵，对角线上的元素是每个特征的方差。\\[C = \\frac{1}{n} \\textstyle\\sum_{i=1}^{n} (x_i-\\mu)(x_i-\\mu)^T\\] 其中，$x_i$ 是第 $i$ 个样本，$\\mu$ 是所有样本的均值。 计算协方差矩阵的特征值和特征向量。协方差矩阵的特征值表示在该方向上的方差，特征向量是一个与特征值相对应的向量。 对特征值进行排序，选择前 $k$ 个特征向量作为主成分。 使用选定的主成分将原始数据映射到新的低维空间中。 以下是使用 Python 中的 scikit-learn 库实现 PCA 的代码示例：from sklearn.datasets import load_irisfrom sklearn.decomposition import PCA# 加载数据集iris = load_iris()X = iris.data# 使用 PCA 进行降维pca = PCA(n_components=2)X_pca = pca.fit_transform(X)# 输出结果print(&quot;PCA 降维前数据形状：&quot;, X.shape)print(&quot;PCA 降维后数据形状：&quot;, X_pca.shape)在示例中，我们使用 PCA 对 iris 数据集进行了降维，并输出了降维前后数据的形状。下面我们将对 PCA 的过程进行详细讲解。首先，我们需要对数据进行中心化处理，即将每个特征的均值归零。在 scikit-learn 中，PCA 的默认行为是对输入数据进行中心化处理。接下来，我们计算协方差矩阵 $C$。在 scikit-learn 中，可以使用 sklearn.covariance 模块的 empirical_covariance 函数计算协方差矩阵。以下是示例代码：from sklearn.covariance import empirical_covariance# 计算协方差矩阵cov = empirical_covariance(X)然后，我们需要计算协方差矩阵的特征值和特征向量。在 scikit-learn 中，可以使用 sklearn.decomposition 模块的 PCA 类来计算。以下是示例代码：from sklearn.decomposition import PCA# 计算协方差矩阵的特征值和特征向量pca = PCA()pca.fit(X)# 输出特征值和特征向量print(&quot;特征值：&quot;, pca.explained_variance_)print(&quot;特征向量：&quot;, pca.components_)在这个示例中，我们使用 PCA 类计算了协方差矩阵的特征值和特征向量，并输出了它们的值。最后，我们需要选择前 $k$ 个特征向量作为主成分，并使用它们将原始数据映射到新的低维空间中。在 scikit-learn 中，可以在创建 PCA 类时指定 n_components 参数来选择要保留的主成分个数。以下是示例代码：# 选择前 2 个特征向量作为主成分pca = PCA(n_components=2)X_pca = pca.fit_transform(X)# 输出结果print(&quot;PCA 降维前数据形状：&quot;, X.shape)print(&quot;PCA 降维后数据形状：&quot;, X_pca.shape)在这个示例中，我们选择前 2 个特征向量作为主成分，并使用它们将 iris 数据集从 4 维降到了 2 维。输出结果表明，降维后的数据形状为 (150, 2)，即有 150 个样本，每个样本有 2 个特征。总的来说，PCA 是一种非常有用的数据降维技术，可以在保留数据信息的同时减少数据的维数。在实际应用中，可以根据需要选择合适的主成分个数，以达到最好的降维效果。" }, { "title": "机器学习基础-监督学习-训练数据-特征提取和特征选择", "url": "/posts/%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE-02-%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E5%92%8C%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-02 00:00:00 +0800", "snippet": "在监督学习中，特征提取和特征选择是非常重要的步骤，它们可以提高模型的泛化能力和性能。以下是对特征提取和特征选择的详细解释。特征提取是指从原始数据中提取有意义的特征，以便更好地表达数据的本质特性。特征提取通常包括以下几个步骤： 数据预处理：在进行特征提取之前，需要对原始数据进行预处理，包括去噪、去冗余、归一化等操作。 特征提取：从预处理后的数据中提取有意义的特征。特征提取的方法包括传统的特征提取方法和深度学习方法。 特征表示：将提取出的特征表示成特征向量或矩阵的形式，以便输入到机器学习模型中。 常用的特征提取方法包括 PCA（主成分分析）、LDA（线性判别分析）、ICA（独立成分分析）、特征哈希等方法。以下是一个使用 PCA 进行特征提取的 Python 示例代码：from sklearn.decomposition import PCAimport numpy as np# 生成数据X = np.random.rand(100, 10)# 使用 PCA 进行特征提取pca = PCA(n_components=3)X_pca = pca.fit_transform(X)# 输出结果print(&quot;原始数据维度：&quot;, X.shape)print(&quot;PCA 特征提取后数据维度：&quot;, X_pca.shape)特征选择是指从已经提取出的特征中选择最重要的特征，以减少模型的复杂度和提高模型性能。常用的特征选择方法包括 Filter 方法、Wrapper 方法和 Embedded 方法。以下是一个使用 Filter 方法进行特征选择的 Python 示例代码：from sklearn.datasets import load_irisfrom sklearn.feature_selection import SelectKBestfrom sklearn.feature_selection import chi2# 加载数据集iris = load_iris()X, y = iris.data, iris.target# 使用卡方检验进行特征选择selector = SelectKBest(chi2, k=2)X_new = selector.fit_transform(X, y)# 输出结果print(&quot;原始数据维度：&quot;, X.shape)print(&quot;特征选择后数据维度：&quot;, X_new.shape)在这个示例中，我们使用了卡方检验进行特征选择，并选取了最优的两个特征。需要注意的是，在进行特征选择时，我们需要对选择的特征进行评估和验证，以确保选择出来的特征对模型的性能有实际的贡献。总之，特征提取和特征选择是监督学习中非常重要的步骤，良好的特征提取和特征选择可以提高模型的准确性和泛化能力。以下是一些常见的特征提取和特征选择技术：特征提取技术： 主成分分析（PCA）：用于减少高维数据的维数，通过将原始数据映射到新的低维空间来提取主要的特征。 线性判别分析（LDA）：用于分类问题中的特征提取，通过最大化类别之间的方差和最小化类别内的方差来提取有用的特征。 独立成分分析（ICA）：用于将混合信号分离成原始信号的线性组合，以提取混合信号中的有用信息。 特征哈希（Feature Hashing）：将高维特征空间映射到低维空间，以减少特征向量的维数。 特征选择技术： Filter 方法：根据某些统计指标（如卡方检验、互信息等）对特征进行评估，选择最好的特征进行训练。 Wrapper 方法：将特征选择问题转化为子集选择问题，使用递归特征消除、前向搜索等方法选择最优的特征子集。 Embedded 方法：在训练模型时进行特征选择，例如 Lasso、Ridge、Elastic Net 等正则化方法。 " }, { "title": "机器学习基础-监督学习-训练数据-数据清洗", "url": "/posts/%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE-01-%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-02 00:00:00 +0800", "snippet": "在机器学习中，数据清洗是指对原始数据进行预处理，以去除数据中的错误、重复、缺失值和异常值等问题。数据清洗可以提高数据质量和模型性能，从而使机器学习算法更加准确地分析和预测。下面我们以 Python 中的 pandas 库为例，介绍常见的数据清洗方法：1. 去重去重是指去除数据中重复的记录，常用的 pandas 函数为 drop_duplicates()。在数据分析和机器学习中，经常会遇到数据中存在重复记录的情况，这些重复的记录可能会对分析和模型训练产生影响。因此，需要对数据进行去重处理。在 Python 中，使用 Pandas 库可以非常方便地进行去重处理。Pandas 中的 drop_duplicates() 函数可以去除数据框或者序列中的重复记录。下面给出一个示例代码，以演示如何使用 drop_duplicates() 函数进行去重处理：import pandas as pd# 创建一个数据框data = pd.DataFrame({&#39;A&#39;: [&#39;foo&#39;, &#39;bar&#39;, &#39;foo&#39;, &#39;bar&#39;, &#39;foo&#39;, &#39;bar&#39;, &#39;foo&#39;, &#39;foo&#39;], &#39;B&#39;: [&#39;one&#39;, &#39;one&#39;, &#39;two&#39;, &#39;three&#39;, &#39;two&#39;, &#39;two&#39;, &#39;one&#39;, &#39;three&#39;], &#39;C&#39;: [1, 2, 1, 1, 2, 1, 1, 2]})print(&quot;原始数据框：&quot;)print(data)# 去重data_drop = data.drop_duplicates()print(&quot;去重后的数据框：&quot;)print(data_drop)上述代码中，首先创建了一个包含重复记录的数据框 data，然后使用 drop_duplicates() 函数对其进行去重处理，得到去重后的数据框 data_drop。drop_duplicates() 函数有几个可选参数： subset：指定需要去重的列名，默认为所有列。 keep：指定去重后保留哪个记录，默认为第一个记录。 inplace：是否原地修改数据框，默认为 False。下面给出一个示例代码，以演示如何使用 subset 和 keep 参数：import pandas as pd# 创建一个数据框data = pd.DataFrame({&#39;A&#39;: [&#39;foo&#39;, &#39;bar&#39;, &#39;foo&#39;, &#39;bar&#39;, &#39;foo&#39;, &#39;bar&#39;, &#39;foo&#39;, &#39;foo&#39;], &#39;B&#39;: [&#39;one&#39;, &#39;one&#39;, &#39;two&#39;, &#39;three&#39;, &#39;two&#39;, &#39;two&#39;, &#39;one&#39;, &#39;three&#39;], &#39;C&#39;: [1, 2, 1, 1, 2, 1, 1, 2]})print(&quot;原始数据框：&quot;)print(data)# 指定需要去重的列和保留最后一个记录data_drop = data.drop_duplicates(subset=[&#39;A&#39;, &#39;B&#39;], keep=&#39;last&#39;)print(&quot;去重后的数据框：&quot;)print(data_drop)上述代码中，使用 subset 参数指定了需要去重的列为 [‘A’, ‘B’]，使用 keep 参数指定了保留最后一个记录。因此，在去重后，数据框 data_drop 中仅保留了每个 [‘A’, ‘B’] 组合中的最后一个记录。总之，数据去重是数据清洗的重要步骤之一，在实际应用中常常需要进行。使用 Pandas 库提供的 drop_duplicates() 函数可以非常方便地实现数据去重。2. 缺失值处理在机器学习中，缺失值处理是数据预处理的重要环节之一。缺失值是指数据中的某些值未被记录或者记录不完整，可能会影响模型的训练和预测结果。因此，需要对缺失值进行处理，常用的方法包括删除缺失值、填充缺失值等。下面我们以 Python 中的 pandas 库为例，介绍常见的缺失值处理方法：2.1 删除缺失值删除缺失值是指直接将缺失值所在的记录从数据中删除，常用的 pandas 函数为 dropna()。以下是示例代码：import pandas as pd# 读取数据文件data = pd.read_csv(&#39;data.csv&#39;)# 删除缺失值data = data.dropna()# 输出处理后的数据记录数print(&quot;处理后的记录数：&quot;, len(data))2.2 填充缺失值填充缺失值是指用某些值替换缺失值，常用的填充方法包括使用平均值、中位数、众数等。以下是示例代码：import pandas as pd# 读取数据文件data = pd.read_csv(&#39;data.csv&#39;)# 使用平均值填充缺失值mean = data[&#39;age&#39;].mean()data[&#39;age&#39;].fillna(mean, inplace=True)# 使用中位数填充缺失值median = data[&#39;income&#39;].median()data[&#39;income&#39;].fillna(median, inplace=True)# 使用众数填充缺失值mode = data[&#39;gender&#39;].mode()[0]data[&#39;gender&#39;].fillna(mode, inplace=True)# 输出处理后的数据记录数print(&quot;处理后的记录数：&quot;, len(data))2.3 插值填充插值填充是指用缺失值前后的数据进行线性插值来填充缺失值。常用的插值函数包括 interpolate() 和 fillna(method=’ffill’)。以下是示例代码：import pandas as pd# 读取数据文件data = pd.read_csv(&#39;data.csv&#39;)# 使用插值填充缺失值data = data.interpolate()# 使用前向填充填充缺失值data.fillna(method=&#39;ffill&#39;, inplace=True)# 输出处理后的数据记录数print(&quot;处理后的记录数：&quot;, len(data))需要注意的是，不同的数据集和任务可能需要采用不同的缺失值处理方法。在进行缺失值处理时，需要对数据的特性和任务需求进行分析和调整，以保证数据的质量和适用性。3. 异常值处理在数据处理中，异常值是指与其他观测值相比显著不同的值，可能是由于数据收集或者输入错误、设备故障等原因导致。异常值会对数据分析和机器学习等任务产生不良影响，因此需要进行处理。下面我们介绍几种常见的异常值处理方法：3.1 删除异常值删除异常值是指直接从数据集中删除所有被认为是异常值的观测值。这种方法的优点是简单、易于实现，但可能会造成信息的丢失，特别是当数据集中的异常值比例较高时。以下是示例代码：import pandas as pd# 读取数据文件data = pd.read_csv(&#39;data.csv&#39;)# 删除年龄小于0或大于100的异常值data = data[(data[&#39;age&#39;] &amp;gt;= 0) &amp;amp; (data[&#39;age&#39;] &amp;lt;= 100)]# 输出异常值数量print(&quot;删除异常值后的记录数：&quot;, len(data))3.2 替换异常值替换异常值是指将异常值替换为某个合理的值，比如平均值、中位数等。这种方法的优点是可以保留所有数据，但可能会引入新的偏差。以下是示例代码：import pandas as pdimport numpy as np# 读取数据文件data = pd.read_csv(&#39;data.csv&#39;)# 计算年龄的中位数age_median = np.median(data[&#39;age&#39;])# 将年龄小于0或大于100的异常值替换为中位数data.loc[data[&#39;age&#39;] &amp;lt; 0, &#39;age&#39;] = age_mediandata.loc[data[&#39;age&#39;] &amp;gt; 100, &#39;age&#39;] = age_median# 输出异常值数量print(&quot;替换异常值后的记录数：&quot;, len(data))3.3 离散化离散化是指将连续的数值变量转换为离散的值，比如将年龄分为多个年龄段。这种方法的优点是可以将连续的数值转换为离散的类别，但可能会引入新的偏差。以下是示例代码：import pandas as pdimport numpy as np# 读取数据文件data = pd.read_csv(&#39;data.csv&#39;)# 将年龄分为3个年龄段：0-20、20-50、50-100data[&#39;age_bin&#39;] = pd.cut(data[&#39;age&#39;], bins=[0, 20, 50, 100], labels=[&#39;0-20&#39;, &#39;20-50&#39;, &#39;50-100&#39;])# 输出异常值数量print(&quot;离散化后的记录数：&quot;, len(data))除了以上几种方法，还有一些其他的异常值处理方法，比如使用机器学习模型预测异常值、使用规则过滤异常值等。在实际应用中，需要根据具体情况选择合适的异常值处理方法。" }, { "title": "机器学习基础-监督学习-训练数据", "url": "/posts/%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE-00/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-02 00:00:00 +0800", "snippet": "在机器学习中，训练数据是用于训练模型的数据集合。训练数据包含一系列的样本，每个样本都包含了一个特征向量和一个对应的标签。特征向量表示了样本的特征，标签则是我们要预测的目标。通过训练数据，我们可以训练出一个模型，用于预测未知数据的标签。下面我们来看一个简单的示例，说明如何使用训练数据训练一个线性回归模型：假设我们有一个训练数据集合包含 $m$ 个样本 $(\\mathbf{x}_1, y_1), (\\mathbf{x}_2, y_2), \\ldots, (\\mathbf{x}_m, y_m)$，其中 $\\mathbf{x}_i$ 是一个 $n$ 维的特征向量，$y_i$ 是对应的标签。我们的目标是学习出一个线性回归模型，将输入特征向量 $\\mathbf{x}$ 映射到标签 $y$，即：\\[h_\\theta(\\mathbf{x}) = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + ... + \\theta_nx_n = \\theta^T\\mathbf{x}\\]其中 $\\theta_0, \\theta_1, \\ldots, \\theta_n$ 是模型的参数，需要通过训练数据来学习。我们定义模型的预测值和真实值之间的误差为：\\[\\mathbf{Error}(\\theta) = \\frac{1}{2m} \\textstyle\\sum_{i=1}^{m} (h_\\theta(\\mathbf{x}_i)-y_i)^2\\]我们的目标是找到一组参数 $\\theta$，使得误差最小。可以使用梯度下降法来最小化误差，更新参数的公式为：\\[\\theta_j = \\theta_j - \\alpha\\frac{\\partial\\mathbf{Error}(\\theta)}{\\partial\\theta_j}\\]其中 $\\alpha$ 是学习率，控制参数的更新步长。梯度下降法的过程就是不断迭代更新参数 $\\theta$，直到误差收敛。下面是一个简单的 Python 代码示例，演示如何使用梯度下降法训练一个线性回归模型：import numpy as np# 训练数据X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])y = np.array([3, 7, 11, 15])# 初始化模型参数theta = np.zeros(2)# 定义模型def h(X, theta): return np.dot(X, theta)# 定义误差函数def error(X, y, theta): m = len(y) return 1.0 / (2 * m) * np.sum((h(X, theta) - y) ** 2)# 定义梯度函数def gradient(X, y, theta):m = len(y)return 1.0 / m * np.dot(X.T, h(X, theta) - y)# 定义梯度下降函数def gradient_descent(X, y, theta, alpha, num_iters): for i in range(num_iters): theta -= alpha * gradient(X, y, theta) if i % 100 == 0: print(&quot;Iteration {}: error = {}&quot;.format(i, error(X, y, theta))) return theta# 调用梯度下降函数训练模型theta = gradient_descent(X, y, theta, alpha=0.01, num_iters=1000)# 输出模型参数print(&quot;theta =&quot;, theta)在这个示例中，我们首先定义了训练数据 $X$ 和标签 $y$，然后初始化了模型参数 $\\theta$。接着我们定义了模型函数 $h(\\mathbf{x}, \\theta)$、误差函数 $\\mathrm{Error}(\\theta)$ 和梯度函数 $\\nabla_{\\theta} \\mathrm{Error}(\\theta)$。最后我们调用了梯度下降函数 gradient_descent()，将训练数据 $X$、标签 $y$、模型参数 $\\theta$、学习率 $\\alpha$ 和迭代次数 num_iters 作为输入，训练出了一个线性回归模型，并输出了最终的模型参数 $\\theta$。需要注意的是，训练数据的质量对模型的性能影响很大。如果训练数据的样本数量太少、特征过于简单或者样本中存在较大的噪声，都会导致模型的泛化性能不佳。因此，在选择训练数据时需要仔细考虑，确保数据集合足够大、特征丰富，并且尽量去除噪声数据。在监督学习中，训练数据的准备和处理是非常重要的。以下是一些常见的训练数据处理方法： 数据清洗：数据清洗是指对原始数据进行去重、缺失值填充、异常值处理等操作，以保证数据的质量。数据清洗通常是数据处理的第一步，可以使用 Python 中的 pandas 库进行实现。 特征提取和特征选择：特征提取是指从原始数据中提取有意义的特征，以便更好地表达数据的本质特性。特征选择则是从已经提取出的特征中选择最重要的特征，以减少模型的复杂度。常用的特征提取方法包括 PCA（主成分分析）、LDA（线性判别分析）和文本特征提取等方法。 数据转换和标准化：数据转换是指将数据从一种形式转换为另一种形式，比如将分类数据转换为数值型数据。数据标准化则是将数据进行缩放和归一化，以便更好地适应模型。常用的数据标准化方法包括 Z-score 标准化、最大最小值标准化等方法。 以上方法不一定适用于所有的监督学习任务，具体应根据任务的特性进行选择和调整。例如，对于图像分类任务，常用的特征提取方法是使用卷积神经网络（CNN）进行特征提取。在实际应用中，特征提取和数据处理通常是一个迭代的过程，需要不断调整和优化以获得更好的模型性能。总之，训练数据的准备和处理是监督学习中非常重要的一步，良好的数据处理可以有效提高模型的泛化能力和性能。" }, { "title": "机器学习基础-监督学习-什么是监督学习", "url": "/posts/%E4%BB%80%E4%B9%88%E6%98%AF%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/", "categories": "机器学习", "tags": "人工智能, 机器学习, 监督学习", "date": "2023-01-01 00:00:00 +0800", "snippet": "监督学习（Supervised Learning）是一种机器学习方法，其基本思想是通过已经标记好的训练数据来训练一个模型，使其能够从新的、未知的数据中进行预测或分类。在监督学习中，每个训练样本都有对应的标签或输出值，模型的目标是尽可能准确地预测或分类新的未知数据。监督学习分为两种情况，分别是分类和回归： 分类（Classification）：将数据分为多个类别中的一个。例如，将一张图片识别为猫或狗。 在分类问题中，输入数据通常被表示为特征向量，每个特征代表输入数据的不同属性。例如，在图像分类任务中，可以使用像素值、颜色、纹理等作为特征。每个样本还有一个标签，表示该样本属于哪个类别。模型的目标是根据输入数据的特征向量预测输出的类别标签。 回归（Regression）：将数据映射到一个连续的输出空间中。例如，预测房价或股票价格。 在回归问题中，输入数据也被表示为特征向量，每个特征代表输入数据的不同属性。每个样本还有一个实数输出值。模型的目标是根据输入数据的特征向量预测输出的实数值。 在监督学习中，常用的算法包括线性回归、逻辑回归、支持向量机、决策树、集成学习等。这些算法在不同的任务和数据集上有不同的表现和优劣，选择适合的算法是进行监督学习的重要步骤。" }, { "title": "Git 设计原理", "url": "/posts/Git-%E5%8E%9F%E7%90%86/", "categories": "技术", "tags": "Git, 原理", "date": "2020-10-13 00:00:00 +0800", "snippet": "Git 三类对象 blob - 二进制文件内容 tree - 特定时间项目目录下文件名和目录名的列表 commit - 提交内容，包含当前 tree hash 和相关提交信息每种对象都会根据内容通过 SHA1 算法生成长度 40 位的哈希值。Git 的所有对象都储存在 .git/objects 目录，哈希值前 2 位用作目录名，后 38 位用作文件名。分析上图每个对象的内容，可通过 git cat-file -p &amp;lt;hash&amp;gt; 命令打印出对象内容。$ git cat-file -p 107dtree f60f9b63f809513028ba3eb95e7c04e37b574024# 因为是第一次提交，所以这里没有出现 parent hash 项，表示上次 commit 对象。# parent hashauthor znlbwo &amp;lt;znlbwo@qq.com&amp;gt; 1602517489 +0800committer znlbwo &amp;lt;znlbwo@qq.com&amp;gt; 1602517489 +0800chore: init【commit 对象】107d 表示最后一次提交所对应的 commit 对象，内容包含： commit 时（特定时间）项目目录 tree hash； parent hash，即上一次 commit hash； author 名字、邮箱、时间 committer 名字、邮箱、时间 commit message$ git cat-file -p f60f100644 blob 92c57b03f4ac4d6608905e3136c8c37650893d6c package.json040000 tree 604697db1cc7de2e719386529148f700c3ee7047 src【tree 对象】很明显这是根目录，一个 package.json 文件和 src 目录。$ git cat-file -p 92c5{ &quot;name&quot;: &quot;git-principle&quot;, &quot;version&quot;: &quot;1.0.0&quot;, &quot;main&quot;: &quot;index.js&quot;, &quot;author&quot;: &quot;znlbwo &amp;lt;znlbwo@qq.com&amp;gt;&quot;, &quot;license&quot;: &quot;MIT&quot;}【blob 对象】文件 package.json 内容$ git cat-file -p 6046100644 blob bef3b32a9404c1a4e297f6bdd0980fe784d3db95 index.js【tree 对象】src 目录下只有一个 index.js 文件$ git cat-file -p bef3console.log(&#39;src/index.js&#39;)【blob 对象】文件 src/index.js 内容思考：这些对象都是在什么时候创建的？Git 分区 工作区 暂存区 版本库git add将工作区内容添加到暂存库，针对新增和修改的文件（不包含目录），创建 blob 对象。git commit将暂存区内容提交到当前分支，针对当前状态下的项目目录创建快照，也就是 tree 对象，然后创建 commit 对象。针对文件的删除、目录的新增和删除、文件名和目录名的修改，创建 tree 对象。删除文件不会删除与其对应的 blob 对象，仅修改目录结构，产生新的 tree 对象。 其本质上是一个 key-value 的数据库加上默克尔树形成的有向无环图（DAG）- 这才是真正的 Git——Git 内部原理揭秘！参考 Git Internals - Git Objects 这才是真正的 Git——Git 内部原理揭秘！" }, { "title": "vue-template-compiler 动态插槽名 bug 追踪", "url": "/posts/vue-template-compiler-bug-%E8%BF%BD%E8%B8%AA/", "categories": "技术", "tags": "Vue, vue-template-compiler, debug, Range, 源码", "date": "2019-06-17 00:00:00 +0800", "snippet": "Issue：https://github.com/vuejs/vue/issues/10165调试过程简化后的模板代码如下图，动态插槽名都是变量 propertyName，最终只会渲染 propertyName 为 goalAmountMonth 的内容。通过 debugger 和 条件断点 找到问题点原因是在 vue renderSlot 方法中，父组件 vm 的 $scopedSlots 对象中只有 goalAmountMonth，当动态插槽名为 achieveRatioMonth 时 renderSlot 方法返回的值为 null。那为什么 $scopedSlots 对象只有 goalAmountMonth 呢？继续查找 $scopedSlots 对象内的数据是如何来的。通过全局搜索 Vue 项目源码 $scopedSlots 关键词，找到 $scopedSlots 的赋值语句在 src/core/instants/render.js 文件内。通过条件断点运行到 $scopedSlots 赋值语句，进入 normalizeScopedSlots 方法，可以发现 _parentVnode.data.scopedSlots 就已经确定了只有 goalAmountMonth，也就是说还要再往上找。往上继续查找 _parentVnode 的 scopedSlots 数据是从哪来的，条件断点继续添加条件。终于找到问题的根节点是在 render 函数，只有动态插槽名为 goalAmountMonth 的 scopedSlots。💀动态插槽名为 achieveRatioMonth 的 scopedSlots 哪去了？这里很容易就能想到，render 函数是 vue-loader 对 .vue SFC 编译后的结果，vue-loader 编译 .vue SFC 使用的是 vue-template-compiler。vue-template-compiler 源码调试想要继续探索问题原因就需要深入 vue-template-compiler 源码调试，但是编译时要怎么调试呢？也不是不能调试编译时，只是太麻烦，没必要。先概览了一遍 vue-template-compiler 文档和源码，知道就是调用 compile 方法传入模板，就会输出一个包含 render 属性的对象。我的目的就是要调试这段过程，所以更简单的方法就是跑一个 node 脚本，引入 vue-template-compiler，调用 compile 方法传入模板，进入 compile 方法进行调试即可。使用 VSCode 可以轻松做到，代码如下：先看最终执行结果：render 属性 with 代码块内代码格式化后如下，对比之前运行时的 render 函数，结果是一致的。证实我是可以使用这段 node 脚本进行调试的，调试过程中有几个关键函数调用节点需要关注： baseCompile parse parseHTMLparseHTML 函数内需要关注 stack 对象，此对象储存模板解析过程中的 HTML 标签信息。解析两个 template 标签时的 stack 内对象如下：调试过程中观察 stack 对象信息，定位到问题的关键函数 closeElement：这是解析第一个 tempalte 结束时 closeElement 方法执行的地方，可以看到 currentParent.scopedSlots 就是保存 动态具名插槽 的对象，以 name 作为 key，name 取的是 element.slotTarget，值为 propertyName。OK，以上都没问题，但是当解析到第二个 template 结束时，再瞧瞧 closeElement 方法执行到这个地方是怎样的。未执行关键步骤时，监视对象内的 element 和 currentParent.scopedSlots 值如下图所示：当执行完关键步骤之后，currentParent.scopedSlots.propertyName 被直接覆盖了！因为 element.slotTarget 值与第一个 template 解析时的值是一样的，都是 propertyName，所以后者覆盖了前者。问题的终点就在此处，也不需要继续往下调试了，后续也就是根据 root tag 对象生成代码对象了。" }, { "title": "探索 Git 之 “交互式变基”", "url": "/posts/%E6%8E%A2%E7%B4%A2-Git-%E4%B9%8B-%E4%BA%A4%E4%BA%92%E5%BC%8F%E5%8F%98%E5%9F%BA/", "categories": "技术", "tags": "Git, rebase, Emacs, VSCode, 原理", "date": "2019-02-26 00:00:00 +0800", "snippet": "先从一个需求说起，我的某个 Git 项目的 commit history 有很多重复的 commit message，这些有着重复 commit message 的 commit 也确实是针对同一个特性的修改，我想合并这些 commit 该怎么办？如何合并多个 commit？假如我有几条 commit 记录是这样的：我想把所有 commit message 为 20190222 的 commit 合并成一条 commit。Google 搜索 “如何合并多个 commit” 排名第一的结果：通过查看文章内容，大概知道了改如何操作。通过如下命令合并指定 commit-hash 之前的 commitgit rebase -i 1522725我在公司使用的是 Windows 系统，输入命令之后会报错：emacs: command not foundCould not execute editor然后我就按照提示安装了 Emacs，Google 了解相关使用方式、基本操作，设置好环境变量，重启终端。说实话，用的挺不习惯的。不过用过几次后我就发现，这不就是编辑 git-rebase-todo 文件吗？git-rebase-todo 文件在 .git/rebase-merge 目录下，rebase-merge 目录仅当 Git 处于 rebase merge 状态时才存在。我为什么一定要用 emacs 去编辑文件了？用 vscode 行不行？Google 之后发现时可行的，只要设置 Git 配置的 core editor 即可。git config --global core.editor &amp;lt;editor&amp;gt;那如何将 editor 配置成 vscode 呢？继续使用 Google，然后在 stack overflow - How to use Visual Studio Code as Default Editor for Git 我找到了答案。git config --global core.editor &quot;code --wait&quot;再次执行 git rebase -i 1522725 命令，成功使用 vscode 打开 git-rebase-todo 文件。根据文章说明，只要将后面两条 commit 的 pick 改为 s 即可。保存并关闭文件，会自动新打开 .git/COMMIT_EDITMSG 文件，意思是修改合并后的 commit message。我希望 commit message 还是 20190222，所以只需删除掉两个 20190222 即可。保存并关闭文件，终端会自动完成 rebase 操作。使用 git log --oneline 查看 commit history 会发现，只有一条 commit message 为 20190222，并且 commit-hash 也发生了改变。问题是解决了，但本着探索之心，我想知道 git rebase 加上 -i 参数之后，到底是什么意思。git rebase -i 交互式变基指南参考阅读 Git 分支 - 分支的变基 git-rebase INTERACTIVE MODE-i 是 --interactive 的简写，中文翻译叫 “互动”，与 rebase 结合可以称之为 “交互式变基”。这种方法通常用于在向别处推送提交之前对它们（commits）进行重写，我们可以对历史 commit 进行重新排序、修改、删除。也就是说，我们之前的需求 “如何合并多个 commit？” 其实只是交互式变基的功能之一，我们继续探索一下它的其它特性。再来看看之前我们在执行 git rebase -i 1522725 命令之后，vscode 打开 git-rebase-todo 文件：可以看到注释说明 Commands: 下面的内容，就是我们在交互式变基中能够使用的所有命令项。我们来一一解读： p, pick = use commit解读：pick，简写 p，意思是使用 commit。通过上图可以看到，git-rebase-todo 内的 commits 默认都是 pick 命令，意思就是选择（需要）这个 commit，并且不做任何改动。 r, reword = use commit, but edit the commit message解读：reword，简写 r，意思是使用 commit，但是需要编辑（修改） commit message。也就是说，如果我们想改写某个 commit message，就在 commit-hash 之前把默认的 pick 改写为 reword 或简写 r。 示例： 现在我们的 commit history 是这样的： 执行交互式变基命令 git rebase -i HEAD~3，在自动打开的 git-rebase-todo 文件内将第 2 行 pick 改写为 r。 保存关闭后，会继续自动打开 COMMIT_EDITMSG 文件，这里就是我们修改 commit-hash 为 ff7caa8 commit message 的地方。 我们将 commit message 修改成 20190222（reword），保存关闭后，再查看 commit history。 之前为 20190222 的 commit message 已经修改成功，同时 commit-hash 也发生了改变，并且在这个 commit 之后的 commit，它们的 commit-hash 也都会发生改变。（可对比查看交互式变基前后两张 commit history 图片） e, edit = use commit, but stop for amending解读：edit，简写 e，意思是可以暂时停止 rebase，此时允许修改 文件内容 和/或者 修改 commit message，然后继续 rebase。也就是说，如果我们想要修改某次 commit 的提交内容，就可以使用 edit 命令。 示例： 执行交互式变基命令 git rebase -i HEAD~3，在自动打开的 git-rebase-todo 文件内将第 2 行 pick 改写为 e。 保存关闭后，终端 Git 会提示我们可执行的操作。 Git 会切换到一个新的变基分支 默认 commit message 是此次编辑 commit 的 message，并且不能被修改。 在根目录新增一个文件 test.js 并 commit 提交 commit 之后会自动打开 COMMIT_EDITMSG 文件 这里才允许我们修改 commit-message，我们删除掉之前 reword 操作时添加的 message 内容 （reword），保存并关闭。此时查看 commit history 会发现 commit message 20190222 后面已经没有 （reword） 了。并且 commit 内容包含了我们刚才在根目录添加的 test.js 文件。 也就是说，我们修改了历史 commit 的 content 和 message。 s, squash = use commit, but meld into previous commit解读：squash，简写 s，意思是使用 commit，但是把修改内容融入到上一个 commit。也就是我们之前的需求 “如何合并多个 commit？” f, fixup = like &quot;squash&quot;, but discard this commit&#39;s log message解读：fixup，简写 f，与 squash 意思一样，但是直接丢弃 commit message。也就是说在编辑 commit message 时，不显示这个 commit 的 commit message。 x, exec = run command (the reset of the line) using shell解读：exec，简写 x，意思是在 rebase 过程执行脚本命令。因为在 rebase 交互过程中的改动是未经测试的，所以为了避免在 rebase 交互过程中的修改产生破坏性内容，所以提供执行脚本命令的功能，比如执行 npm run test 跑测试用例。 示例： 执行交互式变基命令 git rebase -i HEAD~3，在自动打开的 git-rebase-todo 文件内，在每一行 commit 下面添加 exec echo $(date)。意思是每 rebase 一个 commit，都执行一次 echo $(date) 命令吗，在终端打印出当前时间。 保存关闭后，会执行 rebase，过程中终端会打印出当前时间。 d, drop = remove commit解读：drop，简写 d，意思是移除 commit。很显然，就是删除这个 commit，并且会将这次 commit 的内容也删除掉。 结语交互式变基的使用场景在标准、规范的大型开源项目会经常使用到。但对于平时项目开发或是个人项目，可能并不是很在意（也没有精力顾及）commit history。只有当我们有这个意识以及对 Git 了解到一定程度，才能发挥交互式变基强大的能力。" } ]

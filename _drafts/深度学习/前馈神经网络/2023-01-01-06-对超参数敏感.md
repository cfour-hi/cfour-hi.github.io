---
title: 前馈神经网络-对超参数敏感
categories: [深度学习]
tags: [人工智能, 深度学习]
math: true
---

超参数是指在模型训练过程中需要手动设置的参数，例如学习率、正则化系数、迭代次数等。这些超参数的选择对模型性能有很大的影响，因此需要进行反复实验来调整超参数，以获得最佳的模型性能。

对超参数敏感是指，超参数的选择对模型性能有很大的影响，即使是微小的超参数调整也可能会导致模型性能的显著变化。例如，在神经网络中，如果学习率设置过大，可能会导致模型不收敛，而如果学习率设置过小，则可能需要更多的迭代次数才能收敛。因此，超参数的选择需要根据具体的问题进行调整，不能盲目地选择默认值。

以下是一个使用前馈神经网络进行手写数字识别的示例，其中包括了几个常用的超参数，如学习率、隐藏层大小、迭代次数等。可以通过调整这些超参数，来观察它们对模型性能的影响。

```python
import numpy as np
from sklearn.datasets import load_digits
from sklearn.preprocessing import LabelBinarizer
from sklearn.model_selection import train_test_split

# 加载数据集
digits = load_digits()
X = digits.data
y = digits.target
y_onehot = LabelBinarizer().fit_transform(y)
X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.3, random_state=42)

# 定义前馈神经网络
class FeedforwardNeuralNetwork:
    def __init__(self, num_inputs, num_hidden, num_outputs, learning_rate):
        self.num_inputs = num_inputs
        self.num_hidden = num_hidden
        self.num_outputs = num_outputs
        self.learning_rate = learning_rate

        # 初始化权重和偏置项
        self.weights_hidden = np.random.randn(num_inputs, num_hidden)
        self.bias_hidden = np.zeros((1, num_hidden))
        self.weights_output = np.random.randn(num_hidden, num_outputs)
        self.bias_output = np.zeros((1, num_outputs))

    def sigmoid(self, X):
        return 1 / (1 + np.exp(-X))

    def sigmoid_derivative(self, X):
        return X * (1 - X)

    def forward(self, X):
        # 计算隐藏层输出
        hidden_layer_input = np.dot(X, self.weights_hidden) + self.bias_hidden
        hidden_layer_output = self.sigmoid(hidden_layer_input)

        # 计算输出层输出
        output_layer_input = np.dot(hidden_layer_output, self.weights_output) + self.bias_output
        output_layer_output = self.sigmoid(output_layer_input)

        return hidden_layer_output, output_layer_output

    def backward(self, X, y, output):
        hidden_layer_output, output_layer_output = output

        # 计算输出层的误差
        output_error = y - output_layer_output
        output_delta = output_error * self.sigmoid_derivative(output_layer_output)

        # 计算隐藏层的误差
        hidden_error = np.dot(output_delta, self.weights_output.T)
        hidden_delta = hidden_error * self.sigmoid_derivative(hidden_layer_output)

        # 更新权重和偏置项
        self.weights_output += self.learning_rate * np.dot(hidden_layer_output.T, output_delta)
        self.bias_output += self.learning_rate * np.sum(output_delta, axis=0, keepdims=True)
        self.weights_hidden += self.learning_rate * np.dot(X.T, hidden_delta)
        self.bias_hidden += self.learning_rate * np.sum(hidden_delta, axis=0, keepdims=True)

    def train(self, X, y, num_epochs):
        for epoch in range(num_epochs):
            hidden_layer_output, output_layer_output = self.forward(X)
            self.backward(X, y, (hidden_layer_output, output_layer_output))

    def predict(self, X):
        _, output_layer_output = self.forward(X)
        return np.argmax(output_layer_output, axis=1)

# 创建模型并训练
model = FeedforwardNeuralNetwork(num_inputs=64, num_hidden=100, num_outputs=10, learning_rate=0.1)
model.train(X_train, y_train, num_epochs=1000)

# 在测试集上测试模型
y_pred = model.predict(X_test)
accuracy = np.mean(y_pred == np.argmax(y_test, axis=1))
print(f"Accuracy: {accuracy}")
```

在上面的示例中，可以调整超参数 `num_hidden` 和 `learning_rate` 来观察它们对模型性能的影响。如果 `num_hidden` 设置得过小，可能会导致模型欠拟合，而如果设置得过大，则可能会导致模型过拟合。而如果 `learning_rate` 设置得过小，则可能需要更多的迭代次数才能收敛，而如果设置得过大，则可能会导致模型不收敛或者出现震荡。因此，超参数的选择需要进行反复实验和调整，以获得最佳的模型性能。

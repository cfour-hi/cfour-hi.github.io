<feed xmlns="http://www.w3.org/2005/Atom"> <id>https://znlbwo.github.io/</id><title>znlbwo</title><subtitle>znlbwo's blog</subtitle> <updated>2024-03-04T09:16:35+08:00</updated> <author> <name>znlbwo</name> <uri>https://znlbwo.github.io/</uri> </author><link rel="self" type="application/atom+xml" href="https://znlbwo.github.io/feed.xml"/><link rel="alternate" type="text/html" hreflang="zh-CN" href="https://znlbwo.github.io/"/> <generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator> <rights> © 2024 znlbwo </rights> <icon>/assets/img/favicons/favicon.ico</icon> <logo>/assets/img/favicons/favicon-96x96.png</logo> <entry><title>机器学习基础-监督学习-逻辑回归之多元逻辑回归</title><link href="https://znlbwo.github.io/posts/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B9%8B%E5%A4%9A%E5%85%83%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/" rel="alternate" type="text/html" title="机器学习基础-监督学习-逻辑回归之多元逻辑回归" /><published>2023-01-10T00:00:00+08:00</published> <updated>2023-01-10T00:00:00+08:00</updated> <id>https://znlbwo.github.io/posts/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B9%8B%E5%A4%9A%E5%85%83%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/</id> <content src="https://znlbwo.github.io/posts/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B9%8B%E5%A4%9A%E5%85%83%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/" /> <author> <name>znlbwo</name> </author> <category term="机器学习" /> <summary> 多元逻辑回归是逻辑回归在多类分类问题上的扩展。在多元逻辑回归中，我们通过使用 softmax 函数将输入特征映射到多个类别的概率分布。 假设我们有 m 个训练样本，每个样本有 n 个特征，共有 K 个类别。我们用 X 表示一个 m × n 的矩阵，其中每一行表示一个样本的特征向量，用 Y 表示一个 m × K 的矩阵，其中每一行表示一个样本的类别标签。 对于每个样本 i，我们引入一个 K 维的权重向量 W，其中第 k 个元素表示样本 i 属于类别 k 的概率。我们的目标是找到最优的权重向量 W 来最大化训练样本的似然函数。 多元逻辑回归模型的假设函数为： \[h_\theta(x) = \text{softmax}(\theta^Tx)\] 其中，$\theta$ 是一个 (n+1) × K 的参数矩阵，包含了每个特征对于每个类别的权重。x 是一个 (n+1) 维特征向量，... </summary> </entry> <entry><title>机器学习基础-监督学习-逻辑回归之二元逻辑回归</title><link href="https://znlbwo.github.io/posts/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B9%8B%E4%BA%8C%E5%85%83%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/" rel="alternate" type="text/html" title="机器学习基础-监督学习-逻辑回归之二元逻辑回归" /><published>2023-01-10T00:00:00+08:00</published> <updated>2023-01-10T00:00:00+08:00</updated> <id>https://znlbwo.github.io/posts/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B9%8B%E4%BA%8C%E5%85%83%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/</id> <content src="https://znlbwo.github.io/posts/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B9%8B%E4%BA%8C%E5%85%83%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/" /> <author> <name>znlbwo</name> </author> <category term="机器学习" /> <summary> 当我们面对一个二元分类问题时，二元逻辑回归是一种常用的监督学习算法。它基于线性回归模型，并使用逻辑函数（也称为 sigmoid 函数）对输出进行转换，以获得概率估计。以下是对二元逻辑回归的详细讲解，包括算法原理、公式和代码示例。 算法原理 给定一个训练集，其中包含输入特征向量 x 和相应的二元标签 y (0 或 1)，我们的目标是学习一个适当的模型，该模型能够根据输入特征预测出标签的概率。 二元逻辑回归使用以下假设和模型表达式： 假设：给定输入特征 x，对应标签 y 的条件概率服从伯努利分布。 模型表达式：$h_\theta(x)=\frac{1}{1+e^{-\theta^Tx}}$ 其中，$h_\theta(x)$ 是模型的预测输出，表示预测输入 x 为正例的概率；$\theta$ 是模型的参数向量。 为了学习参数 $\theta$，我们需要定义一个代价函... </summary> </entry> <entry><title>线性回归之正则化</title><link href="https://znlbwo.github.io/posts/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%B9%8B%E6%AD%A3%E5%88%99%E5%8C%96/" rel="alternate" type="text/html" title="线性回归之正则化" /><published>2023-01-09T00:00:00+08:00</published> <updated>2023-01-09T00:00:00+08:00</updated> <id>https://znlbwo.github.io/posts/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%B9%8B%E6%AD%A3%E5%88%99%E5%8C%96/</id> <content src="https://znlbwo.github.io/posts/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%B9%8B%E6%AD%A3%E5%88%99%E5%8C%96/" /> <author> <name>znlbwo</name> </author> <summary> 2— title: 机器学习基础-监督学习-线性回归之正则化 categories: [机器学习] tags: [人工智能, 机器学习, 监督学习] math: true 正则化是一种常用的机器学习技术，用于控制模型的复杂度，并防止过拟合。通过在目标函数中添加正则化项，可以使得模型倾向于选择较为简单的参数组合。 在监督学习任务中，通常采用的是 L1 正则化和 L2 正则化两种形式。 L1 正则化（Lasso 正则化） L1 正则化通过在目标函数中添加参数的绝对值之和，来限制参数的大小。L1 正则化的数学表达式如下： Loss = 原始目标函数 + λ * Σ|θ_i| 其中，Loss 表示加入 L1 正则化后的目标函数，λ 是正则化系数，θ_i 表示模型的第 i 个参数。 L1 正则化的效果是使得一部分参数的值变为 0，从而实现特征选择的效果，能够降低模型... </summary> </entry> <entry><title>机器学习基础-监督学习-线性回归之梯度下降法</title><link href="https://znlbwo.github.io/posts/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%B9%8B%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/" rel="alternate" type="text/html" title="机器学习基础-监督学习-线性回归之梯度下降法" /><published>2023-01-09T00:00:00+08:00</published> <updated>2023-01-09T00:00:00+08:00</updated> <id>https://znlbwo.github.io/posts/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%B9%8B%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/</id> <content src="https://znlbwo.github.io/posts/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%B9%8B%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/" /> <author> <name>znlbwo</name> </author> <category term="机器学习" /> <summary> 梯度下降法（Gradient Descent）是一种常用的优化算法，用于求解损失函数的最小值。它通过迭代更新模型参数，沿着损失函数梯度的反方向逐步调整参数，直到达到最优解。下面是梯度下降法的详细讲解，包括算法原理、公式以及示例代码。 算法原理： 初始化模型参数。 计算损失函数关于参数的梯度。 更新参数：将参数沿着梯度的反方向移动一定步长。 重复步骤 2 和 3，直到达到停止条件（例如达到最大迭代次数或损失函数变化小于某个阈值）。 公式： 假设有一个损失函数 J(θ)，其中 θ 表示模型的参数。梯度下降法的参数更新公式如下： θ = θ - α * ∇J(θ) 其中，α 称为学习率（learning rate），控制参数更新的步长；∇J(θ)表示损失函数关于参数 θ 的梯度。 示例代码： 下面是使用梯度下降法求解线性回归模型的示例代码，其中使用均方误差... </summary> </entry> <entry><title>机器学习基础-监督学习-线性回归之最小二乘法</title><link href="https://znlbwo.github.io/posts/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%B9%8B%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95/" rel="alternate" type="text/html" title="机器学习基础-监督学习-线性回归之最小二乘法" /><published>2023-01-09T00:00:00+08:00</published> <updated>2023-01-09T00:00:00+08:00</updated> <id>https://znlbwo.github.io/posts/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%B9%8B%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95/</id> <content src="https://znlbwo.github.io/posts/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%B9%8B%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95/" /> <author> <name>znlbwo</name> </author> <category term="机器学习" /> <summary> 最小二乘法是一种常用的线性回归方法，用于拟合数据并找到最优的模型参数。它通过最小化实际观测值与模型预测值之间的残差平方和来确定最优参数。以下是对最小二乘法的详细讲解，包括公式和代码示例。 理论背景： 假设我们有一个包含 n 个样本的数据集，每个样本包括 d 个特征和一个目标值。用 X 表示特征矩阵，y 表示目标值向量。线性回归模型的一般形式可以表示为：y = Xβ + ε，其中 β 是待估计的模型参数，ε 是误差项。 最小二乘法的目标： 最小二乘法的目标是找到最优的 β，使得残差平方和最小化。残差表示实际观测值与模型预测值之间的差异，即 ε = y - Xβ。最小二乘法通过最小化残差平方和来确定最优的 β，即 min   y - Xβ   ^2。 最小二乘法... </summary> </entry> </feed>
